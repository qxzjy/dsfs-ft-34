{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "97f1398c-40f8-41d0-ac5d-5046c3e487a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Words count with PySpark RDDs\n",
    "If you've ever heard of \"Hello, world!\" for web development, \"Word count\" is the actual equivalent for distributed computing.\n",
    "\n",
    "In this notebook, we will setup a pipeline that will count the words in a document in a distributed manner. For convenience, we will do this on a single small document, but the operation should easily generalize to bigger documents that would not fit into the memory of a single machine. And that's the idea of this whole module, we work with technology that is able to scale according to the task at hand, even if we practice with smaller data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "49ae3d88-b600-4878-8e86-7ac121d4ee26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We start by defining the spark context to play with RDDs\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "de50764a-6e2a-4dbc-a266-9c5194aa21fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We need a S3 filepath\n",
    "\n",
    "FILENAME = 's3://full-stack-bigdata-datasets/Big_Data/purple_rain.txt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the filepath to a Spark RDD using `.textFile(...)` from a SparkContext into `text_file`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0ced4916-b485-4875-8735-db2b8b6a6820",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Print out `text_file`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cbb3d0db-1440-4a9a-af02-d556394a8c2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[3]: s3://full-stack-bigdata-datasets/Big_Data/purple_rain.txt MapPartitionsRDD[34] at textFile at NativeMethodAccessorImpl.java:0</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[3]: s3://full-stack-bigdata-datasets/Big_Data/purple_rain.txt MapPartitionsRDD[34] at textFile at NativeMethodAccessorImpl.java:0</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7c8e5680-2400-4e4c-aead-c770c530cb61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3. That doesn't tell us much, how would you do to see the first 3 elements of this RDD? take the first 3 elements of the RDD `text_file`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1cd9a55c-0857-4740-8e64-772337e691b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[4]: [&#39;I never meant to cause you any sorrow&#39;,\n",
       " &#39;I never meant to cause you any pain&#39;,\n",
       " &#39;I only wanted one time to see you laughing&#39;]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[4]: [&#39;I never meant to cause you any sorrow&#39;,\n &#39;I never meant to cause you any pain&#39;,\n &#39;I only wanted one time to see you laughing&#39;]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f2b6f7b8-e656-4308-bd27-867a5e89cbdd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4. This is a list of sentences, what we want is a list of tokens. Use the map function and a string method in order to split the charater strings into lists of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b42b030d-e2de-4f7c-9e8f-2891ca510bf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[5]: [[&#39;I&#39;, &#39;never&#39;, &#39;meant&#39;, &#39;to&#39;, &#39;cause&#39;, &#39;you&#39;, &#39;any&#39;, &#39;sorrow&#39;],\n",
       " [&#39;I&#39;, &#39;never&#39;, &#39;meant&#39;, &#39;to&#39;, &#39;cause&#39;, &#39;you&#39;, &#39;any&#39;, &#39;pain&#39;],\n",
       " [&#39;I&#39;, &#39;only&#39;, &#39;wanted&#39;, &#39;one&#39;, &#39;time&#39;, &#39;to&#39;, &#39;see&#39;, &#39;you&#39;, &#39;laughing&#39;]]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[5]: [[&#39;I&#39;, &#39;never&#39;, &#39;meant&#39;, &#39;to&#39;, &#39;cause&#39;, &#39;you&#39;, &#39;any&#39;, &#39;sorrow&#39;],\n [&#39;I&#39;, &#39;never&#39;, &#39;meant&#39;, &#39;to&#39;, &#39;cause&#39;, &#39;you&#39;, &#39;any&#39;, &#39;pain&#39;],\n [&#39;I&#39;, &#39;only&#39;, &#39;wanted&#39;, &#39;one&#39;, &#39;time&#39;, &#39;to&#39;, &#39;see&#39;, &#39;you&#39;, &#39;laughing&#39;]]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "748172f9-85fa-409c-8fc6-27facc2c0779",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "That's not exactly what we wanted... We wanted a list of tokens, we got a... **list of list of tokens**!\n",
    "\n",
    "That's because, in this case, we need a special version of `.map()` called `flatMap`: it will flatten the list of list of tokens into a list of tokens.\n",
    "\n",
    "Let's try it out: we take the same expression as the previous one, but replace `.map()` with `.flatMap()` and call the resulting variable `tokens`.\n",
    "\n",
    "---\n",
    "ðŸ’¡ It usually takes time to understand the notion of `.flatMap` and flattening in general, like `.map()`, these are concepts from the functionnal programming world. Unless you come from such a background, it probably **won't be easy to grasp these concepts the first time you encouter them**.\n",
    "\n",
    "**Let's keep our eyes on the goal: to develop a broader understanding of how Spark works.**\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Copy/paste the previous cell, and:\n",
    "- replace `.map(...)` with `.flatMap(...)`\n",
    "- rename the variable `tokenized_text` to `tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "752b4838-81c5-4a54-a898-a619d3c383bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Use this cell to play with `tokens`, take different amounts of it, or collect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d7bd70bc-24f1-438e-baf0-e5e79d7567d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[7]: [&#39;I&#39;, &#39;never&#39;, &#39;meant&#39;, &#39;to&#39;, &#39;cause&#39;, &#39;you&#39;, &#39;any&#39;, &#39;sorrow&#39;, &#39;I&#39;, &#39;never&#39;]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[7]: [&#39;I&#39;, &#39;never&#39;, &#39;meant&#39;, &#39;to&#39;, &#39;cause&#39;, &#39;you&#39;, &#39;any&#39;, &#39;sorrow&#39;, &#39;I&#39;, &#39;never&#39;]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4e2bc2e3-62bc-46e5-a775-2ad14d1f00af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now that we have our list of words (well, **not exactly a list of words, it is still a RDD**), we can start counting things.\n",
    "\n",
    "In order to do that, we need to map each word to an initial count, so instead of having:\n",
    "```\n",
    "['I',\n",
    " 'never',\n",
    " 'meant',\n",
    " ...,\n",
    " 'I',\n",
    " 'never',\n",
    " ...]\n",
    "```\n",
    "We would like our list to look like this:\n",
    "```\n",
    "[('I', 1),\n",
    " ('never', 1),\n",
    " ('meant', 1),\n",
    " ...,\n",
    " ('I', 1),\n",
    " ('never', 1),\n",
    " ...]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Write a function `token_to_tuple` that takes:\n",
    "- a token as input (a string)\n",
    "- and returns (token, 1) (a tuple) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad38100e-8267-47df-9e6c-5523f6f3d1df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. map `tokens` to your new function `token_to_tuple` to create a variable called `partial_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "92894ab6-39cd-4b9f-a94c-afeda6c74d08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[9]: PythonRDD[38] at RDD at PythonRDD.scala:58</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[9]: PythonRDD[38] at RDD at PythonRDD.scala:58</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Take the first 10 elements of `partial_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "82738211-77ff-45b3-a070-cd9b67e98a3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[10]: [(&#39;I&#39;, 1),\n",
       " (&#39;never&#39;, 1),\n",
       " (&#39;meant&#39;, 1),\n",
       " (&#39;to&#39;, 1),\n",
       " (&#39;cause&#39;, 1),\n",
       " (&#39;you&#39;, 1),\n",
       " (&#39;any&#39;, 1),\n",
       " (&#39;sorrow&#39;, 1),\n",
       " (&#39;I&#39;, 1),\n",
       " (&#39;never&#39;, 1)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[10]: [(&#39;I&#39;, 1),\n (&#39;never&#39;, 1),\n (&#39;meant&#39;, 1),\n (&#39;to&#39;, 1),\n (&#39;cause&#39;, 1),\n (&#39;you&#39;, 1),\n (&#39;any&#39;, 1),\n (&#39;sorrow&#39;, 1),\n (&#39;I&#39;, 1),\n (&#39;never&#39;, 1)]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a9247d56-82c0-49d9-be4b-b46e524756d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Good job!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0761bfa5-7161-44b0-ab54-ea65a4882cb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Beware, now comes the hard-part... We need to reduce this.\n",
    "\n",
    "Don't forget RDD are very low level objects, when we start using DataFrames (the other main kind of data format) all of this will become easier because of their higher level of abstraction.\n",
    "\n",
    "What we want is to take tuples with the same key, like `('never', 1)` and `('never', 1)` and aggregate them, so in the end we have `('never', 2)` (or more than 2 if there are more occurences of 'never').\n",
    "\n",
    "These kind of tuples are called **key-value pairs**, and while most Spark operations work on RDDs containing any type of objects, a few special operations are only available on RDDs of key-value pairs. You can read more about it [in the documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html#working-with-key-value-pairs).\n",
    "\n",
    "Among these operations are `.groupByKey(...)` and `.reduceByKey(...)`: the latter has better performances, but the former is easier to understand so we will start with this one."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "54f1395a-b2ac-418c-b40f-3cd2a9fb503f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### groupByKey"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Call `.groupByKey(...)` on `partial_count` and put it inside `grouped_by_key` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.  take the first 3 elements of `grouped_by_key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a21afd44-ee0a-48a8-b6f4-348dd50e4ac7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[12]: [(&#39;never&#39;, &lt;pyspark.resultiterable.ResultIterable at 0x7fa6180e3430&gt;),\n",
       " (&#39;cause&#39;, &lt;pyspark.resultiterable.ResultIterable at 0x7fa6180e32b0&gt;),\n",
       " (&#39;pain&#39;, &lt;pyspark.resultiterable.ResultIterable at 0x7fa6180e32e0&gt;)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[12]: [(&#39;never&#39;, &lt;pyspark.resultiterable.ResultIterable at 0x7fa6180e3430&gt;),\n (&#39;cause&#39;, &lt;pyspark.resultiterable.ResultIterable at 0x7fa6180e32b0&gt;),\n (&#39;pain&#39;, &lt;pyspark.resultiterable.ResultIterable at 0x7fa6180e32e0&gt;)]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3a333cde-a12f-41a2-96e0-2a3ba8b7f978",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "What's this: `<pyspark.resultiterable.ResultIterable at 0x10bc0c2d0>` ?\n",
    "\n",
    "You don't have to worry about the details, but one thing has to attract your attention: `Iterable`, this seems to suggest those objects are iterable, an iterable in Python is something that you can iterate on: basically something that you can call `for` on, like a list, or a string, etc.\n",
    "\n",
    "```python\n",
    "for letter in 'Spark':\n",
    "    print(letter)\n",
    "> S\n",
    "> p\n",
    "> a\n",
    "> r\n",
    "> k\n",
    "```\n",
    "\n",
    "Each element of `grouped_by_key` is a tuple, and inside a tuple there is an iterable we can iterate over.\n",
    "\n",
    "We will first try with the first element."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Take the first element of `grouped_by_key`, put it in `first_item` variable and print out its type.\n",
    "\n",
    "_WARNING: the type should be a tuple, not a list._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "78646ec9-dfe6-404c-8c70-015da6c81724",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[18]: (&#39;never&#39;, &lt;pyspark.resultiterable.ResultIterable at 0x7fa6118c4340&gt;)</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[18]: (&#39;never&#39;, &lt;pyspark.resultiterable.ResultIterable at 0x7fa6118c4340&gt;)</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a0f175b4-d5b3-4011-af65-b344633f6f9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We'd like a way to print these items, for example, such that 'never' would look like this:\n",
    "```\n",
    "'never': [1, 1, 1, 1]\n",
    "```\n",
    "\n",
    "We will write a function that does this, take an item as a tuple of (`str`, `ResultIterable`), and print out:\n",
    "```\n",
    "ITEM_NAME: OCCURENCES_AS_A_LIST\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Define a way to print our items like above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7e98f773-f170-42c1-b08a-466650790c37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. take the first 10 items from `grouped_by_key` and then iterate over them. Then, inside the loop, use the function `print_item(...)` on each item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d5f8c007-a614-4bd6-aa22-bf10b562a076",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">never: [1, 1, 1, 1]\n",
       "cause: [1, 1]\n",
       "pain: [1]\n",
       "only: [1, 1, 1, 1, 1, 1, 1]\n",
       "in: [1, 1]\n",
       "rain: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
       ": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
       "Purple: [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
       "rain,: [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
       "bathing: [1]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">never: [1, 1, 1, 1]\ncause: [1, 1]\npain: [1]\nonly: [1, 1, 1, 1, 1, 1, 1]\nin: [1, 1]\nrain: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nPurple: [1, 1, 1, 1, 1, 1, 1, 1, 1]\nrain,: [1, 1, 1, 1, 1, 1, 1, 1, 1]\nbathing: [1]\n</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2485ee18-7276-4f19-a02d-d1df5fc79861",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next step might be challenging.\n",
    "\n",
    "When you take the first 10 elements of `grouped_by_key`, it returns a list of `Tuple[str, ResultIterable]`.  \n",
    "What we want instead is a list of `Tuple[str, int]` where the second element is the total number of occurence for the fist element.\n",
    "\n",
    "NOTE: you might wanna try to first return a list of `Tuple[str, list]`.\n",
    "\n",
    "You should be able to do all this using only list comprehensions.\n",
    "\n",
    "15. Follow previous instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d0df028b-b59c-4ce5-8599-d81ae4117194",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[21]: [(&#39;never&#39;, 4),\n",
       " (&#39;cause&#39;, 2),\n",
       " (&#39;pain&#39;, 1),\n",
       " (&#39;only&#39;, 7),\n",
       " (&#39;in&#39;, 2),\n",
       " (&#39;rain&#39;, 14),\n",
       " (&#39;&#39;, 10),\n",
       " (&#39;Purple&#39;, 9),\n",
       " (&#39;rain,&#39;, 9),\n",
       " (&#39;bathing&#39;, 1)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[21]: [(&#39;never&#39;, 4),\n (&#39;cause&#39;, 2),\n (&#39;pain&#39;, 1),\n (&#39;only&#39;, 7),\n (&#39;in&#39;, 2),\n (&#39;rain&#39;, 14),\n (&#39;&#39;, 10),\n (&#39;Purple&#39;, 9),\n (&#39;rain,&#39;, 9),\n (&#39;bathing&#39;, 1)]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0a154155-7f12-4554-9180-ddf611baf0ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As you've seen this can be done using standard list comprehension.  \n",
    "If you're curious, even though Python is not a purely functional language, you can write this in a functional fashion and achieve the same result.\n",
    "\n",
    "_Please note this would look obviouly more elegant in a purely functional language.  \n",
    "And I put it in there only to introduce you to another programming paradigm if you've mostly encountered imperative programming before. This little introduction is helpful because Spark is based on Scala, which, although not being a purely functional language, provides support for many functional programming features._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b3ac4e38-3c0d-4691-9e8e-bf3f10ee25a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[22]: [(&#39;never&#39;, 4),\n",
       " (&#39;cause&#39;, 2),\n",
       " (&#39;pain&#39;, 1),\n",
       " (&#39;only&#39;, 7),\n",
       " (&#39;in&#39;, 2),\n",
       " (&#39;rain&#39;, 14),\n",
       " (&#39;&#39;, 10),\n",
       " (&#39;Purple&#39;, 9),\n",
       " (&#39;rain,&#39;, 9),\n",
       " (&#39;bathing&#39;, 1)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[22]: [(&#39;never&#39;, 4),\n (&#39;cause&#39;, 2),\n (&#39;pain&#39;, 1),\n (&#39;only&#39;, 7),\n (&#39;in&#39;, 2),\n (&#39;rain&#39;, 14),\n (&#39;&#39;, 10),\n (&#39;Purple&#39;, 9),\n (&#39;rain,&#39;, 9),\n (&#39;bathing&#39;, 1)]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Don't try at home ;)\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "list(map(lambda t: (t[0], reduce(lambda a, b: a + b, t[1])),\n",
    "         grouped_by_key.take(10)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "25cdfefc-b9bf-4dff-bc17-3b877629a221",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "That would work, but that's using regular Python, hence we're not profiting from Spark's distributed computing capabilities, which means:\n",
    "\n",
    "- the computation would be much slower on big datasets,\n",
    "- if the datasets is too big to be stored on the memory of our machine our program would crash.\n",
    "\n",
    "That's exactly what `.reduceByKey(...)` will help us to solve. It's usage is a bit similar to `.groupByKey(...)` but it takes a function as a parameter, this function should tell Spark how to aggregate 2 items, in our case, that the value of each tuple. For example, let's say we have a group:\n",
    "\n",
    "```python\n",
    "('dog', 1), ('dog', 1)\n",
    "```\n",
    "\n",
    "We want a formula applied on values that will give us the end result, e.g. \"how many dogs\". In our case, that's a simple sum:\n",
    "\n",
    "```python\n",
    "def reduce_function(value_1, value_2):\n",
    "    return value_1 + value_2\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Write our reduce function: reduce_function which takes 2 values and return their sum:\n",
    "\n",
    "_NOTE: name these parameters `a` and `b`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ec34b385-ac37-4d7e-b7c6-b744033a7618",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "264306c5-874d-4864-94c1-99fabe4d7f94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "17. We're now ready to reduce. You will pass your function as parameter to `.reduceByKey(...)`. call `.reduceByKey(...)` on `partial_count`, name it `reduced`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "493fa0d2-6815-42dd-9ff3-65086c36e1cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Take the 10 first values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b30039e8-6763-4a22-a37e-8cbda56b16a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[25]: [(&#39;never&#39;, 4),\n",
       " (&#39;cause&#39;, 2),\n",
       " (&#39;pain&#39;, 1),\n",
       " (&#39;only&#39;, 7),\n",
       " (&#39;in&#39;, 2),\n",
       " (&#39;rain&#39;, 14),\n",
       " (&#39;&#39;, 10),\n",
       " (&#39;Purple&#39;, 9),\n",
       " (&#39;rain,&#39;, 9),\n",
       " (&#39;bathing&#39;, 1)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[25]: [(&#39;never&#39;, 4),\n (&#39;cause&#39;, 2),\n (&#39;pain&#39;, 1),\n (&#39;only&#39;, 7),\n (&#39;in&#39;, 2),\n (&#39;rain&#39;, 14),\n (&#39;&#39;, 10),\n (&#39;Purple&#39;, 9),\n (&#39;rain,&#39;, 9),\n (&#39;bathing&#39;, 1)]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ff7f41c8-2bb3-4325-b21f-f0e646ff626b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We're almost there... ðŸ˜…\n",
    "We've got a list of tuples, where the key is the token, and the value is its count within the text, but.. \n",
    "**they're not ordered...** which is inconvenient if we want to have the 10 most popular tokens within the text.\n",
    "\n",
    "We will use `.sortBy(...)`, but before we do, let's have a refresher on sorting with Python.\n",
    "\n",
    "For example, how would you sort this grocery list by the number of items?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "089be583-dd9a-42a5-b7c0-2b1991b88cbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[26]: [(&#39;banana&#39;, 3), (&#39;orange&#39;, 5), (&#39;pineapple&#39;, 2)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[26]: [(&#39;banana&#39;, 3), (&#39;orange&#39;, 5), (&#39;pineapple&#39;, 2)]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fruits = [('banana', 3), ('orange', 5), ('pineapple', 2)]\n",
    "fruits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "44b20b92-f8cf-4c57-8069-edfc732c34c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "`sorted(fruits)` won't work because by default sorting on tuple take the first element, in our case, it would sort alphabetically on the name of the fruits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "59a37670-b88a-48e8-a290-76dd9999a4ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[27]: [(&#39;banana&#39;, 3), (&#39;orange&#39;, 5), (&#39;pineapple&#39;, 2)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[27]: [(&#39;banana&#39;, 3), (&#39;orange&#39;, 5), (&#39;pineapple&#39;, 2)]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted(fruits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2ca891d4-339f-402a-96a8-05b2574859b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can force the `key` parameter to sort on the second item of each tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a8df4918-c1b3-4150-85e5-a08ebf7b254d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[28]: [(&#39;pineapple&#39;, 2), (&#39;banana&#39;, 3), (&#39;orange&#39;, 5)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[28]: [(&#39;pineapple&#39;, 2), (&#39;banana&#39;, 3), (&#39;orange&#39;, 5)]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted(fruits, key=lambda x: x[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9e70edec-d923-4c00-9f68-5c042af52189",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "19. Now, we will do the same on our rdd. Just like `key` in Python's `sorted`, PySpark's `.sortBy(...)` can take a function as a parameter. Use `.sortBy(...)` on `reduced`, name it `sorted_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1f4b7de4-450b-40a9-962e-fd0509ee60bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. Take the 10 first values of `sorted_counts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e7e4ba55-6a70-410c-8294-31943b23cde1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[30]: [(&#39;pain&#39;, 1),\n",
       " (&#39;bathing&#39;, 1),\n",
       " (&#39;kind&#39;, 1),\n",
       " (&#39;of&#39;, 1),\n",
       " (&#39;steal&#39;, 1),\n",
       " (&#39;end&#39;, 1),\n",
       " (&#39;underneath&#39;, 1),\n",
       " (&#39;are&#39;, 1),\n",
       " (&#39;we&#39;, 1),\n",
       " (&#39;out&#39;, 1)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[30]: [(&#39;pain&#39;, 1),\n (&#39;bathing&#39;, 1),\n (&#39;kind&#39;, 1),\n (&#39;of&#39;, 1),\n (&#39;steal&#39;, 1),\n (&#39;end&#39;, 1),\n (&#39;underneath&#39;, 1),\n (&#39;are&#39;, 1),\n (&#39;we&#39;, 1),\n (&#39;out&#39;, 1)]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b5b3444b-3d6d-483f-983a-01761256f005",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "What do you think?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2a60fd53-1f19-4e47-98a2-8e3cb5698f6c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It seems sorted, but in **ascending order**!\n",
    "\n",
    "If we wanted to do this in Python, we could just set the `reverse` argument to `True` when calling `sorted(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b60a2faa-8ff4-47b6-9d43-b07e9937cf15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[31]: [(&#39;orange&#39;, 5), (&#39;banana&#39;, 3), (&#39;pineapple&#39;, 2)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[31]: [(&#39;orange&#39;, 5), (&#39;banana&#39;, 3), (&#39;pineapple&#39;, 2)]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted(fruits, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1548ef8e-e5e7-4a9d-8dbb-f060dd761473",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can't do this with Spark's RDDs. What we can do instead is **take the opposite value and order by it**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. Use `.sortBy(...)` on `reduced`, but with a descending sort, name it `desc_sorted_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c0cd2f44-bd74-40b3-b79c-aad207914a54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. Take the 10 first values of `desc_sorted_counts`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3abbbdd2-4993-4633-a5d7-cbf1595bdfb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[33]: [(&#39;rain&#39;, 14),\n",
       " (&#39;I&#39;, 14),\n",
       " (&#39;you&#39;, 14),\n",
       " (&#39;purple&#39;, 14),\n",
       " (&#39;to&#39;, 13),\n",
       " (&#39;&#39;, 10),\n",
       " (&#39;Purple&#39;, 9),\n",
       " (&#39;rain,&#39;, 9),\n",
       " (&#39;only&#39;, 7),\n",
       " (&#39;see&#39;, 6)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[33]: [(&#39;rain&#39;, 14),\n (&#39;I&#39;, 14),\n (&#39;you&#39;, 14),\n (&#39;purple&#39;, 14),\n (&#39;to&#39;, 13),\n (&#39;&#39;, 10),\n (&#39;Purple&#39;, 9),\n (&#39;rain,&#39;, 9),\n (&#39;only&#39;, 7),\n (&#39;see&#39;, 6)]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bdbeeb48-79ab-40f0-b789-4139f5c7e66c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Finally, what's the most common word in our document?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b3fa5da0-79ee-4988-9753-466466f2c888",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **Bonus**: putting everything together\n",
    "\n",
    "We will create a function `count_words` that will do everything we did previously, but this time in one swell swoop, we won't use intermediary variables.\n",
    "\n",
    "The function will:\n",
    "- take a filepath as argument\n",
    "- load the content of this filepath into a Spark RDD\n",
    "- `flatMap(...)` each line of this RDD into tokens by splitting on the ' ' string\n",
    "- `.map(...)` each token to `(token, 1)` so this can be then reduced\n",
    "- by calling `.reduceByKey(...)` with a function that sums the values\n",
    "- and then sort the results with `.sortBy(...)` using the proper function to sort in descending order\n",
    "- and return an RDD\n",
    "\n",
    "---\n",
    "âš ï¸ Make sure your function returns a RDD\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "45add4da-07f1-40b0-bec7-77113ddc5d9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def count_words(filepath):\n",
    "    # TODO: implement the content of the function\n",
    "    # \n",
    "    # NOTE: you can remove `pass`\n",
    "    # it's just here to avoid the cell crashing while the\n",
    "    # content of the function is empty\n",
    "    pass\n",
    "    ### BEGIN STRIP ###\n",
    "    return sc.textFile(filepath)\\\n",
    "    .flatMap(lambda line: line.split(' '))\\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda t: -t[1])\n",
    "    ### END STRIP ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use `count_words` with `FILENAME` and check its type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "770a1b71-3f67-4652-a5ac-81ab6742f9f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[36]: pyspark.rdd.PipelinedRDD</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[36]: pyspark.rdd.PipelinedRDD</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e5bad4b7-6b2f-4c3e-99b0-f06d716b1826",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It should be a `pyspark.rdd.PipelineRDD`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Finally, take the 10 first elements of your RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "69ba8844-6e64-4e60-bc98-93babc9fba1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[37]: [(&#39;rain&#39;, 14),\n",
       " (&#39;I&#39;, 14),\n",
       " (&#39;you&#39;, 14),\n",
       " (&#39;purple&#39;, 14),\n",
       " (&#39;to&#39;, 13),\n",
       " (&#39;&#39;, 10),\n",
       " (&#39;Purple&#39;, 9),\n",
       " (&#39;rain,&#39;, 9),\n",
       " (&#39;only&#39;, 7),\n",
       " (&#39;see&#39;, 6)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[37]: [(&#39;rain&#39;, 14),\n (&#39;I&#39;, 14),\n (&#39;you&#39;, 14),\n (&#39;purple&#39;, 14),\n (&#39;to&#39;, 13),\n (&#39;&#39;, 10),\n (&#39;Purple&#39;, 9),\n (&#39;rain,&#39;, 9),\n (&#39;only&#39;, 7),\n (&#39;see&#39;, 6)]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fe6707df-01f8-401b-be34-7463c5d1c5d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "That's it, you've done it!\n",
    "\n",
    "You've created a Spark job, the next step would be to neatly package this into a Python executable and submit it to a Spark Cluster for batch or stream execution, but this is beyond the content of this course."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0e7100db-e601-4c24-a87c-efb7a368f890",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Going further\n",
    "\n",
    "We used a toy dataset, we suggest you try with a bigger one."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02-Words_count_with_PySpark_rdds_solutions_databricks",
   "notebookOrigID": 2835273710881635,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
