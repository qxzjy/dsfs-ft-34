{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7877e55c-13cc-4502-975c-15a838cb1f13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# First steps with PySpark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3a6a8e64-96de-4097-aa7b-cb1b8e0ca40c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Learning objectives\n",
    "\n",
    "- Get familiar with PySpark RDDs\n",
    "- Become imbued with the concept of lazyness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4edbf173-1d53-4bcf-a75f-0231273ce55e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's start by loading the file from it's URI: `s3://full-stack-bigdata-datasets/Big_Data/tears_in_rain.txt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ca328d46-7d4b-4714-a929-89efc0f949f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark\n",
    "sc = spark.sparkContext\n",
    "\n",
    "text_rdd = sc.textFile(\"s3://full-stack-bigdata-datasets/Big_Data/tears_in_rain.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4dee91ef-54c0-4313-a787-575ded834a3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Print out the first line to make sure everything went well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "51b69984-47cb-477c-80e5-3d61d610043a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[2]: [&#34;I&#39;ve seen things you people wouldn&#39;t believe. &#34;]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[2]: [&#34;I&#39;ve seen things you people wouldn&#39;t believe. &#34;]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e1d48d5e-4042-4894-89c2-0f8d9e75892b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Good you remember how to load a file from a URI, however, most of the time you will need to access files from distributed files systems, like amazon S3, here we will show you how you can access the file from an S3 bucket using specific credentials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3aceeb3a-b326-4e77-8c16-c5c15c37debc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-4165348753901380&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> text_rdd <span class=\"ansi-blue-fg\">=</span> sc<span class=\"ansi-blue-fg\">.</span>textFile<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;s3://full-stack-bigdata-datasets/Big_Data/tears_in_rain_not_public.txt&#34;</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>text_rdd<span class=\"ansi-blue-fg\">.</span>take<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-red-fg\"># Note that you will only get the error after trying to perform an action, that&#39;s only then that you are trying to access the data.</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">take</span><span class=\"ansi-blue-fg\">(self, num)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1566</span>         &#34;&#34;&#34;\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1567</span>         items <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">]</span>\n",
       "<span class=\"ansi-green-fg\">-&gt; 1568</span><span class=\"ansi-red-fg\">         </span>totalParts <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>getNumPartitions<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1569</span>         partsScanned <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-cyan-fg\">0</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1570</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">getNumPartitions</span><span class=\"ansi-blue-fg\">(self)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    467</span>         <span class=\"ansi-cyan-fg\">2</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    468</span>         &#34;&#34;&#34;\n",
       "<span class=\"ansi-green-fg\">--&gt; 469</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>partitions<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>size<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    470</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    471</span>     <span class=\"ansi-green-fg\">def</span> filter<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> f<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    108</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    109</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 110</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    111</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    112</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n",
       "\n",
       "<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o316.partitions.\n",
       ": java.nio.file.AccessDeniedException: s3://full-stack-bigdata-datasets/Big_Data/tears_in_rain_not_public.txt: getFileStatus on s3://full-stack-bigdata-datasets/Big_Data/tears_in_rain_not_public.txt: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden; request: HEAD https://full-stack-bigdata-datasets.s3.eu-west-3.amazonaws.com Big_Data/tears_in_rain_not_public.txt {} Hadoop 2.7.4, aws-sdk-java/1.11.655 Linux/5.4.0-1039-aws OpenJDK_64-Bit_Server_VM/25.282-b08 java/1.8.0_282 scala/2.12.10 vendor/Azul_Systems,_Inc. com.amazonaws.services.s3.model.GetObjectMetadataRequest; Request ID: 2B19GCQY0A6EVMBF, Extended Request ID: zzjmYTQrwdPCRzLKLYidAGG4jlapODg3u/qzgDuQ7X2b6EPxHNh6ura+8VPaBApEPcgg7U3npzw=, Cloud Provider: AWS, Instance ID: i-0d7c7f70557ae6b5a (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 2B19GCQY0A6EVMBF; S3 Extended Request ID: zzjmYTQrwdPCRzLKLYidAGG4jlapODg3u/qzgDuQ7X2b6EPxHNh6ura+8VPaBApEPcgg7U3npzw=), S3 Extended Request ID: zzjmYTQrwdPCRzLKLYidAGG4jlapODg3u/qzgDuQ7X2b6EPxHNh6ura+8VPaBApEPcgg7U3npzw=:403 Forbidden\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:243)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:154)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2854)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2824)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2763)\n",
       "\tat org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:57)\n",
       "\tat org.apache.hadoop.fs.Globber.glob(Globber.java:252)\n",
       "\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.globStatus(S3AFileSystem.java:3662)\n",
       "\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:259)\n",
       "\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n",
       "\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n",
       "\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:213)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:307)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:303)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:307)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:303)\n",
       "\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:62)\n",
       "\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:62)\n",
       "\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:295)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden; request: HEAD https://full-stack-bigdata-datasets.s3.eu-west-3.amazonaws.com Big_Data/tears_in_rain_not_public.txt {} Hadoop 2.7.4, aws-sdk-java/1.11.655 Linux/5.4.0-1039-aws OpenJDK_64-Bit_Server_VM/25.282-b08 java/1.8.0_282 scala/2.12.10 vendor/Azul_Systems,_Inc. com.amazonaws.services.s3.model.GetObjectMetadataRequest; Request ID: 2B19GCQY0A6EVMBF, Extended Request ID: zzjmYTQrwdPCRzLKLYidAGG4jlapODg3u/qzgDuQ7X2b6EPxHNh6ura+8VPaBApEPcgg7U3npzw=, Cloud Provider: AWS, Instance ID: i-0d7c7f70557ae6b5a (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 2B19GCQY0A6EVMBF; S3 Extended Request ID: zzjmYTQrwdPCRzLKLYidAGG4jlapODg3u/qzgDuQ7X2b6EPxHNh6ura+8VPaBApEPcgg7U3npzw=), S3 Extended Request ID: zzjmYTQrwdPCRzLKLYidAGG4jlapODg3u/qzgDuQ7X2b6EPxHNh6ura+8VPaBApEPcgg7U3npzw=\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n",
       "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n",
       "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n",
       "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4926)\n",
       "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4872)\n",
       "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1321)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1569)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1566)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2845)\n",
       "\t... 31 more\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4165348753901380&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> text_rdd <span class=\"ansi-blue-fg\">=</span> sc<span class=\"ansi-blue-fg\">.</span>textFile<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;s3://full-stack-bigdata-datasets/Big_Data/tears_in_rain_not_public.txt&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>text_rdd<span class=\"ansi-blue-fg\">.</span>take<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-red-fg\"># Note that you will only get the error after trying to perform an action, that&#39;s only then that you are trying to access the data.</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">take</span><span class=\"ansi-blue-fg\">(self, num)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1566</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">   1567</span>         items <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-fg\">-&gt; 1568</span><span class=\"ansi-red-fg\">         </span>totalParts <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>getNumPartitions<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1569</span>         partsScanned <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-cyan-fg\">0</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1570</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">getNumPartitions</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    467</span>         <span class=\"ansi-cyan-fg\">2</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    468</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 469</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>partitions<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>size<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    470</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    471</span>     <span class=\"ansi-green-fg\">def</span> filter<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> f<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    108</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    109</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 110</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    111</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    112</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o316.partitions.\n: java.nio.file.AccessDeniedException: s3://full-stack-bigdata-datasets/Big_Data/tears_in_rain_not_public.txt: getFileStatus on s3://full-stack-bigdata-datasets/Big_Data/tears_in_rain_not_public.txt: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden; request: HEAD https://full-stack-bigdata-datasets.s3.eu-west-3.amazonaws.com Big_Data/tears_in_rain_not_public.txt {} Hadoop 2.7.4, aws-sdk-java/1.11.655 Linux/5.4.0-1039-aws OpenJDK_64-Bit_Server_VM/25.282-b08 java/1.8.0_282 scala/2.12.10 vendor/Azul_Systems,_Inc. com.amazonaws.services.s3.model.GetObjectMetadataRequest; Request ID: 2B19GCQY0A6EVMBF, Extended Request ID: zzjmYTQrwdPCRzLKLYidAGG4jlapODg3u/qzgDuQ7X2b6EPxHNh6ura+8VPaBApEPcgg7U3npzw=, Cloud Provider: AWS, Instance ID: i-0d7c7f70557ae6b5a (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 2B19GCQY0A6EVMBF; S3 Extended Request ID: zzjmYTQrwdPCRzLKLYidAGG4jlapODg3u/qzgDuQ7X2b6EPxHNh6ura+8VPaBApEPcgg7U3npzw=), S3 Extended Request ID: zzjmYTQrwdPCRzLKLYidAGG4jlapODg3u/qzgDuQ7X2b6EPxHNh6ura+8VPaBApEPcgg7U3npzw=:403 Forbidden\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:243)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:154)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2854)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2824)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2763)\n\tat org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:57)\n\tat org.apache.hadoop.fs.Globber.glob(Globber.java:252)\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.globStatus(S3AFileSystem.java:3662)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:259)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:213)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:303)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:303)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:62)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:62)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden; request: HEAD https://full-stack-bigdata-datasets.s3.eu-west-3.amazonaws.com Big_Data/tears_in_rain_not_public.txt {} Hadoop 2.7.4, aws-sdk-java/1.11.655 Linux/5.4.0-1039-aws OpenJDK_64-Bit_Server_VM/25.282-b08 java/1.8.0_282 scala/2.12.10 vendor/Azul_Systems,_Inc. com.amazonaws.services.s3.model.GetObjectMetadataRequest; Request ID: 2B19GCQY0A6EVMBF, Extended Request ID: zzjmYTQrwdPCRzLKLYidAGG4jlapODg3u/qzgDuQ7X2b6EPxHNh6ura+8VPaBApEPcgg7U3npzw=, Cloud Provider: AWS, Instance ID: i-0d7c7f70557ae6b5a (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 2B19GCQY0A6EVMBF; S3 Extended Request ID: zzjmYTQrwdPCRzLKLYidAGG4jlapODg3u/qzgDuQ7X2b6EPxHNh6ura+8VPaBApEPcgg7U3npzw=), S3 Extended Request ID: zzjmYTQrwdPCRzLKLYidAGG4jlapODg3u/qzgDuQ7X2b6EPxHNh6ura+8VPaBApEPcgg7U3npzw=\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4926)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4872)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1321)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1569)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1566)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2845)\n\t... 31 more\n</div>",
       "errorSummary": "java.nio.file.AccessDeniedException: s3://full-stack-bigdata-datasets/Big_Data/tears_in_rain_not_public.txt: getFileStatus on s3://full-stack-bigdata-datasets/Big_Data/tears_in_rain_not_public.txt: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden; request: HEAD https://full-stack-bigdata-datasets.s3.eu-west-3.amazonaws.com Big_Data/tears_in_rain_not_public.txt {} Hadoop 2.7.4, aws-sdk-java/1.11.655 Linux/5.4.0-1039-aws OpenJDK_64-Bit_Server_VM/25.282-b08 java/1.8.0_282 scala/2.12.10 vendor/Azul_Systems,_Inc. com.amazonaws.services.s3.model.GetObjectMetadataRequest; Request ID: 2B19GCQY0A6EVMBF, Extended Request ID: zzjmYTQrwdPCRzLKLYidAGG4jlapODg3u/qzgDuQ7X2b6EPxHNh6ura+8VPaBApEPcgg7U3npzw=, Cloud Provider: AWS, Instance ID: i-0d7c7f70557ae6b5a (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 2B19GCQY0A6EVMBF; S3 Extended Request ID: zzjmYTQrwdPCRzLKLYidAGG4jlapODg3u/qzgDuQ7X2b6EPxHNh6ura+8VPaBApEPcgg7U3npzw=), S3 Extended Request ID: zzjmYTQrwdPCRzLKLYidAGG4jlapODg3u/qzgDuQ7X2b6EPxHNh6ura+8VPaBApEPcgg7U3npzw=:403 Forbidden",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# When you try to load a non public file you will get an error\n",
    "\n",
    "text_rdd = sc.textFile(\n",
    "    \"s3://full-stack-bigdata-datasets/Big_Data/tears_in_rain_not_public.txt\"\n",
    ")\n",
    "text_rdd.take(1)\n",
    "\n",
    "# Note that you will only get the error after trying to perform an action, that's only then that you are trying to access the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the file from `filepath` into a PySpark RDD into a `text_file` variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8ec58bdb-6d1a-4721-b137-466ff03a2c28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Print out `text_file`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fc11187b-5b3c-485f-93f3-acf9f9cd1500",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[3]: s3://full-stack-bigdata-datasets/Big_Data/tears_in_rain_not_public.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[3]: s3://full-stack-bigdata-datasets/Big_Data/tears_in_rain_not_public.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "baded0c8-c4f1-4dba-b64d-4c936fd2a82d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3. That doesn't tell us much, what would you do to see the first 3 elements of this RDD? Print out the first 3 elements of the `text_file`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0f9eea5a-284d-431e-ac2b-961f05892c67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[4]: [&#34;I&#39;ve seen things you people wouldn&#39;t believe. &#34;,\n",
       " &#39;Attack ships on fire off the shoulder of Orion. &#39;,\n",
       " &#39;I watched C-beams glitter in the dark near the Tannhäuser Gate. &#39;]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[4]: [&#34;I&#39;ve seen things you people wouldn&#39;t believe. &#34;,\n &#39;Attack ships on fire off the shoulder of Orion. &#39;,\n &#39;I watched C-beams glitter in the dark near the Tannhäuser Gate. &#39;]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "92efdd37-6d8c-40e6-884d-038a75216ca5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4. What's the type of `text_file`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7bd6b0e2-b603-4fef-ac82-9f12ddb664f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[5]: pyspark.rdd.RDD</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[5]: pyspark.rdd.RDD</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "05537fe5-2ce9-4738-866e-9c69d957bc12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It's a PySpark `RDD`. It means we can call **actions** on it and it will return a result.\n",
    "\n",
    "We want the results to be all elements of the `rdd`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. collect all elements of `text_file`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1e9900eb-7445-4023-ae4c-08a8f4fe3ae0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[6]: [&#34;I&#39;ve seen things you people wouldn&#39;t believe. &#34;,\n",
       " &#39;Attack ships on fire off the shoulder of Orion. &#39;,\n",
       " &#39;I watched C-beams glitter in the dark near the Tannhäuser Gate. &#39;,\n",
       " &#39;All those moments will be lost in time, like tears in rain. &#39;,\n",
       " &#39;Time to die.&#39;]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[6]: [&#34;I&#39;ve seen things you people wouldn&#39;t believe. &#34;,\n &#39;Attack ships on fire off the shoulder of Orion. &#39;,\n &#39;I watched C-beams glitter in the dark near the Tannhäuser Gate. &#39;,\n &#39;All those moments will be lost in time, like tears in rain. &#39;,\n &#39;Time to die.&#39;]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6fae9f22-2262-41d6-b897-a8ffff58ad50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "6. How many lines are there in `text_file`? Count the number of lines in `text_file`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "29820e71-848e-484d-9bbe-5175cf6061e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[7]: 5</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[7]: 5</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f6b0f906-5172-4f33-b66d-dc824fe8bf6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "7. What's the length of each sentence? Call `.map(...)` on your rdd and give it a function that computes the lenght of a string: `lineLengths`\n",
    "\n",
    "_NOTE: `lineLengths` is how you should name your result variable_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8bf5724c-ca0b-4e77-95f5-dadaa5cb6454",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Take the first 3 elements of lineLengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0ec7aa70-6332-407f-82cf-b874ca5a92c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[10]: [46, 48, 64]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[10]: [46, 48, 64]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Collect all elements of lineLenghts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e2427bd5-0e55-4229-8064-cb4cb11b5f3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[12]: [46, 48, 64, 60, 12]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[12]: [46, 48, 64, 60, 12]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b95695ec-05de-43e0-9032-44b796226afc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "10. What's the average length? Compute the average value of `lineLengths`: `avgLength`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4afefb48-74fc-40ae-a52f-61bda8923325",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. What's the type of `avgLength`? Print it out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b07832a2-7738-4961-bf5e-8252f435e395",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[12]: float</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[12]: float</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Print out `avgLength`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "488a9d40-f4ee-4700-a763-a61aa364eca1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[13]: 46.0</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[13]: 46.0</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0b2cb83f-7d5e-485d-b701-cacde5aa8b05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "13. Now we want to compute the total length of the document. Compute the sum of all `lineLengths`: `totalLength`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cd64a20c-9568-4cf6-8505-afb4102413c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. What's the type of `totalLength`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d750d4d5-0771-4f6c-9d6a-67665dd2c826",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[15]: int</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[15]: int</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Print out `totalLength`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b0f4e7d5-7731-450b-b521-b59bd677a0ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[16]: 230</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[16]: 230</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "095ee3c0-7f49-47f9-9bfe-d99649806b25",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Bonus: another way to compute the sum would be to use a `reducer`\n",
    "\n",
    "This is a step exercise to get you prepare for the next (optional) assignment.\n",
    "\n",
    "Your goal is to compute the sum of lineLenghts, just like we did, but this time using `.reduce(...)`.  \n",
    "Here is the link to the [documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Try to compute the total sum, but this time using `.reduce(...)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f5a37451-a781-44a5-8589-98026a090033",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[24]: 230</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[24]: 230</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01-First_steps_with_PySpark_RDD_solutions_databricks",
   "notebookOrigID": 2835273710881929,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fbc4d3870518eee81184ced0d2279c769a0eca59aab465c4e7ec13e5e6c47a3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
