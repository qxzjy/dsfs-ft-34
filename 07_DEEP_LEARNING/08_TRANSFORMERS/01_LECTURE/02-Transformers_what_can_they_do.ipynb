{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4W1FKdc3vAj"
      },
      "source": [
        "# Transformers, what can they do? üßôüèΩ‚Äç‚ôÇÔ∏è\n",
        "\n",
        "A lot! And this section is dedicated to show you very quickly as many usecases as possible using as little code as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pa0Axfk3vAl"
      },
      "source": [
        "## What will you learn in this course? üßêüßê\n",
        "\n",
        "In this section, we will look at what Transformer models can do and use our first tool from the ü§ó Transformers library: the `pipeline()` function. [For the full documentation click here](https://huggingface.co/docs/transformers/main_classes/pipelines)\n",
        "\n",
        "<Note type=\"tip\" title=\"Pipeline\">\n",
        "\n",
        "Pipeline let's you use all basic functionnalities of LLMs with a few lines of code, however you get little room for customization.\n",
        "\n",
        "</Note>\n",
        "\n",
        "## Working with pipelines\n",
        "\n",
        "The most basic object in the ü§ó Transformers library is the `pipeline()` function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:\n",
        "\n",
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ulDmRCP3vAl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (3.5.1)\n",
            "Requirement already satisfied: evaluate in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (0.4.3)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (2.2.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (3.11.18)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from transformers[sentencepiece]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from transformers[sentencepiece]) (6.30.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "#!pip install datasets evaluate \"transformers[sentencepiece]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sentiment analysis\n",
        "\n",
        "The sentiment analysis pipeline gives a score associated to a `POSITIVE` label, and a `NEGATIVE` label, it helps assessing the sentiment expressed in the input text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXQqFlFK3vAm",
        "outputId": "6b287cd3-2961-40a6-aad1-f8ee82a3d1ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598050713539124}]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tqdm as notebook_tqdm\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can even pass several sentences!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q_bUCasC3vAm",
        "outputId": "3f77adbc-36f6-4059-a258-e275cfd62545"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598050713539124},\n",
              " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier(\n",
        "    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English. The model is downloaded and cached when you create the `classifier` object. If you rerun the command, the cached model will be used instead and there is no need to download the model again.\n",
        "\n",
        "There are three main steps involved when you pass some text to a pipeline:\n",
        "\n",
        "1.  The text is preprocessed into a format the model can understand.\n",
        "2.  The preprocessed inputs are passed to the model.\n",
        "3.  The predictions of the model are post-processed, so you can make sense of them.\n",
        "\n",
        "Some of the currently [available pipelines](https://huggingface.co/transformers/main_classes/pipelines) are:\n",
        "\n",
        "-   `feature-extraction` (get the vector representation of a text)\n",
        "-   `fill-mask`\n",
        "-   `ner` (named entity recognition)\n",
        "-   `question-answering`\n",
        "-   `sentiment-analysis`\n",
        "-   `summarization`\n",
        "-   `text-generation`\n",
        "-   `translation`\n",
        "-   `zero-shot-classification`\n",
        "\n",
        "Let‚Äôs have a look at a few of these!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zero-shot classification\n",
        "\n",
        "We‚Äôll start by tackling a more challenging task where we need to classify texts that haven‚Äôt been labelled. This is a common scenario in real-world projects because annotating text is usually time-consuming and requires domain expertise. For this use case, the `zero-shot-classification` pipeline is very powerful: it allows you to specify which labels to use for the classification, so you don‚Äôt have to rely on the labels of the pretrained model. You‚Äôve already seen how the model can classify a sentence as positive or negative using those two labels ‚Äî but it can also classify the text using any other set of labels you like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xkRL69gs3vAm",
        "outputId": "344032da-a6bd-4415-94af-7ccb73e3374b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'sequence': 'This is a course about the Transformers library',\n",
              " 'labels': ['education', 'business', 'politics'],\n",
              " 'scores': [0.8445958495140076, 0.11197652667760849, 0.04342763125896454]}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "classifier(\n",
        "    \"This is a course about the Transformers library\",\n",
        "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This pipeline is called _zero-shot_ because you don‚Äôt need to fine-tune the model on your data to use it. It can directly return probability scores for any list of labels you want!\n",
        "\n",
        "‚úèÔ∏è **Try it out!** Play around with your own sequences and labels and see how the model behaves.\n",
        "\n",
        "### Text generation\n",
        "\n",
        "Now let‚Äôs see how to use a pipeline to generate some text. The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text. This is similar to the predictive text feature that is found on many phones. Text generation involves randomness, so it‚Äôs normal if you don‚Äôt get the same results as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Y52Pv_dM3vAm",
        "outputId": "ec1ec8a4-392d-40c4-d036-88a2be93dabe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use mps:0\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'In this course, we will teach you how to navigate the environment of the virtual real-time games world.\\n\\nHere you will learn how to create worlds that provide unique experiences for people of all ages. With these lessons in mind, prepare to'}]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\")\n",
        "generator(\"In this course, we will teach you how to\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We mentionned earlier that little customization is possible with pipeline, here's an example where we use a specific text-generation model, impose a max length and number of returned sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QnC00wQQ3vAm",
        "outputId": "64453aca-6feb-4375-ec26-d62c76002696"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'In this course, we will teach you how to code. We will explain the basics of writing programs that are not a regular feature, and we will'},\n",
              " {'generated_text': \"In this course, we will teach you how to control the growth of your own baby by controlling your child's ability to make babies. This will help\"}]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "generator(\n",
        "    \"In this course, we will teach you how to\",\n",
        "    max_length=30,\n",
        "    num_return_sequences=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fill Mask\n",
        "\n",
        "Fill mask replaces the `<mask>` token with the top k most porbable words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "STU6I4QU3vAn",
        "outputId": "7e4f21df-dace-4e34-8283-bc143c87432f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'score': 0.19619987905025482,\n",
              "  'token': 30412,\n",
              "  'token_str': ' mathematical',\n",
              "  'sequence': 'This course will teach you all about mathematical models.'},\n",
              " {'score': 0.04052761569619179,\n",
              "  'token': 38163,\n",
              "  'token_str': ' computational',\n",
              "  'sequence': 'This course will teach you all about computational models.'}]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\")\n",
        "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NER (Named Entity Recognition)\n",
        "\n",
        "As the name states, this usecase identifies the type and position of character sequences representing named entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TdoYIB3q3vAn",
        "outputId": "9a394596-05db-43a0-fa2c-019e5a70a895"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use mps:0\n",
            "/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER',\n",
              "  'score': np.float32(0.9981694),\n",
              "  'word': 'Sylvain',\n",
              "  'start': 11,\n",
              "  'end': 18},\n",
              " {'entity_group': 'ORG',\n",
              "  'score': np.float32(0.97960186),\n",
              "  'word': 'Hugging Face',\n",
              "  'start': 33,\n",
              "  'end': 45},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': np.float32(0.9932106),\n",
              "  'word': 'Brooklyn',\n",
              "  'start': 49,\n",
              "  'end': 57}]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ner = pipeline(\"ner\", grouped_entities=True)\n",
        "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question-answering\n",
        "\n",
        "This pipeline creates a question answering chat bot with the possibility of including context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "r6pJ_SwG3vAn",
        "outputId": "15d2dda9-8e55-4b7a-f768-b7c170f35dec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'score': 0.6949760913848877, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "question_answerer(\n",
        "    question=\"Where do I work?\",\n",
        "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summarization\n",
        "\n",
        "This pipeline summarizes the input text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KU-CIjXV3vAn",
        "outputId": "18dfa676-0b0e-4e17-a925-da12c8e26a1f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'summary_text': ' The number of engineering graduates in the United States has declined in recent years . China and India graduate six and eight times as many traditional engineers as the U.S. does . Rapidly developing economies such as China continue to encourage and advance the teaching of engineering . There are declining offerings in engineering subjects dealing with infrastructure, infrastructure, the environment, and related issues .'}]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\")\n",
        "summarizer(\n",
        "    \"\"\"\n",
        "    America has changed dramatically during recent years. Not only has the number of\n",
        "    graduates in traditional engineering disciplines such as mechanical, civil,\n",
        "    electrical, chemical, and aeronautical engineering declined, but in most of\n",
        "    the premier American universities engineering curricula now concentrate on\n",
        "    and encourage largely the study of engineering science. As a result, there\n",
        "    are declining offerings in engineering subjects dealing with infrastructure,\n",
        "    the environment, and related issues, and greater concentration on high\n",
        "    technology subjects, largely supporting increasingly complex scientific\n",
        "    developments. While the latter is important, it should not be at the expense\n",
        "    of more traditional engineering.\n",
        "\n",
        "    Rapidly developing economies such as China and India, as well as other\n",
        "    industrial countries in Europe and Asia, continue to encourage and advance\n",
        "    the teaching of engineering. Both China and India, respectively, graduate\n",
        "    six and eight times as many traditional engineers as does the United States.\n",
        "    Other industrial countries at minimum maintain their output, while America\n",
        "    suffers an increasingly serious decline in the number of engineering graduates\n",
        "    and a lack of well-educated engineers.\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Translation\n",
        "\n",
        "For translation tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rJVdJGQA3vAn",
        "outputId": "1dab222d-d299-488a-aed3-7c2db3c2425a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'translation_text': 'This course is produced by Hugging Face.'}]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
        "translator(\"Ce cours est produit par Hugging Face.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bias and limitations\n",
        "\n",
        "Be careful, LLMs are great, but make no mistake they have limitations, never forget that garbage in means garbage out. And, even if you use carefully curated data, models will still carry the biases of the society they were created in. Let's take a look at the eye-opening example below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
            "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "result = unmasker(\"This man works as a [MASK].\")\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n",
        "result = unmasker(\"This woman works as a [MASK].\")\n",
        "print([r[\"token_str\"] for r in result])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pipelines are the quickest, easiest way to get started with transformers.\n",
        "\n",
        "## Resources üìöüìö\n",
        "\n",
        "* [Pipeline documentation](https://huggingface.co/docs/transformers/main_classes/pipelines)\n",
        "* [Pipeline tutorial](https://huggingface.co/learn/nlp-course/chapter1/3?fw=pt)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Transformers, what can they do?",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
