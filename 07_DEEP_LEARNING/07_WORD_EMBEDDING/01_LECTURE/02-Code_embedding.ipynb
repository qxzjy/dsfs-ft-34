{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dl7xp6e34c1n"
   },
   "source": [
    "# Code Embedding\n",
    "\n",
    "Now that you have learned a little about the theory behind word embedding it is time you practice it. We'll also take this opportunity to teachyou how you can work with text data in Pytorch!\n",
    "\n",
    "## What will you learn in this course? üßêüßê\n",
    "\n",
    "This course is a code demonstration that will walk you through manipulating text data with pytorch as well as train a model with an embedding layer !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 217,
     "status": "ok",
     "timestamp": 1626857538635,
     "user": {
      "displayName": "Charles Tanguy",
      "photoUrl": "",
      "userId": "11930294859591867631"
     },
     "user_tz": -120
    },
    "id": "1s88l77e4HNe"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import tarfile\n",
    "import string\n",
    "import torch\n",
    "import tiktoken\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eJViPhxrdTDz"
   },
   "source": [
    "### Load and format the dataset\n",
    "\n",
    "We begin by loading the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48653,
     "status": "ok",
     "timestamp": 1626857597859,
     "user": {
      "displayName": "Charles Tanguy",
      "photoUrl": "",
      "userId": "11930294859591867631"
     },
     "user_tz": -120
    },
    "id": "3a7CUNdf8NIO",
    "outputId": "3dd4f803-8702-4dcd-df04-f316508dfb26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete.\n",
      "Extraction complete!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "output_path = \"aclImdb_v1.tar.gz\"\n",
    "extract_dir = \"./aclImdb\"\n",
    "def load_and_extract_files(url,output_path,extract_dir):\n",
    "    # Download the dataset\n",
    "    if not os.path.exists(output_path):\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(output_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "        print(\"Download complete.\")\n",
    "    if not os.path.exists(extract_dir):\n",
    "        # Extract all contents to a specific folder\n",
    "        with tarfile.open(output_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=extract_dir)  # Replace with your target folder\n",
    "\n",
    "        print(\"Extraction complete!\")\n",
    "\n",
    "load_and_extract_files(url,output_path,extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1952,
     "status": "ok",
     "timestamp": 1626857624288,
     "user": {
      "displayName": "Charles Tanguy",
      "photoUrl": "",
      "userId": "11930294859591867631"
     },
     "user_tz": -120
    },
    "id": "iKqgNPmB47Th"
   },
   "outputs": [],
   "source": [
    "remove_dir = os.path.join(extract_dir,\"aclImdb/train\", 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "M16hcEuuaQVm"
   },
   "source": [
    "We have imported data corresponding to film reviews organized in two categories: positive or negative. \n",
    "We'll load the text data into python variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_data(split=\"train\"):\n",
    "    data, labels = [], []\n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        folder_path = os.path.join(extract_dir,\"aclImdb/\",split,label)\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            with open(os.path.join(folder_path, file_name), \"r\", encoding=\"utf-8\") as f:\n",
    "                data.append(f.read())\n",
    "                labels.append(1 if label == \"pos\" else 0)\n",
    "    return data, labels\n",
    "\n",
    "train_texts, train_labels = load_imdb_data(\"train\")\n",
    "test_texts, test_labels = load_imdb_data(\"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wZM9Z-d3ciUv"
   },
   "source": [
    "Let's take a look at a sample of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2773,
     "status": "ok",
     "timestamp": 1626857953071,
     "user": {
      "displayName": "Charles Tanguy",
      "photoUrl": "",
      "userId": "11930294859591867631"
     },
     "user_tz": -120
    },
    "id": "N99nk0zP9hgm",
    "outputId": "0b0ee25b-aac5-4eb8-f056-8a7d09fa3b1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.\n"
     ]
    }
   ],
   "source": [
    "print( train_labels[0], train_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to turn text into sequences of indices ready to be embededded, we need a tokenizer. This is where a lot of the Language Model magic happens. We'll cover this in the next lecture.\n",
    "\n",
    "Here we use the `cl100k_base` tokenizer which is used in GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def encode_texts(texts):\n",
    "    return [tokenizer.encode(text) for text in texts]\n",
    "\n",
    "train_tokens = encode_texts(train_texts)\n",
    "test_tokens = encode_texts(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100277"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the vocabulary size is significant. However we can count on the fact that most of these token will not appear at all in our text dataset, so very few of these token will actually lead to parameter training at each step. Which means learning will not be slowed down by the total vocab size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build the data loader, we need all sequences to be of the same length.\n",
    "\n",
    "This is where truncating and padding comes in. Long sequences can be cut shorter, this called truncating. However shorter sequences have to be lengthened, we can fill the missing spots with `0` until the sequence reaches the desired length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(297.07392)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How are sequence lengths distributed?\n",
    "seq_lens = [len(seq) for seq in train_tokens]\n",
    "np.mean(seq_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average length is arount 300 tokens. Let's use this as our sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, max_length=300):\n",
    "    return [seq[:max_length] + [0] * (max_length - len(seq)) for seq in sequences]\n",
    "\n",
    "train_tokens = pad_sequences(train_tokens)\n",
    "test_tokens = pad_sequences(test_tokens)\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = torch.tensor(texts, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "df_dataset = IMDBDataset(train_tokens, train_labels)\n",
    "test_dataset = IMDBDataset(test_tokens, test_labels)\n",
    "\n",
    "# Split dataset into training (80%) and validation (20%)\n",
    "train_size = int(0.8 * len(df_dataset))\n",
    "val_size = len(df_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(df_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17773,   383, 14355,  ...,  1109,   430,    13],\n",
      "        [ 3915, 10307,  1193,  ...,     0,  3639,   264],\n",
      "        [74627,   380,   676,  ...,   832,    11,  8051],\n",
      "        ...,\n",
      "        [ 2028,  1633, 15234,  ...,     0,     0,     0],\n",
      "        [12331, 12490,  3320,  ...,     0,     0,     0],\n",
      "        [   40,   574, 14243,  ...,    11,   779,  1364]])\n"
     ]
    }
   ],
   "source": [
    "text, label = next(iter(train_loader))\n",
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "18MTWm4thciT"
   },
   "source": [
    "### Embedding layer\n",
    "\n",
    "Pytorch makes it really easy to use an embedding layer. It can be seen as a look up table that's mapping encoded tokens to their vector representation. It takes two main arguments: `num_embeddings` the number of unique tokens in the input vocabulary, and `embedding_dim` the number of components for representing the words. We should also use `padding_idx=0` to indicate that `0`is not a token but only padding that should be ignored during gradient descent.\n",
    "\n",
    "The input data for the embedding layer already needs to be encoded, meaning that each token needs to be replaced by an integer. Let's give an example of how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 217,
     "status": "ok",
     "timestamp": 1626858100737,
     "user": {
      "displayName": "Charles Tanguy",
      "photoUrl": "",
      "userId": "11930294859591867631"
     },
     "user_tz": -120
    },
    "id": "xSV5g5D1CDg2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size,\n",
    "                               embedding_dim=5, \n",
    "                               padding_idx=0)\n",
    "# We create a random list of three integers and use it as input for the embedding\n",
    "# layer and take a look at the output.\n",
    "sample_input = torch.tensor([[1, 2, 3]])\n",
    "embedded_output = embedding_layer(sample_input)\n",
    "print(embedded_output.shape)  # Output: (3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5571, -0.8905, -0.1998, -0.6145,  0.0438],\n",
       "         [ 1.0645,  1.4452,  0.5393,  0.0646,  0.3227],\n",
       "         [-1.7981, -0.3515,  0.2251,  0.6581,  0.8487]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The result is a collection of three 5-dimensional vectors. Each element in the \n",
    "# original list has been replaced by a vector of 5 floating points.\n",
    "embedded_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "umpFU7JjlTNG"
   },
   "source": [
    "More generally the embedding layer takes an input with dimension `(batch_size, sequence_length)`, and ouputs an object with dimensions `(batch_size, sequence_length, output_dim)`.\n",
    "\n",
    "The number of parameters in an embedding layer is equal to `input_dim * output_dim`, every unique token in the vocaulary maps to a unique vector with `output_dim` components, each component being a parameter of the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1626858280306,
     "user": {
      "displayName": "Charles Tanguy",
      "photoUrl": "",
      "userId": "11930294859591867631"
     },
     "user_tz": -120
    },
    "id": "j85AgTZ7mKst",
    "outputId": "9d980380-606e-48c4-a0a6-ec51389aea3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100277, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The shape of the trainable_variables attribute confirms what we just explained.\n",
    "embedding_layer.weight.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7o5Net_am6j4"
   },
   "source": [
    "### Text Classification\n",
    "\n",
    "Now that we have built the vectorization layer, we are ready to build the model. We will build a very simple classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1626858765766,
     "user": {
      "displayName": "Charles Tanguy",
      "photoUrl": "",
      "userId": "11930294859591867631"
     },
     "user_tz": -120
    },
    "id": "L-1U8eAh2rFN"
   },
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        pooled = self.pooling(embedded.permute(0, 2, 1)).squeeze(2)\n",
    "        return torch.sigmoid(self.fc(pooled))\n",
    "\n",
    "model = TextClassifier(vocab_size=vocab_size,\n",
    "                      embed_dim=16, \n",
    "                      num_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextClassifier(\n",
      "  (embedding): Embedding(100277, 16, padding_idx=0)\n",
      "  (pooling): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TextClassifier                           [1, 1]                    --\n",
       "‚îú‚îÄEmbedding: 1-1                         [1, 3, 16]                1,604,432\n",
       "‚îú‚îÄAdaptiveAvgPool1d: 1-2                 [1, 16, 1]                --\n",
       "‚îú‚îÄLinear: 1-3                            [1, 1]                    17\n",
       "==========================================================================================\n",
       "Total params: 1,604,449\n",
       "Trainable params: 1,604,449\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 1.60\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 6.42\n",
       "Estimated Total Size (MB): 6.42\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Print model summary\n",
    "summary(model, input_data=sample_input)  # (batch_size, input_features)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AeB8sFzqj_Wu"
   },
   "source": [
    "We'll then compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1626858812491,
     "user": {
      "displayName": "Charles Tanguy",
      "photoUrl": "",
      "userId": "11930294859591867631"
     },
     "user_tz": -120
    },
    "id": "aVqh5M_O3sEu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6828, Acc: 0.6133, Val Loss: 0.6632, Val Acc: 0.6812\n",
      "Epoch [2/10], Loss: 0.6098, Acc: 0.7559, Val Loss: 0.5644, Val Acc: 0.7676\n",
      "Epoch [3/10], Loss: 0.4982, Acc: 0.8159, Val Loss: 0.4771, Val Acc: 0.8074\n",
      "Epoch [4/10], Loss: 0.4106, Acc: 0.8563, Val Loss: 0.4165, Val Acc: 0.8362\n",
      "Epoch [5/10], Loss: 0.3505, Acc: 0.8786, Val Loss: 0.3795, Val Acc: 0.8492\n",
      "Epoch [6/10], Loss: 0.3085, Acc: 0.8934, Val Loss: 0.3557, Val Acc: 0.8578\n",
      "Epoch [7/10], Loss: 0.2767, Acc: 0.9051, Val Loss: 0.3405, Val Acc: 0.8628\n",
      "Epoch [8/10], Loss: 0.2510, Acc: 0.9146, Val Loss: 0.3278, Val Acc: 0.8654\n",
      "Epoch [9/10], Loss: 0.2290, Acc: 0.9223, Val Loss: 0.3202, Val Acc: 0.8658\n",
      "Epoch [10/10], Loss: 0.2099, Acc: 0.9291, Val Loss: 0.3131, Val Acc: 0.8680\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, epochs=100):\n",
    "    \"\"\"\n",
    "    Function to train a PyTorch model with training and validation datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    model: The neural network model to train.\n",
    "    train_loader: DataLoader for the training dataset.\n",
    "    val_loader: DataLoader for the validation dataset.\n",
    "    criterion: Loss function (e.g., Binary Cross Entropy for classification).\n",
    "    optimizer: Optimization algorithm (e.g., Adam, SGD).\n",
    "    epochs: Number of training epochs (default=100).\n",
    "    \n",
    "    Returns:\n",
    "    history: Dictionary containing loss and accuracy for both training and validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dictionary to store training & validation loss and accuracy over epochs\n",
    "    history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
    "    \n",
    "    for epoch in range(epochs):  # Loop over the number of epochs\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss, correct = 0, 0  # Initialize total loss and correct predictions\n",
    "        \n",
    "        # Training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()  # Reset gradients before each batch\n",
    "            outputs = model(inputs).squeeze()  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation (compute gradients)\n",
    "            optimizer.step()  # Update model parameters\n",
    "            \n",
    "            total_loss += loss.item()  # Accumulate batch loss\n",
    "            correct += ((outputs > 0.5) == labels).sum().item()  # Count correct predictions\n",
    "        \n",
    "        # Compute average loss and accuracy for training\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation phase (without gradient computation)\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss, val_correct = 0, 0\n",
    "        with torch.no_grad():  # No need to compute gradients during validation\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs).squeeze()  # Forward pass\n",
    "                loss = criterion(outputs, labels)  # Compute loss\n",
    "                val_loss += loss.item()  # Accumulate validation loss\n",
    "                val_correct += ((outputs > 0.5) == labels).sum().item()  # Count correct predictions\n",
    "        \n",
    "        # Compute average loss and accuracy for validation\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "        \n",
    "        # Store metrics in history dictionary\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['accuracy'].append(train_acc)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "        \n",
    "        # Print training progress\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    return history  # Return training history\n",
    "\n",
    "history = train(model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer,\n",
    "                epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rLFtHlagq5Hq"
   },
   "source": [
    "We have successfully trained an embedding layer! \n",
    "\n",
    "An important note! Embedding layers are trainable, therefore in this case, the tokens are mapped to embedding vectors so as to best determine which review is positive or negative, this embedding would not necessarily work well for a recipe classication problem or translation because of its specificity. The upside from this is that pre-trained word embeddings exist, and may be used for transfer learning!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyObiEB84YGkS4MHGEvC/MUI",
   "collapsed_sections": [],
   "name": "Code_embedding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
