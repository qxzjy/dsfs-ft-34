{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEa8O_OU8RO_"
      },
      "source": [
        "# Embedding for Sentiment Analysis \n",
        "\n",
        "Now that we know how word embedding works, we'll apply it to a supervised problem of sentiment analysis. The idea is to classify the comments left by users according to the number of stars they gave the Disneyland resort park in their reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlZE3Ntm8gv4"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "### Import Data \n",
        "\n",
        "1. Import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMBnbVdZygEr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using mps device\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import torch\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchinfo import summary\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIiJ3wIKkC_c"
      },
      "source": [
        "2. Copy the link below and read the file it contains with `pandas`.\n",
        "\n",
        "* https://go.aws/314bBDq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "Irik4DyW0acS",
        "outputId": "de9e2a19-203f-40c1-99ef-e4fa34f45edf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>review</th>\n",
              "      <th>stars</th>\n",
              "      <th>date_format</th>\n",
              "      <th>time_of_day</th>\n",
              "      <th>hour_of_day</th>\n",
              "      <th>day_of_week</th>\n",
              "      <th>review_format</th>\n",
              "      <th>review_lang</th>\n",
              "      <th>month_year</th>\n",
              "      <th>review_len</th>\n",
              "      <th>review_nb_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>efb62a167fee5cf3678b24427de8e31f</td>\n",
              "      <td>Génial, fabuleux, exceptionnel ! J'aimerais qu...</td>\n",
              "      <td>5</td>\n",
              "      <td>2017-09-29 18:17:00</td>\n",
              "      <td>18:17</td>\n",
              "      <td>18</td>\n",
              "      <td>Ven</td>\n",
              "      <td>génial  fabuleux  exceptionnel   j aimerais qu...</td>\n",
              "      <td>french</td>\n",
              "      <td>2017-09</td>\n",
              "      <td>115</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>e3be4f9c9e0b9572bfb2a5f88497bb14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>2017-09-29 17:29:00</td>\n",
              "      <td>17:29</td>\n",
              "      <td>17</td>\n",
              "      <td>Ven</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2017-09</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1b8e5760162d867e9b9ca80f645bdc60</td>\n",
              "      <td>Toujours aussi magic, féerique !</td>\n",
              "      <td>5</td>\n",
              "      <td>2017-09-29 16:46:00</td>\n",
              "      <td>16:46</td>\n",
              "      <td>16</td>\n",
              "      <td>Ven</td>\n",
              "      <td>toujours aussi magic  féerique</td>\n",
              "      <td>french</td>\n",
              "      <td>2017-09</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>fa330e5891a1bb486c3e9bf95c098726</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>2017-09-29 15:52:00</td>\n",
              "      <td>15:52</td>\n",
              "      <td>15</td>\n",
              "      <td>Ven</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2017-09</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>c1a693206aee1a2412d4bd9e45b80ec5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>2017-09-29 15:29:00</td>\n",
              "      <td>15:29</td>\n",
              "      <td>15</td>\n",
              "      <td>Ven</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2017-09</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299630</th>\n",
              "      <td>299be03d0583edfb9625a7947fbc631a</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>2012-11-11 11:46:00</td>\n",
              "      <td>11:46</td>\n",
              "      <td>11</td>\n",
              "      <td>Dim</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2012-11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299631</th>\n",
              "      <td>39b4e66e3b78d4f8ce60a6b4801b862d</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>2012-11-11 11:46:00</td>\n",
              "      <td>11:46</td>\n",
              "      <td>11</td>\n",
              "      <td>Dim</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2012-11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299632</th>\n",
              "      <td>924eb5ec58470cd00c16060e6ee3c316</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>2012-11-11 11:46:00</td>\n",
              "      <td>11:46</td>\n",
              "      <td>11</td>\n",
              "      <td>Dim</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2012-11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299633</th>\n",
              "      <td>5b484e48319355c12a941577d74a5839</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>2012-11-11 11:45:00</td>\n",
              "      <td>11:45</td>\n",
              "      <td>11</td>\n",
              "      <td>Dim</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2012-11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299634</th>\n",
              "      <td>d93cedcc610f80cd7cea3420e2d67977</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>2012-11-11 11:45:00</td>\n",
              "      <td>11:45</td>\n",
              "      <td>11</td>\n",
              "      <td>Dim</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2012-11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>299635 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 user_id  \\\n",
              "0       efb62a167fee5cf3678b24427de8e31f   \n",
              "1       e3be4f9c9e0b9572bfb2a5f88497bb14   \n",
              "2       1b8e5760162d867e9b9ca80f645bdc60   \n",
              "3       fa330e5891a1bb486c3e9bf95c098726   \n",
              "4       c1a693206aee1a2412d4bd9e45b80ec5   \n",
              "...                                  ...   \n",
              "299630  299be03d0583edfb9625a7947fbc631a   \n",
              "299631  39b4e66e3b78d4f8ce60a6b4801b862d   \n",
              "299632  924eb5ec58470cd00c16060e6ee3c316   \n",
              "299633  5b484e48319355c12a941577d74a5839   \n",
              "299634  d93cedcc610f80cd7cea3420e2d67977   \n",
              "\n",
              "                                                   review  stars  \\\n",
              "0       Génial, fabuleux, exceptionnel ! J'aimerais qu...      5   \n",
              "1                                                     NaN      2   \n",
              "2                        Toujours aussi magic, féerique !      5   \n",
              "3                                                     NaN      5   \n",
              "4                                                     NaN      3   \n",
              "...                                                   ...    ...   \n",
              "299630                                                NaN      5   \n",
              "299631                                                NaN      5   \n",
              "299632                                                NaN      5   \n",
              "299633                                                NaN      5   \n",
              "299634                                                NaN      5   \n",
              "\n",
              "                date_format time_of_day  hour_of_day day_of_week  \\\n",
              "0       2017-09-29 18:17:00       18:17           18         Ven   \n",
              "1       2017-09-29 17:29:00       17:29           17         Ven   \n",
              "2       2017-09-29 16:46:00       16:46           16         Ven   \n",
              "3       2017-09-29 15:52:00       15:52           15         Ven   \n",
              "4       2017-09-29 15:29:00       15:29           15         Ven   \n",
              "...                     ...         ...          ...         ...   \n",
              "299630  2012-11-11 11:46:00       11:46           11         Dim   \n",
              "299631  2012-11-11 11:46:00       11:46           11         Dim   \n",
              "299632  2012-11-11 11:46:00       11:46           11         Dim   \n",
              "299633  2012-11-11 11:45:00       11:45           11         Dim   \n",
              "299634  2012-11-11 11:45:00       11:45           11         Dim   \n",
              "\n",
              "                                            review_format review_lang  \\\n",
              "0       génial  fabuleux  exceptionnel   j aimerais qu...      french   \n",
              "1                                                     NaN         NaN   \n",
              "2                        toujours aussi magic  féerique        french   \n",
              "3                                                     NaN         NaN   \n",
              "4                                                     NaN         NaN   \n",
              "...                                                   ...         ...   \n",
              "299630                                                NaN         NaN   \n",
              "299631                                                NaN         NaN   \n",
              "299632                                                NaN         NaN   \n",
              "299633                                                NaN         NaN   \n",
              "299634                                                NaN         NaN   \n",
              "\n",
              "       month_year  review_len  review_nb_words  \n",
              "0         2017-09         115               19  \n",
              "1         2017-09           0                0  \n",
              "2         2017-09          32                4  \n",
              "3         2017-09           0                0  \n",
              "4         2017-09           0                0  \n",
              "...           ...         ...              ...  \n",
              "299630    2012-11           0                0  \n",
              "299631    2012-11           0                0  \n",
              "299632    2012-11           0                0  \n",
              "299633    2012-11           0                0  \n",
              "299634    2012-11           0                0  \n",
              "\n",
              "[299635 rows x 12 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import dataset with Pandas \n",
        "all_comments = pd.read_csv(\"https://go.aws/314bBDq\", encoding=\"utf-8\")\n",
        "all_comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_SUNZnzldIC"
      },
      "source": [
        "3. We will need the reviews in French. Filter the reviews so that they are in the right language. For this you need to find a column that gives you that information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As6OupSYl5La"
      },
      "source": [
        "4. Keep only the `review` & `stars` columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "lXLZFpg50swK",
        "outputId": "142e5043-436a-49c2-9176-06aaead0a721"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>stars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Génial, fabuleux, exceptionnel ! J'aimerais qu...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Toujours aussi magic, féerique !</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>En vacances en région parisienne nous nous som...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Tropbeaufinalpleinlesyeuxoreil</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>L'univers Disney reste merveilleux. Toutefois ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295057</th>\n",
              "      <td>Toujours aussi magique même si à la fin du séj...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295549</th>\n",
              "      <td>Séjour au top!!! Mes enfants les plus heureux ...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298475</th>\n",
              "      <td>Magnifique un monde parfait &lt;span class=\"\"\"\"_4...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298832</th>\n",
              "      <td>Oui j'ai aimé  car j'adore disney et tout ce q...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299402</th>\n",
              "      <td>Je vais à Disney minimum 1 fois par saison car...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8474 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   review  stars\n",
              "0       Génial, fabuleux, exceptionnel ! J'aimerais qu...      5\n",
              "2                        Toujours aussi magic, féerique !      5\n",
              "11      En vacances en région parisienne nous nous som...      2\n",
              "12                         Tropbeaufinalpleinlesyeuxoreil      5\n",
              "23      L'univers Disney reste merveilleux. Toutefois ...      4\n",
              "...                                                   ...    ...\n",
              "295057  Toujours aussi magique même si à la fin du séj...      5\n",
              "295549  Séjour au top!!! Mes enfants les plus heureux ...      5\n",
              "298475  Magnifique un monde parfait <span class=\"\"\"\"_4...      5\n",
              "298832  Oui j'ai aimé  car j'adore disney et tout ce q...      4\n",
              "299402  Je vais à Disney minimum 1 fois par saison car...      5\n",
              "\n",
              "[8474 rows x 2 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Taking only french reviews\n",
        "# Let's take the columns we're interested in \n",
        "dataset = all_comments.loc[all_comments[\"review_lang\"]==\"french\", [\"review\", \"stars\"]]\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8Duro5f8i7U"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "We will now go through a preprocessing phase. The goal is to convert the character strings into sequences of tokens represented by integers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1khjji0HMvk"
      },
      "source": [
        "1. Use the tiktoken library in order to tokenize each sentence based on the `cl100k_base` tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[38, 10610, 532, 11, 9765, 1130, 2249, 11, 4788, 8301]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "dataset_tokenized = [tokenizer.encode(text) for text in dataset[\"review\"]]\n",
        "\n",
        "# print the first ten tokens of the first tokenized sentence\n",
        "dataset_tokenized[0][:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBnV02P_HYHB"
      },
      "source": [
        "2. In order to build the data loader, we need all sequences to be of the same length. Calculate the max and average senquence length, and decide which length you want all sequences to adopt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8474"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# How are sequence lengths distributed?\n",
        "seq_lens = [len(seq) for seq in dataset_tokenized]\n",
        "print(\"avg seq len\",np.mean(seq_lens))\n",
        "print(\"max seq len\",np.max(seq_lens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pad_sequences(sequences, max_length=100):\n",
        "    return [seq[:max_length] + [0] * (max_length - len(seq)) for seq in sequences]\n",
        "\n",
        "dataset_tokenized = pad_sequences(dataset_tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQL-RX0KHb-F"
      },
      "source": [
        "3. Form a torch dataset object based on the token sequences and labels, and split the data into a train and validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "TZNd26-z1xIT"
      },
      "outputs": [],
      "source": [
        "# Define a custom PyTorch dataset class for Disney reviews\n",
        "class DisneyDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom dataset class for Disney reviews.\n",
        "\n",
        "    This class is used to convert text data (already tokenized) and their corresponding labels\n",
        "    into a PyTorch Dataset object, which can be easily loaded into a DataLoader.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels):\n",
        "        \"\"\"\n",
        "        Initializes the dataset by storing texts and labels as PyTorch tensors.\n",
        "\n",
        "        Args:\n",
        "        - texts (list or numpy array): Tokenized text data, where each text has been converted \n",
        "                                       into a sequence of word indices (integer tokens).\n",
        "        - labels (list or numpy array): The corresponding labels for each text (e.g., sentiment scores or star ratings).\n",
        "        \"\"\"\n",
        "        # Convert text sequences to a PyTorch tensor (long type since they are indices)\n",
        "        self.texts = torch.tensor(texts, dtype=torch.long)\n",
        "\n",
        "        # Convert labels to a PyTorch tensor (float32 for compatibility with loss functions)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of samples in the dataset.\n",
        "\n",
        "        This method is required for PyTorch datasets as it allows DataLoader to determine\n",
        "        how many batches it needs.\n",
        "        \"\"\"\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves a single data point (text and label) from the dataset based on an index.\n",
        "\n",
        "        Args:\n",
        "        - idx (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "        - tuple: A tuple containing:\n",
        "            - self.texts[idx]: The tokenized text at index `idx`.\n",
        "            - self.labels[idx]: The corresponding label for that text.\n",
        "        \"\"\"\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "    \n",
        "label = dataset[\"stars\"]\n",
        "\n",
        "# Example usage: Creating a dataset instance\n",
        "disney_dataset = DisneyDataset(dataset_tokenized, label)\n",
        "\n",
        "# Split dataset into training (80%) and validation (20%)\n",
        "train_size = int(0.8 * len(disney_dataset))\n",
        "val_size = len(disney_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(disney_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([5., 5., 5., 2., 4., 5., 4., 4., 4., 4., 5., 5., 5., 5., 4., 5., 5., 4.,\n",
            "        5., 1., 4., 4., 3., 4., 4., 3., 5., 5., 4., 5., 5., 4.])\n",
            "tensor([[  5479,   1880,   4983,  ...,      0,      0,      0],\n",
            "        [   806,     14,    806,  ...,      0,      0,      0],\n",
            "        [ 66932,  57038,  81621,  ...,      0,      0,      0],\n",
            "        ...,\n",
            "        [ 30854,   3355,    729,  ..., 100164,    758,  27530],\n",
            "        [    19,  49301,   1522,  ...,      0,      0,      0],\n",
            "        [ 30854,  36731,  12584,  ...,    324,    662,  34447]])\n"
          ]
        }
      ],
      "source": [
        "text, label = next(iter(train_loader))\n",
        "\n",
        "print(label)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the embedding based prediction model\n",
        "\n",
        "Now that the data is duely tokenized, let's create a prediction model based on the embedding layer.\n",
        "\n",
        "1. The first question you need to ask yourself is what kind of prediction problem are we dealing with? The target variable represents the number of stars associated with each comment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Treating this as a regression problem seems relevant for two reasons :\n",
        "- The target variable is qualitative ordinal, therefore values of stars can be compared\n",
        "- This would help the model associate tokens with quantitative measures on only one dimension (as opposed to 5 dimensions in the case of classification) observations associated with each number of stars will actually benefit the training for all values of stars."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Build a prediction model based on your choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the vocabulary size from the tokenizer\n",
        "# This represents the total number of unique words in the dataset,\n",
        "# which will be used as the input size for the embedding layer.\n",
        "\n",
        "# Define a neural network model for text regression\n",
        "class TextRegressor(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple text regression model using embeddings and pooling.\n",
        "\n",
        "    This model takes tokenized text as input and predicts a continuous value (e.g., sentiment score or rating).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        \"\"\"\n",
        "        Initializes the model layers.\n",
        "\n",
        "        Args:\n",
        "        - vocab_size (int): The number of unique words in the vocabulary.\n",
        "        - embed_dim (int): The size of each word's embedding vector.\n",
        "\n",
        "        The model consists of:\n",
        "        1. An Embedding layer that converts tokenized words into dense vectors.\n",
        "        2. A Pooling layer that reduces the sequence length by averaging word embeddings.\n",
        "        3. A Fully Connected (Linear) layer that maps the pooled embeddings to the output value.\n",
        "        \"\"\"\n",
        "        super(TextRegressor, self).__init__()\n",
        "\n",
        "        # Embedding layer: Maps word indices to dense vector representations\n",
        "        # padding_idx=0 ensures that padding tokens (index 0) do not contribute to learning\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "\n",
        "        # Adaptive Average Pooling: Computes the average of the word embeddings along the sequence length\n",
        "        # This helps reduce variable-length text into a fixed-size representation\n",
        "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Fully Connected (Linear) layer: Maps the fixed-size vector to a single output value\n",
        "        self.fc = nn.Linear(embed_dim, 1)\n",
        "\n",
        "    def forward(self, text):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "        - text (Tensor): A batch of tokenized text (word indices).\n",
        "\n",
        "        Returns:\n",
        "        - Tensor: The predicted output (e.g., a continuous score or rating).\n",
        "        \"\"\"\n",
        "        # Convert input word indices into dense embeddings\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        # Permute to match the expected shape for pooling: (batch, channels, sequence_length)\n",
        "        # Then, apply average pooling to reduce sequence length to 1\n",
        "        pooled = self.pooling(embedded.permute(0, 2, 1)).squeeze(2)\n",
        "\n",
        "        # Pass the pooled embeddings through the linear layer to\n",
        "        return self.fc(pooled)\n",
        "    \n",
        "vocab_size = tokenizer.n_vocab\n",
        "\n",
        "# Create an instance of the model\n",
        "model = TextRegressor(vocab_size=vocab_size, embed_dim=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Print out the sructure of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TextRegressor(\n",
            "  (embedding): Embedding(100277, 16, padding_idx=0)\n",
            "  (pooling): AdaptiveAvgPool1d(output_size=1)\n",
            "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "TextRegressor                            [32, 1]                   --\n",
              "├─Embedding: 1-1                         [32, 100, 16]             1,604,432\n",
              "├─AdaptiveAvgPool1d: 1-2                 [32, 16, 1]               --\n",
              "├─Linear: 1-3                            [32, 1]                   17\n",
              "==========================================================================================\n",
              "Total params: 1,604,449\n",
              "Trainable params: 1,604,449\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 51.34\n",
              "==========================================================================================\n",
              "Input size (MB): 0.03\n",
              "Forward/backward pass size (MB): 0.41\n",
              "Params size (MB): 6.42\n",
              "Estimated Total Size (MB): 6.85\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(model)\n",
        "\n",
        "# Print model summary\n",
        "summary(model, input_data=text)  # (batch_size, input_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Prepare and run the training loop for 50 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Train Loss: 10.7488, Train metric: 3.2785, Val Loss: 7.9421, Val metric: 2.8182\n",
            "Epoch [2/50], Train Loss: 6.8957, Train metric: 2.6260, Val Loss: 5.9919, Val metric: 2.4478\n",
            "Epoch [3/50], Train Loss: 5.8676, Train metric: 2.4223, Val Loss: 5.3846, Val metric: 2.3205\n",
            "Epoch [4/50], Train Loss: 5.2974, Train metric: 2.3016, Val Loss: 4.8717, Val metric: 2.2072\n",
            "Epoch [5/50], Train Loss: 4.7605, Train metric: 2.1819, Val Loss: 4.3760, Val metric: 2.0919\n",
            "Epoch [6/50], Train Loss: 4.2482, Train metric: 2.0611, Val Loss: 3.9112, Val metric: 1.9777\n",
            "Epoch [7/50], Train Loss: 3.7650, Train metric: 1.9404, Val Loss: 3.4780, Val metric: 1.8649\n",
            "Epoch [8/50], Train Loss: 3.3210, Train metric: 1.8224, Val Loss: 3.0939, Val metric: 1.7590\n",
            "Epoch [9/50], Train Loss: 2.9286, Train metric: 1.7113, Val Loss: 2.7653, Val metric: 1.6629\n",
            "Epoch [10/50], Train Loss: 2.5929, Train metric: 1.6102, Val Loss: 2.4912, Val metric: 1.5783\n",
            "Epoch [11/50], Train Loss: 2.3139, Train metric: 1.5211, Val Loss: 2.2698, Val metric: 1.5066\n",
            "Epoch [12/50], Train Loss: 2.0837, Train metric: 1.4435, Val Loss: 2.0843, Val metric: 1.4437\n",
            "Epoch [13/50], Train Loss: 1.8901, Train metric: 1.3748, Val Loss: 1.9291, Val metric: 1.3889\n",
            "Epoch [14/50], Train Loss: 1.7264, Train metric: 1.3139, Val Loss: 1.7958, Val metric: 1.3401\n",
            "Epoch [15/50], Train Loss: 1.5818, Train metric: 1.2577, Val Loss: 1.6772, Val metric: 1.2951\n",
            "Epoch [16/50], Train Loss: 1.4558, Train metric: 1.2066, Val Loss: 1.5721, Val metric: 1.2538\n",
            "Epoch [17/50], Train Loss: 1.3438, Train metric: 1.1592, Val Loss: 1.4783, Val metric: 1.2159\n",
            "Epoch [18/50], Train Loss: 1.2428, Train metric: 1.1148, Val Loss: 1.3941, Val metric: 1.1807\n",
            "Epoch [19/50], Train Loss: 1.1527, Train metric: 1.0736, Val Loss: 1.3194, Val metric: 1.1487\n",
            "Epoch [20/50], Train Loss: 1.0724, Train metric: 1.0356, Val Loss: 1.2519, Val metric: 1.1189\n",
            "Epoch [21/50], Train Loss: 0.9999, Train metric: 0.9999, Val Loss: 1.1915, Val metric: 1.0916\n",
            "Epoch [22/50], Train Loss: 0.9358, Train metric: 0.9674, Val Loss: 1.1404, Val metric: 1.0679\n",
            "Epoch [23/50], Train Loss: 0.8791, Train metric: 0.9376, Val Loss: 1.0946, Val metric: 1.0462\n",
            "Epoch [24/50], Train Loss: 0.8281, Train metric: 0.9100, Val Loss: 1.0519, Val metric: 1.0256\n",
            "Epoch [25/50], Train Loss: 0.7827, Train metric: 0.8847, Val Loss: 1.0149, Val metric: 1.0074\n",
            "Epoch [26/50], Train Loss: 0.7432, Train metric: 0.8621, Val Loss: 0.9841, Val metric: 0.9920\n",
            "Epoch [27/50], Train Loss: 0.7077, Train metric: 0.8413, Val Loss: 0.9570, Val metric: 0.9782\n",
            "Epoch [28/50], Train Loss: 0.6764, Train metric: 0.8224, Val Loss: 0.9344, Val metric: 0.9667\n",
            "Epoch [29/50], Train Loss: 0.6485, Train metric: 0.8053, Val Loss: 0.9134, Val metric: 0.9557\n",
            "Epoch [30/50], Train Loss: 0.6237, Train metric: 0.7897, Val Loss: 0.8979, Val metric: 0.9476\n",
            "Epoch [31/50], Train Loss: 0.6016, Train metric: 0.7757, Val Loss: 0.8843, Val metric: 0.9404\n",
            "Epoch [32/50], Train Loss: 0.5818, Train metric: 0.7627, Val Loss: 0.8744, Val metric: 0.9351\n",
            "Epoch [33/50], Train Loss: 0.5640, Train metric: 0.7510, Val Loss: 0.8624, Val metric: 0.9287\n",
            "Epoch [34/50], Train Loss: 0.5476, Train metric: 0.7400, Val Loss: 0.8540, Val metric: 0.9241\n",
            "Epoch [35/50], Train Loss: 0.5327, Train metric: 0.7299, Val Loss: 0.8460, Val metric: 0.9198\n",
            "Epoch [36/50], Train Loss: 0.5188, Train metric: 0.7203, Val Loss: 0.8424, Val metric: 0.9178\n",
            "Epoch [37/50], Train Loss: 0.5065, Train metric: 0.7117, Val Loss: 0.8377, Val metric: 0.9153\n",
            "Epoch [38/50], Train Loss: 0.4947, Train metric: 0.7033, Val Loss: 0.8344, Val metric: 0.9134\n",
            "Epoch [39/50], Train Loss: 0.4834, Train metric: 0.6952, Val Loss: 0.8311, Val metric: 0.9116\n",
            "Epoch [40/50], Train Loss: 0.4723, Train metric: 0.6872, Val Loss: 0.8307, Val metric: 0.9114\n",
            "Epoch [41/50], Train Loss: 0.4630, Train metric: 0.6804, Val Loss: 0.8279, Val metric: 0.9099\n",
            "Epoch [42/50], Train Loss: 0.4529, Train metric: 0.6730, Val Loss: 0.8283, Val metric: 0.9101\n",
            "Epoch [43/50], Train Loss: 0.4438, Train metric: 0.6662, Val Loss: 0.8280, Val metric: 0.9100\n",
            "Epoch [44/50], Train Loss: 0.4350, Train metric: 0.6596, Val Loss: 0.8284, Val metric: 0.9102\n",
            "Epoch [45/50], Train Loss: 0.4268, Train metric: 0.6533, Val Loss: 0.8314, Val metric: 0.9118\n",
            "Epoch [46/50], Train Loss: 0.4193, Train metric: 0.6475, Val Loss: 0.8297, Val metric: 0.9109\n",
            "Epoch [47/50], Train Loss: 0.4111, Train metric: 0.6411, Val Loss: 0.8307, Val metric: 0.9114\n",
            "Epoch [48/50], Train Loss: 0.4040, Train metric: 0.6356, Val Loss: 0.8327, Val metric: 0.9125\n",
            "Epoch [49/50], Train Loss: 0.3969, Train metric: 0.6300, Val Loss: 0.8366, Val metric: 0.9147\n",
            "Epoch [50/50], Train Loss: 0.3901, Train metric: 0.6246, Val Loss: 0.8352, Val metric: 0.9139\n"
          ]
        }
      ],
      "source": [
        "# Define the loss function\n",
        "# This function measures how well the model's predictions match the actual values.\n",
        "# Mean Squared Error (MSE) is commonly used for regression problems.\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Define the optimizer\n",
        "# The optimizer updates the model's weights to minimize the loss function.\n",
        "# Adam is an adaptive optimization algorithm that adjusts learning rates during training.\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, epochs=100):\n",
        "    \"\"\"\n",
        "    Function to train a PyTorch model with training and validation datasets.\n",
        "    \n",
        "    Parameters:\n",
        "    model: The neural network model to train.\n",
        "    train_loader: DataLoader for the training dataset.\n",
        "    val_loader: DataLoader for the validation dataset.\n",
        "    criterion: Loss function (e.g., Mean Squared Error for regression).\n",
        "    optimizer: Optimization algorithm (e.g., Adam, SGD).\n",
        "    epochs: Number of training epochs (default=100).\n",
        "    \n",
        "    Returns:\n",
        "    history: Dictionary containing loss and metric for both training and validation.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Dictionary to store training & validation loss and accuracy over epochs\n",
        "    history = {'train_loss': [], 'val_loss': [], 'train_metric': [], 'val_metric': []}\n",
        "\n",
        "    for epoch in range(epochs):  # Loop over the number of epochs\n",
        "        model.train()  # Set model to training mode\n",
        "        total_loss, train_metric = 0, 0  # Initialize total loss and correct predictions\n",
        "        \n",
        "        # Training loop\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()  # Reset gradients before each batch\n",
        "            outputs = model(inputs).squeeze() # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Compute loss\n",
        "            loss.backward()  # Backpropagation (compute gradients)\n",
        "            optimizer.step()  # Update model parameters\n",
        "            \n",
        "            total_loss += loss.item()  # Accumulate batch loss\n",
        "        \n",
        "        # Compute average loss and accuracy for training\n",
        "        train_loss = total_loss / len(train_loader)\n",
        "        train_metric = (total_loss / len(train_loader))**(1/2)\n",
        "\n",
        "        # Validation phase (without gradient computation)\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        val_loss, val_metric = 0, 0\n",
        "        with torch.no_grad():  # No need to compute gradients during validation\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs).squeeze()  # Forward pass\n",
        "                loss = criterion(outputs, labels)  # Compute loss\n",
        "                val_loss += loss.item()  # Accumulate validation loss\n",
        "        \n",
        "        # Compute average loss and accuracy for validation\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_metric = (val_loss)**(1/2)\n",
        "        \n",
        "        # Store metrics in history dictionary\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_metric'].append(val_metric)\n",
        "        history['val_metric'].append(val_metric)\n",
        "        \n",
        "        # Print training progress\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Train Metric: {train_metric:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Metric: {val_metric:.4f}\")\n",
        "    \n",
        "    return history  # Return training history\n",
        "\n",
        "# Train the model using the training function with defined parameters\n",
        "history = train(model,\n",
        "                train_loader=train_loader,\n",
        "                val_loader=val_loader,\n",
        "                criterion=criterion,\n",
        "                optimizer=optimizer,\n",
        "                epochs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error analysis\n",
        "\n",
        "Error analysis consists in focusing on the observations in the training set and validation sets that were predicted the worst by the model. This often reveals potential inconsistencies in the data, and helps identifies improvement opportunies for our model.\n",
        "\n",
        "1. Create a function that creates a Dataframe containing:\n",
        "    - the prediction value\n",
        "    - the true label of the observation\n",
        "    - the tokenized input\n",
        "    - the text input\n",
        "Based on a data loader, the tokenizer, and the model.\n",
        "Apply this function to both the train loader, and the val loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to evaluate the model and get worst predictions\n",
        "def evaluate_worst_predictions(model, dataloader, tokenizer, device=\"cpu\"):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    \n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_errors = []\n",
        "    all_inputs = []\n",
        "\n",
        "    with torch.no_grad():  # No gradients needed during evaluation\n",
        "        for batch in dataloader:\n",
        "            inputs, labels = batch  # Assuming (inputs, labels) in DataLoader\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            model.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Convert outputs to predicted class (for classification)\n",
        "            if outputs.shape[-1] > 1:  # Multi-class classification\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                errors = (preds != labels).float()  # Misclassified observations\n",
        "            else:  # Regression\n",
        "                preds = outputs.squeeze()\n",
        "                errors = torch.abs(preds - labels)  # Absolute error\n",
        "\n",
        "            # Save results\n",
        "            all_predictions.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_errors.extend(errors.cpu().numpy())\n",
        "            all_inputs.extend(inputs.cpu().numpy())\n",
        "\n",
        "    # Convert to DataFrame for analysis\n",
        "    df_results = pd.DataFrame({\n",
        "        \"True_Label\": all_labels,\n",
        "        \"Predicted\": all_predictions,\n",
        "        \"Error\": all_errors,\n",
        "        \"Inputs\": all_inputs,\n",
        "        \"Text\" : [tokenizer.decode(input) for input in all_inputs]\n",
        "    })\n",
        "\n",
        "    # Sort by highest error (worst predictions)\n",
        "    df_results_sorted = df_results.sort_values(by=\"Error\", ascending=False)\n",
        "\n",
        "    return df_results_sorted\n",
        "\n",
        "# Example usage:\n",
        "worst_predictions_val = evaluate_worst_predictions(model, val_loader, tokenizer, device=device)\n",
        "worst_predictions_train = evaluate_worst_predictions(model, train_loader, tokenizer, device=device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Display the first ten rows of each dataframe to get an idea of the worst predicted data points. Is there anything that raises questions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>True_Label</th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Error</th>\n",
              "      <th>Inputs</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4264</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4.531943</td>\n",
              "      <td>3.531943</td>\n",
              "      <td>[34, 1826, 653, 842, 69596, 4809, 588, 4618, 2...</td>\n",
              "      <td>C est un endroit merveilleux!!!!!!!!!!!!!!!!!!...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>759</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4.362868</td>\n",
              "      <td>3.362868</td>\n",
              "      <td>[47696, 708, 404, 11, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>Bonsoir,!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6366</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4.270876</td>\n",
              "      <td>3.270876</td>\n",
              "      <td>[1305, 12416, 14707, 1174, 389, 264, 81621, 40...</td>\n",
              "      <td>Très bien , on a passé de bon temps!!!!!!!!!!!...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>594</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4.218419</td>\n",
              "      <td>3.218419</td>\n",
              "      <td>[53, 969, 3904, 55455, 264, 13510, 2307, 0, 0,...</td>\n",
              "      <td>Vraiment rien a dire super!!!!!!!!!!!!!!!!!!!!...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5982</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4.085513</td>\n",
              "      <td>3.085513</td>\n",
              "      <td>[56948, 40574, 978, 481, 86323, 1522, 8047, 11...</td>\n",
              "      <td>Une agréable journée passée, la rencontre avec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4320</th>\n",
              "      <td>1.0</td>\n",
              "      <td>3.893484</td>\n",
              "      <td>2.893484</td>\n",
              "      <td>[73, 364, 1955, 285, 264, 834, 3520, 4363, 514...</td>\n",
              "      <td>j 'etais a disney land le 26.03.2015 avec ma f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4989</th>\n",
              "      <td>1.0</td>\n",
              "      <td>3.813063</td>\n",
              "      <td>2.813063</td>\n",
              "      <td>[1966, 39904, 829, 86323, 3869, 20028, 1208, 7...</td>\n",
              "      <td>On passe sa journée à faire la queue. Le temps...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3335</th>\n",
              "      <td>1.0</td>\n",
              "      <td>3.805321</td>\n",
              "      <td>2.805321</td>\n",
              "      <td>[1378, 8301, 11, 4983, 2428, 11, 58482, 261, 2...</td>\n",
              "      <td>Exceptionnel, magique, féerique.&lt;br/&gt; On est d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>586</th>\n",
              "      <td>1.0</td>\n",
              "      <td>3.802280</td>\n",
              "      <td>2.802280</td>\n",
              "      <td>[83, 897, 23008, 5019, 11083, 594, 40751, 5019...</td>\n",
              "      <td>trop cher pour mes ressources pourtant aimerai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6109</th>\n",
              "      <td>1.0</td>\n",
              "      <td>3.797496</td>\n",
              "      <td>2.797496</td>\n",
              "      <td>[1737, 220, 17, 73, 5544, 342, 312, 1892, 72, ...</td>\n",
              "      <td>En 2jrs g reussi a faire peu de manège trop d'...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      True_Label  Predicted     Error  \\\n",
              "4264         1.0   4.531943  3.531943   \n",
              "759          1.0   4.362868  3.362868   \n",
              "6366         1.0   4.270876  3.270876   \n",
              "594          1.0   4.218419  3.218419   \n",
              "5982         1.0   4.085513  3.085513   \n",
              "4320         1.0   3.893484  2.893484   \n",
              "4989         1.0   3.813063  2.813063   \n",
              "3335         1.0   3.805321  2.805321   \n",
              "586          1.0   3.802280  2.802280   \n",
              "6109         1.0   3.797496  2.797496   \n",
              "\n",
              "                                                 Inputs  \\\n",
              "4264  [34, 1826, 653, 842, 69596, 4809, 588, 4618, 2...   \n",
              "759   [47696, 708, 404, 11, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "6366  [1305, 12416, 14707, 1174, 389, 264, 81621, 40...   \n",
              "594   [53, 969, 3904, 55455, 264, 13510, 2307, 0, 0,...   \n",
              "5982  [56948, 40574, 978, 481, 86323, 1522, 8047, 11...   \n",
              "4320  [73, 364, 1955, 285, 264, 834, 3520, 4363, 514...   \n",
              "4989  [1966, 39904, 829, 86323, 3869, 20028, 1208, 7...   \n",
              "3335  [1378, 8301, 11, 4983, 2428, 11, 58482, 261, 2...   \n",
              "586   [83, 897, 23008, 5019, 11083, 594, 40751, 5019...   \n",
              "6109  [1737, 220, 17, 73, 5544, 342, 312, 1892, 72, ...   \n",
              "\n",
              "                                                   Text  \n",
              "4264  C est un endroit merveilleux!!!!!!!!!!!!!!!!!!...  \n",
              "759   Bonsoir,!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!...  \n",
              "6366  Très bien , on a passé de bon temps!!!!!!!!!!!...  \n",
              "594   Vraiment rien a dire super!!!!!!!!!!!!!!!!!!!!...  \n",
              "5982  Une agréable journée passée, la rencontre avec...  \n",
              "4320  j 'etais a disney land le 26.03.2015 avec ma f...  \n",
              "4989  On passe sa journée à faire la queue. Le temps...  \n",
              "3335  Exceptionnel, magique, féerique.<br/> On est d...  \n",
              "586   trop cher pour mes ressources pourtant aimerai...  \n",
              "6109  En 2jrs g reussi a faire peu de manège trop d'...  "
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "worst_predictions_train.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>True_Label</th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Error</th>\n",
              "      <th>Inputs</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>956</th>\n",
              "      <td>3.0</td>\n",
              "      <td>-1.037281</td>\n",
              "      <td>4.037281</td>\n",
              "      <td>[4643, 13281, 272, 1826, 25945, 23008, 5019, 6...</td>\n",
              "      <td>79 € c est très cher pour un parc, surtout qua...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1411</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4.647778</td>\n",
              "      <td>3.647778</td>\n",
              "      <td>[1844, 72006, 13612, 321, 11, 6316, 39892, 863...</td>\n",
              "      <td>Un beau soleil, une belle journée … mais une i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>962</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4.437809</td>\n",
              "      <td>3.437809</td>\n",
              "      <td>[30854, 36731, 6502, 35597, 69003, 978, 1744, ...</td>\n",
              "      <td>Je suis pas encore arrivé que je deteste déjà....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>373</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4.219281</td>\n",
              "      <td>3.219281</td>\n",
              "      <td>[1951, 2249, 49301, 220, 975, 1880, 220, 868, ...</td>\n",
              "      <td>Deux jours 14 et 15, juillet hôtel cheyenne ri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4.205205</td>\n",
              "      <td>3.205205</td>\n",
              "      <td>[66, 96287, 4983, 2428, 0, 3625, 60404, 1880, ...</td>\n",
              "      <td>c'était magique! les enfants et nous même en o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>814</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4.155590</td>\n",
              "      <td>3.155590</td>\n",
              "      <td>[1951, 40970, 665, 40970, 58482, 261, 2428, 12...</td>\n",
              "      <td>De moins en moins féerique!!! Dommage c était ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>975</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4.149827</td>\n",
              "      <td>3.149827</td>\n",
              "      <td>[2356, 3197, 1826, 2267, 1892, 978, 14465, 367...</td>\n",
              "      <td>Le plan est faussé Je suis à Disney Village!!!...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1561</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4.148654</td>\n",
              "      <td>3.148654</td>\n",
              "      <td>[30854, 308, 17771, 6502, 5363, 978, 514, 3990...</td>\n",
              "      <td>Je n'est pas regardé le passe Disney avant de ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4.108095</td>\n",
              "      <td>3.108095</td>\n",
              "      <td>[32960, 13510, 1744, 4864, 11457, 2852, 55398,...</td>\n",
              "      <td>Et dire que je venais jusqu'à 7 fois par an av...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1678</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4.099079</td>\n",
              "      <td>3.099079</td>\n",
              "      <td>[40, 4835, 80664, 306, 75831, 951, 83229, 70, ...</td>\n",
              "      <td>Ils semblent créer des règlements à la tête de...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      True_Label  Predicted     Error  \\\n",
              "956          3.0  -1.037281  4.037281   \n",
              "1411         1.0   4.647778  3.647778   \n",
              "962          1.0   4.437809  3.437809   \n",
              "373          1.0   4.219281  3.219281   \n",
              "767          1.0   4.205205  3.205205   \n",
              "814          1.0   4.155590  3.155590   \n",
              "975          1.0   4.149827  3.149827   \n",
              "1561         1.0   4.148654  3.148654   \n",
              "168          1.0   4.108095  3.108095   \n",
              "1678         1.0   4.099079  3.099079   \n",
              "\n",
              "                                                 Inputs  \\\n",
              "956   [4643, 13281, 272, 1826, 25945, 23008, 5019, 6...   \n",
              "1411  [1844, 72006, 13612, 321, 11, 6316, 39892, 863...   \n",
              "962   [30854, 36731, 6502, 35597, 69003, 978, 1744, ...   \n",
              "373   [1951, 2249, 49301, 220, 975, 1880, 220, 868, ...   \n",
              "767   [66, 96287, 4983, 2428, 0, 3625, 60404, 1880, ...   \n",
              "814   [1951, 40970, 665, 40970, 58482, 261, 2428, 12...   \n",
              "975   [2356, 3197, 1826, 2267, 1892, 978, 14465, 367...   \n",
              "1561  [30854, 308, 17771, 6502, 5363, 978, 514, 3990...   \n",
              "168   [32960, 13510, 1744, 4864, 11457, 2852, 55398,...   \n",
              "1678  [40, 4835, 80664, 306, 75831, 951, 83229, 70, ...   \n",
              "\n",
              "                                                   Text  \n",
              "956   79 € c est très cher pour un parc, surtout qua...  \n",
              "1411  Un beau soleil, une belle journée … mais une i...  \n",
              "962   Je suis pas encore arrivé que je deteste déjà....  \n",
              "373   Deux jours 14 et 15, juillet hôtel cheyenne ri...  \n",
              "767   c'était magique! les enfants et nous même en o...  \n",
              "814   De moins en moins féerique!!! Dommage c était ...  \n",
              "975   Le plan est faussé Je suis à Disney Village!!!...  \n",
              "1561  Je n'est pas regardé le passe Disney avant de ...  \n",
              "168   Et dire que je venais jusqu'à 7 fois par an av...  \n",
              "1678  Ils semblent créer des règlements à la tête de...  "
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "worst_predictions_val.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Calculate the mean error for each category of the target. Also calculate the number of samples belonging to each category. What do you think?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set prediction error by star review\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True_Label\n",
              "1.0    1.022648\n",
              "2.0    0.671437\n",
              "3.0    0.482908\n",
              "4.0    0.392095\n",
              "5.0    0.373207\n",
              "Name: Error, dtype: float32"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Train set prediction error by star review\")\n",
        "worst_predictions_train.groupby(\"True_Label\")[\"Error\"].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set star distribution\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True_Label\n",
              "5.0    3919\n",
              "4.0    1214\n",
              "3.0     797\n",
              "1.0     454\n",
              "2.0     395\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Train set star distribution\")\n",
        "worst_predictions_train[\"True_Label\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set prediction error by star review\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True_Label\n",
              "1.0    1.842363\n",
              "2.0    0.957422\n",
              "3.0    0.765634\n",
              "4.0    0.600415\n",
              "5.0    0.553516\n",
              "Name: Error, dtype: float32"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Validation set prediction error by star review\")\n",
        "worst_predictions_val.groupby(\"True_Label\")[\"Error\"].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set star distribution\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True_Label\n",
              "5.0    962\n",
              "4.0    324\n",
              "3.0    213\n",
              "1.0    104\n",
              "2.0     92\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Validation set star distribution\")\n",
        "worst_predictions_val[\"True_Label\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "01-Embedding_for_sentiment_analysis_solutions.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
