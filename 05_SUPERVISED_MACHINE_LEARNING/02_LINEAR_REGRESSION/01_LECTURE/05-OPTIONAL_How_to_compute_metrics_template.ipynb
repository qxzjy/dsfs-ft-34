{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to compute evaluation metrics in Python\n",
    "This template shows you two methods for computing the different metrics used for performance evaluation and feature selection. \n",
    "\n",
    "### F-Statistics by Fisher\n",
    "\n",
    "A statistical test is a process by which we try to show whether a hypothesis is confirmed or disproved by the data at our disposal. This test hypothesis, also called null hypothesis and noted $H_{0}$, would have consequences on the properties of the observed data if it is actually verified. These properties are summarized by a test statistic, the value of which gives an idea of the probability that H_0 is true.\n",
    "\n",
    "Fisher's F-statistic allows to test the veracity of the following hypotheses:\n",
    "\n",
    "\n",
    "* When the Fisher test is applied to the model as a whole, the null hypothesis, noted $H_{0}$, is \"the variables chosen to construct the model are not jointly significant in describing the target variable\". If the hypothesis is true, the F-statistic should follow a Fisher probability distribution law noted F-distribution of parameters $(n - 1, n - 1)$ where $n$ is the number of observations used to train the model. However, if the value of the F-statistic, noted \"F\", is outside the most probable regions of the distribution, then we can reject the null hypothesis and conclude that the chosen model has a real explanatory power on the target variable.\n",
    "\n",
    "It may seem a little farfetched but all statistical tests work like that. We make an assumption, this assumption if it held would cause the test statistic to follow a given distribution, if the actual value of the statistic lands too far from the probable scope of the hypothetical distribution we are allowed to reject the null hypothesis.\n",
    "\n",
    "Mathematically, the F-statistic is written:\n",
    "\n",
    "$$\n",
    "F = \\frac{SSE}{SSR}\n",
    "$$\n",
    "\n",
    "The F-test can also compare two nested models (model 1 which includes \"model_1_variables\" and model 2 which includes \"model_1_variables + $X_d$\". In this case the F-statistic follows an F-law of parameters $(n - 1, n - 1)$ if the assumption that the simplest model (model 1) of the two models best describes the target variable is verified. The mathematical formula of F is then :\n",
    "\n",
    "$$\n",
    "F = (\\frac{SSR_{2}-SSR_{1}}{p_{2}-p_{1}})(\\frac{n-p_{1}}{SSR_{1}})\n",
    "$$\n",
    "\n",
    "If the value of F-statistic is in an unlikely region of the F-distribution, then the hypothesis is rejected and the test suggests that the more complex Model 2 provides significant additional information compared to the simpler Model 1.\n",
    "\n",
    "Graphically the F-test can be illustrated as follows:\n",
    "\n",
    "![F-statistic](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/curve.png)\n",
    "\n",
    "We represent the density distribution of the F-distribution, as in any test we define a level $\\alpha$ between 0 and 1 which will influence the size of the hypothesis rejection zone. Very often, we choose $\\alpha = 5%$ when no specific knowledge can help us modulate our standards. The F test is one-sided, only large values of F will allow us to reject the hypothesis. More precisely if the value of F is in the upper part of the expected distribution equivalent to 5% probability, then we can say that the hypothesis is rejected at $1 - \\alpha$ 95%.\n",
    "\n",
    "This first metric allows us to test the hypothesis that the explanatory variables have no influence on the target variable, we will now look at metrics that indicate the performance level of the model.\n",
    "\n",
    "* $R^2_{adjusted}$\n",
    "\n",
    "$R^2_{adjusted}$ is a modified version of $RÂ²$ that penalizes the number of explanatory variables selected to build the model. Its mathematical formula is:\n",
    "\n",
    "$$\n",
    "R^2_{ajusted} = 1-\\frac{n-1}{n-p-1}(1-R^2)\n",
    "$$\n",
    "\n",
    "Where $p$ is the number of explanatory variables used and $n$ is the number of observations used. The growth of $R^2$ as a function of $p$ is compensated by the decrease of $\\frac{n-1}{(n-p-1)}$ as a function of $p$. Consequently, if the information contribution of an explanatory variable is not significant enough, then $R^2_{adjusted}$ will decrease. In fact, it is possible to use this indicator to compare the performance of models that do not necessarily have the same number of explanatory variables.\n",
    "\n",
    "* P-values\n",
    "\n",
    "P-values are evaluation metrics that make it possible to evaluate the contribution of each explanatory variable individually as opposed to evaluating the model as a whole. Unfortunately it cannot be easily computed using sklearn so we will introduce the statsmodels library that calculates all important metrics automatically. The p_value can be interpreted as the probability that a given parameter's true value is 0, in other words the probability that a variable does not bring any significant information to the linear model. Usually we consider that a p_value inferior to 5% means that the variable is significant, otherwise it is consider not significant, however it depends on the context and the standards of the industry you are working in : for example web marketing agencies typically have lower standards than pharmaceutical companies because their goals and constraints are fundamentally different.\n",
    "\n",
    "\n",
    "## Computation by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Square Total 9.74\n",
      "Sum of Square Explained 9.612353658536577\n",
      "Sum of Square Residual 0.1276463414634147\n",
      "\n",
      "\n",
      "R square 0.9868946261331196\n",
      "R square 0.9868946261331196\n",
      "R square adjusted 0.9737892522662392\n"
     ]
    }
   ],
   "source": [
    "# SST, SSE and SSR have to be calculated manually\n",
    "# generate some example data\n",
    "X = np.array([\n",
    "    [1,3,5,6,7],\n",
    "    [4.6, 3.7, 3.4, 3.0, 3.1]\n",
    "]).transpose()\n",
    "Y = np.array([2.1, 3.5, 4.4, 5.6, 5.9])\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression() # create and instanceof the model\n",
    "\n",
    "model.fit(X,Y) # fit the model\n",
    "\n",
    "# calculate evaluation metrics\n",
    "SST = np.sum(np.square(Y - np.mean(Y)))\n",
    "print(\"Sum of Square Total {}\".format(SST))\n",
    "\n",
    "SSE = np.sum(np.square(model.predict(X) - np.mean(Y)))\n",
    "print(\"Sum of Square Explained {}\".format(SSE))\n",
    "\n",
    "SSR = np.sum(np.square(Y - model.predict(X)))\n",
    "print(\"Sum of Square Residual {}\".format(SSR))\n",
    "print(\"\\n\")\n",
    "\n",
    "# calculate R square and adjusted R-square\n",
    "R_2 = 1 - SSR/SST\n",
    "print(\"R square {}\".format(R_2))\n",
    "R_2_alt = model.score(X,Y) # alternative method to calculate R square\n",
    "print(\"R square {}\".format(R_2_alt))\n",
    "n = X.shape[0]\n",
    "p = X.shape[1]\n",
    "R_2_adj = 1 - (n-1)/(n-p-1)*(1-R_2)\n",
    "print(\"R square adjusted {}\".format(R_2_adj))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation with statsmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/statsmodels/compat/pandas.py:65: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import Int64Index as NumericIndex\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "------------------------Results from statsmodels-----------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.987\n",
      "Model:                            OLS   Adj. R-squared:                  0.974\n",
      "Method:                 Least Squares   F-statistic:                     75.30\n",
      "Date:                Thu, 23 Jun 2022   Prob (F-statistic):             0.0131\n",
      "Time:                        17:06:35   Log-Likelihood:                 2.0751\n",
      "No. Observations:                   5   AIC:                             1.850\n",
      "Df Residuals:                       2   BIC:                            0.6781\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          4.0554      3.157      1.285      0.328      -9.527      17.638\n",
      "x1             0.5016      0.179      2.798      0.108      -0.270       1.273\n",
      "x2            -0.5512      0.672     -0.820      0.498      -3.442       2.340\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                   2.600\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.614\n",
      "Skew:                          -0.828   Prob(JB):                        0.736\n",
      "Kurtosis:                       2.548   Cond. No.                         169.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/statsmodels/stats/stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 5 samples were given.\n",
      "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n"
     ]
    }
   ],
   "source": [
    "# alternative solution with library statsmodels (useful mainly for linear models)\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X2 = sm.add_constant(X) # the coefficient beta_0 also called intercept is not automatically included, so we need to manually add a constant variable equal to one.\n",
    "est = sm.OLS(Y, X2)\n",
    "est2 = est.fit()\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "print(\"------------------------Results from statsmodels-----------------------------------------\")\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "print(\"\\n\")\n",
    "print(est2.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
