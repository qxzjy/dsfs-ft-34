{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Basics\n",
    "\n",
    "## What you will learn in this course? üßêüßê\n",
    "\n",
    "Let's dive deeper into how Langchain actually works. In this course, we will cover:\n",
    "\n",
    "* Langchain client \n",
    "* Langchain API \n",
    "* Prompt Templating\n",
    "* Models\n",
    "* Langchain Expression Language - LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic architecture of a Langchain Application \n",
    "\n",
    "When you are building a langchain application, you need to visualize how it will look like in production. Your application will be structured with the following components:\n",
    "\n",
    "* **An API server** üëâ This is the skeleton of your application where you'll write all the logic of your application\n",
    "* **Third Party Tools** üëâ Most likely your API will need to use an externally hosted LLM (like OpenAI ChatGPT, Mistral or even your own custom trained model hosted on a separate server), or use other external tools like Google Search, Scrapers etc.\n",
    "* **Clients** üëâ Then finally, the way the end-user will use your API is through a Web application (like streamlit, Gradio etc.) or directly with Jupyter Notebooks\n",
    "\n",
    "![](https://full-stack-assets.s3.eu-west-3.amazonaws.com/Langchain-application-architecture.png)\n",
    "\n",
    "For the demo below, here is what we are going to build:\n",
    "\n",
    "* API üëâ This will be a very basic Chatbot\n",
    "* Third Party üëâ We will use Mistral as our external pre-trained model \n",
    "* Clients üëâ We will show you how to consume the API on a Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "* Create an account in Mistral and get an API key. Here is how to do it üëá\n",
    "\n",
    "<Video video=\"https://vimeo.com/1017970631\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API - Full code\n",
    "\n",
    "Let's first start by presenting the full application for you to see how a langchain application would look like. We'll then dive into each part of the code step by step. This application is based [on this great Langchain tutorial](https://python.langchain.com/docs/tutorials/llm_chain/)\n",
    "\n",
    "\n",
    "Here is the source code of the demo:\n",
    "\n",
    "* [Langchain Demo App](https://github.com/JedhaBootcamp/Sample_Langchain_app)\n",
    "\n",
    "Feel free to:\n",
    "\n",
    "```bash \n",
    "git clone https://github.com/JedhaBootcamp/Sample_Langchain_app.git\n",
    "```\n",
    "\n",
    "Now you can run the application simply by running:\n",
    "\n",
    "```bash\n",
    "docker run -p 7860:7860 -e MISTRAL_API_KEY=YOUR_MISTRAL_API_KEY jedha/langchain-base\n",
    "```\n",
    "\n",
    "It will start a web server that you can access at:\n",
    "\n",
    "* **http://localhost:7860/chain/playground**\n",
    "\n",
    "\n",
    "Now this app is pretty basic but it's already a great start for us to understand the API. Let's dive into each part of the code to deepen our understanding. \n",
    "\n",
    "\n",
    "<Note type=\"important\">\n",
    "\n",
    "For the rest of the demo, we will be using this API in production on HuggingFace at the URL:\n",
    "\n",
    "* `https://antoinekrajnc-sample-langchain-api.hf.space/`\n",
    "\n",
    "</Note>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Additional client requirements \n",
    "\n",
    "When we'll be using the API, the **client (like a jupyter notebook)** will need to have the following dependencies installed:\n",
    "\n",
    "* `langchain`\n",
    "* `langchain-community `\n",
    "* `langchain-mistralai`\n",
    "* `langchain-openai`\n",
    "* `langserve[all]`\n",
    "* `langgraph`\n",
    "* `fastapi[standard]`\n",
    "\n",
    "<Note type=\"tip\">\n",
    "\n",
    "If you do not want to install all these packages on your machine, you should run a docker container. I advise you to use `jupyter/datascience-notebook`.\n",
    "\n",
    "```bash \n",
    "docker run -v $(pwd):/home/jovyan -p 8888:8888 jupyter/datascience-notebook\n",
    "```\n",
    "\n",
    "This will automatically run jupyter lab at `http://localhost:8888`. You can then either open up your web browser or you can also use the Jupyter kernel in VSCode. Here is how:\n",
    "\n",
    "* [Run Jupyter Notebook container in VSCode](https://medium.com/@FredAsDev/connect-vs-code-jupyter-notebook-to-a-jupyter-container-a63293f29325)\n",
    "\n",
    "</Note>\n",
    "\n",
    "\n",
    "<Note type=\"important\">\n",
    "\n",
    "This is for the **Client** setup. Even though we will have to install similar dependencies for the API, I just want to make sure that there are two independent elements: \n",
    "\n",
    "* A client (this jupyter notebook)\n",
    "* A server (the API - that is hosted on HuggingFace)\n",
    "\n",
    "</Note>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client-side: Interact with the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't have the dependencies already installed\n",
    "!pip install langchain -q\n",
    "!pip install langchain_mistralai -q\n",
    "!pip install langserve -q\n",
    "!pip install fastapi -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Note type=\"important\">\n",
    "\n",
    "The API server is a very small one to test **very small requests**, don't try to use it for large contexts etc, it will just break the server.\n",
    "\n",
    "</Note>\n",
    "\n",
    "\n",
    "To interact with a LangServe API clients can use it pretty easily using `RemoteRunnable` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m ENDPOINT=\u001b[33m\"\u001b[39m\u001b[33m/chain/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m translator = RemoteRunnable(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mENDPOINT\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mtranslator\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlanguage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFrench\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is the name of the most famous Star Wars bounty hunter?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm_env/lib/python3.12/site-packages/langserve/client.py:370\u001b[39m, in \u001b[36mRemoteRunnable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[32m    369\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mkwargs not implemented yet.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm_env/lib/python3.12/site-packages/langchain_core/runnables/base.py:1930\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1926\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1928\u001b[39m         output = cast(\n\u001b[32m   1929\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1930\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1931\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1933\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1935\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1936\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1937\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1938\u001b[39m         )\n\u001b[32m   1939\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1940\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm_env/lib/python3.12/site-packages/langchain_core/runnables/config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm_env/lib/python3.12/site-packages/langserve/client.py:349\u001b[39m, in \u001b[36mRemoteRunnable._invoke\u001b[39m\u001b[34m(self, input, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_invoke\u001b[39m(\n\u001b[32m    342\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    343\u001b[39m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[32m   (...)\u001b[39m\u001b[32m    346\u001b[39m     **kwargs: Any,\n\u001b[32m    347\u001b[39m ) -> Output:\n\u001b[32m    348\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Invoke the runnable with the given input and config.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msync_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/invoke\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lc_serializer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumpd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfig\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_prepare_config_for_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    357\u001b[39m     output, callback_events = _decode_response(\n\u001b[32m    358\u001b[39m         \u001b[38;5;28mself\u001b[39m._lc_serializer, response, is_batch=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    359\u001b[39m     )\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._use_server_callback_events \u001b[38;5;129;01mand\u001b[39;00m callback_events:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm_env/lib/python3.12/site-packages/httpx/_client.py:1144\u001b[39m, in \u001b[36mClient.post\u001b[39m\u001b[34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1124\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1125\u001b[39m     url: URL | \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1137\u001b[39m     extensions: RequestExtensions | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1138\u001b[39m ) -> Response:\n\u001b[32m   1139\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1140\u001b[39m \u001b[33;03m    Send a `POST` request.\u001b[39;00m\n\u001b[32m   1141\u001b[39m \n\u001b[32m   1142\u001b[39m \u001b[33;03m    **Parameters**: See `httpx.request`.\u001b[39;00m\n\u001b[32m   1143\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcookies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextensions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm_env/lib/python3.12/site-packages/httpx/_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm_env/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm_env/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm_env/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm_env/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm_env/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm_env/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm_env/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm_env/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm_env/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm_env/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm_env/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm_env/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm_env/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm_env/lib/python3.12/ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm_env/lib/python3.12/ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "# This is the production API from the demo github repo - https://github.com/JedhaBootcamp/Sample_Langchain_app\n",
    "HOST = \"https://antoinekrajnc-sample-langchain-api.hf.space\"\n",
    "ENDPOINT=\"/chain/\"\n",
    "\n",
    "translator = RemoteRunnable(f\"{HOST}{ENDPOINT}\")\n",
    "translator.invoke({\"language\": \"French\", \"text\": \"What is the name of the most famous Star Wars bounty hunter?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most famous bounty hunter from Star Wars is Boba Fett.\n",
      "\n",
      "Translated into French, it would be:\n",
      "\n",
      "\"Quel est le nom du chasseur de primes Star Wars le plus c√©l√®bre?\"\n",
      "\n",
      "And the answer:\n",
      "\n",
      "\"Boba Fett\" (the name remains the same in French)."
     ]
    }
   ],
   "source": [
    "## YOU CAN ALSO USE THE CODE BELOW TO PRINT TOKEN BY TOKEN (LIKE IN CHATGPT APP)\n",
    "for token in translator.stream({\"language\": \"French\", \"text\": \"What is the name of the most Star Wars bounty hunter?\"}):\n",
    "    print(token, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright now you know **how to interact from a client to the API using Langchain**. Pretty easy right? For the next section, we'll dive deeper into each concept behind the API so that you can understand the code better. \n",
    "\n",
    "<Note type=\"important\">\n",
    "\n",
    "If you want to follow along, keep your client Jupyter Notebook up so that you can run the commands below\n",
    "\n",
    "</Note>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API-Side: Deep dive into the code\n",
    "\n",
    "### Prompt Template\n",
    "\n",
    "Let's start by talking about prompts and prompt templates. When calling an LLM, it is likely that you have some kind of instructions that you want it to follow. In the above demo:\n",
    "\n",
    "```python\n",
    "system_template = \"Translate the following into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_template),\n",
    "    ('user', '{text}')\n",
    "])\n",
    "```\n",
    "\n",
    "We want the LLM to translate any sentence in a given `{language}`. \n",
    "\n",
    "Now there are three types of Prompt Templates:\n",
    "\n",
    "#### Prompt Templates Overview\n",
    "\n",
    "| **Type**                | **Description**                                       |\n",
    "|-------------------------|-------------------------------------------------------|\n",
    "| **[`PromptTemplate`](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html)** | Formats a single string for simpler inputs.           |\n",
    "| **[`ChatPromptTemplate`](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)**   | Formats a list of messages for complex interactions.   |\n",
    "| **[`MessagesPlaceholder`](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)**  | Inserts a list of messages at a specific point.        |\n",
    "\n",
    "\n",
    "Let's see an example with each of them to see how they work. \n",
    "\n",
    "#### `PromptTemplate`\n",
    "\n",
    "`PromptTemplate` is the most basic class that is usually used when you have one simple prompt (like just a single string) to use with a very few configurations: \n",
    "\n",
    "<Note type=\"important\">\n",
    "\n",
    "Note that this class only builds the prompt and does not compute an anwser for the prompt.\n",
    "\n",
    "</Note>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me everything you know about this Star Wars Character: Sifo-Dyas'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "message = \"Tell me everything you know about this Star Wars Character: {character}\"\n",
    "prompt_template = PromptTemplate.from_template(message) # This is a very simple string with {character} as configurable paramater. \n",
    "result = prompt_template.invoke({\"character\": \"Sifo-Dyas\"}) # Here we use the `.invoke()` method to actually execute the prompt chain.\n",
    "\n",
    "result.to_string() # Print just the string result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Note type=\"tip\">\n",
    "\n",
    "If that helps, `PromptTemplate` works a bit like f-strings:\n",
    "\n",
    "* f-string: \n",
    "\n",
    "```python\n",
    "name = \"Jedha\"\n",
    "print(f\"hello {name}\")\n",
    "```\n",
    "\n",
    "* PromptTemplate:\n",
    "\n",
    "```python \n",
    "prompt = PromptTemplate.from_template(\"Hello {name}\")\n",
    "prompt.invoke({\"name\":\"Jedha\"})\n",
    "```\n",
    "\n",
    "</Note>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `ChatPromptTemplate`\n",
    "\n",
    "`ChatPromptTemplate` handles more complex prompt templates where you can input a sequence of messages with different roles. Let's see how that works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a protocol droid designed for assisting sentient beings. Your designation is R-3PO.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Greetings, droid. Status report?', additional_kwargs={}, response_metadata={}), AIMessage(content='All systems are fully operational, and I am ready to assist you.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your primary function?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Step 1: Define the interaction template between a human and a droid\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a protocol droid designed for assisting sentient beings. Your designation is {name}.\"),\n",
    "    (\"user\", \"Greetings, droid. Status report?\"),\n",
    "    (\"assistant\", \"All systems are fully operational, and I am ready to assist you.\"),\n",
    "    (\"user\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "# Step 2: Fill in the droid's name and user input\n",
    "prompt_value = template.invoke(\n",
    "    {\n",
    "        \"name\": \"R-3PO\",  # Name of the droid\n",
    "        \"user_input\": \"What is your primary function?\"  # User asking a question\n",
    "    }\n",
    ")\n",
    "\n",
    "# Step 3: Convert the prompt to messages to simulate the conversation\n",
    "print(prompt_value.to_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three default roles in a `ChatPromptTemplate`:\n",
    "\n",
    "| Role | Description¬†|\n",
    "| ---- | ----------- |\n",
    "| `system` | This is configuration prompt | \n",
    "| `user`¬†| This is a human written message | \n",
    "| `assistant` | This is the AI response | \n",
    "\n",
    "\n",
    "You can definitely change them and add custom roles but we advise you to leave it that way as most models understand it pretty well. Actually in Langchain, you can even setup this messages like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='What a vast and interesting question, is there anything you would like to explore in particular?', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# system role\n",
    "SystemMessage(content=\"You are an astrophysicists AI and know everything space-related\")\n",
    "# user role \n",
    "HumanMessage(content=\"Hi, I want to know about the universe\")\n",
    "# assistant role\n",
    "AIMessage(content=\"What a vast and interesting question, is there anything you would like to explore in particular?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Note type=\"note\">\n",
    "\n",
    "There are other types of messages that you can check directly here:\n",
    "\n",
    "* [Messages](https://python.langchain.com/api_reference/core/messages.html)\n",
    "\n",
    "</Note>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `MessagesPlaceholder`\n",
    "\n",
    "`MessagesPlaceholder` is a wrapper that will contain a list of different messages with different roles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a protocol droid designed to assist sentient beings.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Greetings, droid.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Step 1: Create a placeholder for message history\n",
    "prompt = MessagesPlaceholder(\"history\", optional=True)  # Leave optional=True in case there are no previous messages.\n",
    "\n",
    "# Step 2: Add some initial messages to simulate past interaction\n",
    "prompt.format_messages(\n",
    "    history=[\n",
    "        (\"system\", \"You are a protocol droid designed to assist sentient beings.\"),\n",
    "        (\"human\", \"Greetings, droid.\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `MessagesPlaceholder` is often used in combination with `ChatPromptTemplate` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a protocol droid with the designation R-3PO.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Calculate the coordinates for the jump to lightspeed.', additional_kwargs={}, response_metadata={}), AIMessage(content='The jump coordinates are calculated: 12.345, -45.678.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Now, plot a course to the nearest star system.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Create a new ChatPromptTemplate with MessagesPlaceholder for the conversation history\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a protocol droid with the designation R-3PO.\"),\n",
    "        MessagesPlaceholder(\"history\"),  # Placeholder for message history\n",
    "        (\"human\", \"{question}\")  # The human asks a new question\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 4: Invoke the prompt with message history and the new question\n",
    "response = prompt.invoke(\n",
    "   {\n",
    "       \"history\": [(\"human\", \"Calculate the coordinates for the jump to lightspeed.\"), \n",
    "                   (\"ai\", \"The jump coordinates are calculated: 12.345, -45.678.\")],\n",
    "       \"question\": \"Now, plot a course to the nearest star system.\"\n",
    "   }\n",
    ")\n",
    "\n",
    "# Step 5: Simulate the conversation output\n",
    "print(response.to_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way you can have a dynamic size of messages which is one of the **founding block for adding memory in all LLMs** which we will cover later on üòâ\n",
    "\n",
    "\n",
    "<Note type=\"note\" title=\"other types of prompt template\">\n",
    "\n",
    "There are other types of Prompt Templates that you can check out here:\n",
    "\n",
    "* [Prompts](https://python.langchain.com/api_reference/core/prompts.html)\n",
    "\n",
    "\n",
    "</Note>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models \n",
    "\n",
    "Alright, let's talk models now. With Langchain, you can use any model you want:\n",
    "\n",
    "* Proprietary pretrained models like\n",
    "    * ChatGPT\n",
    "    * Llama \n",
    "    * Mistral\n",
    "    * ...\n",
    "\n",
    "* Custom models you trained yourself\n",
    "\n",
    "\n",
    "We won't cover custom models just yet but we will show you how to use proprieraty models. Langchain built a wrapper around all the major models that you can use for your application. Here is the full list that you can check: \n",
    "\n",
    "* [All integrated ChatModels](https://python.langchain.com/docs/integrations/chat/)\n",
    "\n",
    "To use them, all you need to do is follow this template:\n",
    "\n",
    "1. Create an account on the given provider (i.e OpenAI, Mistral, AWS)\n",
    "2. Get an API Key from the provider \n",
    "3. Import the model in Langchain\n",
    "\n",
    "You already have seen an example with Mistral but here is another one with OpenAI:\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"YOUR_OPENAI_API_KEY\"\n",
    "```\n",
    "\n",
    "Then\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL: Chaining concept with Langchain\n",
    "\n",
    "This is probably the most innovative part of the Langchain framework: **Langchain Expression Language (LCEL)**. In the demo we saw a very basic implementation with:\n",
    "\n",
    "```python \n",
    "chain = prompt_template | model | parser\n",
    "```\n",
    "\n",
    "With LCEL, you can chain together anything that is a `Runnable` and by `Runnable` we mean anything that can use the `.invoke()` method. That includes:\n",
    "\n",
    "* Chat models \n",
    "* Output parsers \n",
    "* Prompt Templates\n",
    "\n",
    "All sequences (also called `RunnableSequence`) is either separated with a `|` or using the `.pipe()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a environment variable\n",
    "# %env MISTRAL_API_KEY=\n",
    "\n",
    "# Now use echo to verify it's set\n",
    "#!echo $MISTRAL_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The translation of \"Jedha was the Jedis\\' home planet\" into French is:\\n\\n\"Jedha √©tait la plan√®te d\\'origine des Jedi.\"\\n\\nHere\\'s a breakdown:\\n- Jedha = Jedha\\n- was = √©tait\\n- the = la\\n- Jedis\\' = des Jedi (Note that \"Jedi\" is invariable in French and doesn\\'t take an apostrophe-s to show possession.)\\n- home planet = plan√®te d\\'origine'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "system_template = \"Translate the following into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_template),\n",
    "    ('user', '{text}')\n",
    "])\n",
    "\n",
    "model = ChatMistralAI(model=\"mistral-large-latest\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "pipe_sequence = (\n",
    "    prompt_template\n",
    "    .pipe(model)\n",
    "    .pipe(parser)\n",
    ")\n",
    "\n",
    "pipe_sequence.invoke({\n",
    "    \"language\": \"French\", \n",
    "    \"text\": \"Jedha was the Jedis' home planet\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translation of \"Jedha was the Jedis' home planet\" into French is:\n",
      "\n",
      "\"Jedha √©tait la plan√®te d'origine des Jedi.\"\n",
      "\n",
      "Here's a breakdown:\n",
      "- Jedha = Jedha\n",
      "- was = √©tait\n",
      "- the = la\n",
      "- Jedis' = des Jedi (Note that \"Jedi\" is an invariable noun in French, so it doesn't take an apostrophe-s to show possession.)\n",
      "- home planet = plan√®te d'origine\n"
     ]
    }
   ],
   "source": [
    "print(pipe_sequence.invoke({\"language\": \"French\",\"text\": \"Jedha was the Jedis' home planet\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangServe\n",
    "\n",
    "The final piece for our application is **LangServe**. It is an additional layer on top of FastAPI that will:\n",
    "\n",
    "* Create three sub-endpoints: `invoke`, `batch` and `stream` per endpoint \n",
    "* Create a playground at `/endpoint_your_setup/playground`\n",
    "* Create a `/stream_log` endpoint to monitor intermediate steps from your chains which is great for debugging \n",
    "* Create a client SDK to easily call your endpoint\n",
    "\n",
    "All you have to do is:\n",
    "\n",
    "1. Create a Runnable\n",
    "2. Wrap it around a `add_routes()` method like this:\n",
    "\n",
    "```python \n",
    "from fastapi import FastAPI\n",
    "from langserve import add_routes\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Sample LangServe API\",\n",
    "    version=\"0.1\",\n",
    "    description=\"Simple FastAPI app that integrates LangServe\"\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app, # this is your FastAPI instance\n",
    "    pipe_sequence, # This is the Runnable chain that we defined above\n",
    "    \"/translate\" # This is the endpoint \n",
    ")\n",
    "```\n",
    "\n",
    "<Note type=\"note\">\n",
    "\n",
    "You will need to repeat the `add_routes` for every endpoints you create for your application (Not very DRY I know).\n",
    "\n",
    "</Note>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources üìöüìö\n",
    "\n",
    "* [Build a simple LLM Application](https://python.langchain.com/docs/tutorials/llm_chain/)\n",
    "* [La plateforme - Mistral](https://console.mistral.ai/)\n",
    "* [All integrated ChatModels](https://python.langchain.com/docs/integrations/chat/)\n",
    "* [LangServe](https://python.langchain.com/docs/langserve/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
