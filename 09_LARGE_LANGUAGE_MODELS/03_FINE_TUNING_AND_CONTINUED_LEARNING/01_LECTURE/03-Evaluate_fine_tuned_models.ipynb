{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Fine-Tuned Models\n",
    "\n",
    "## What you will learn in this course üßêüßê\n",
    "\n",
    "You've trained a model but now what? The problem with LLMs is that they will always provide an answer to a user. The only question is: *Will it be the one you expected?* (when fine-tuning the model). This is why you need to have some kind of an evaluation process. \n",
    "\n",
    "In this course, we will teach you a simple technic to evaluate a model thanks to another LLM\n",
    "\n",
    "## Methods to Evaluate an LLM \n",
    "\n",
    "### Metrics in Short\n",
    "\n",
    "If you need a quick overview before diving into each metric, check-out the table below üëá\n",
    "\n",
    "| Metric                   | Best For                           | Interpretation                                                                                       |\n",
    "|--------------------------|------------------------------------|------------------------------------------------------------------------------------------------------|\n",
    "| Perplexity               | Language modeling                 | Lower perplexity = better fit to natural language patterns                                           |\n",
    "| BLEU, ROUGE              | Translation, Summarization        | Higher = closer to reference; may penalize creative wording                                          |\n",
    "| Accuracy (HellaSwag, MMLU) | Reasoning, Knowledge             | Higher accuracy = better at reasoning or factual understanding                                       |\n",
    "| TruthfulQA Accuracy      | Truthfulness                      | Higher accuracy = avoids common misconceptions, especially in complex or controversial topics        |\n",
    "| BLEURT, BERTScore        | QA, Text Generation               | Higher = closer semantic meaning to reference, allows for varied wording                             |\n",
    "| Human Evaluation         | Conversational AI, Open-ended generation | Provides qualitative feedback on coherence, relevance, fluency, and engagement                  |\n",
    "| LLM-based Evaluation     | Automated, scalable assessments   | Uses another LLM to score relevance, coherence, or factuality; efficient but may reinforce biases    |\n",
    "\n",
    "\n",
    "There are several ways to evaluate an LLM. Among the most popular are:\n",
    "\n",
    "### Extended classification\n",
    "\n",
    "This category aims at evaluating the LLM's capacity to classify, translate or summarize text. These metrics are great when the need for a model's creativity is limited:\n",
    "\n",
    "- **Perplexity**: Measures how well the model predicts the next word. It's a common metric in language modeling tasks.\n",
    "   - **Interpretation**: A lower perplexity indicates the model finds the sequence of words more ‚Äúpredictable‚Äù and thus has a stronger grasp of natural language patterns. This works well in **language modeling** tasks but can be limiting for **open-ended generation** since it doesn‚Äôt directly correlate with meaningfulness or truthfulness.\n",
    "   \n",
    "- **BLEU (Bilingual Evaluation Understudy)**: Often used in **machine translation** and tasks with a clear reference output.\n",
    "   - **Interpretation**: BLEU compares model-generated output to reference output based on word overlaps. Higher BLEU scores indicate closer similarity to the reference but can penalize creative wording and isn't ideal for open-ended tasks.\n",
    "\n",
    "- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: Primarily used in **summarization** to measure overlap between generated summaries and reference summaries.\n",
    "   - **Interpretation**: Higher ROUGE scores suggest that the generated text retains important content from the reference. Like BLEU, ROUGE may not fully capture nuanced quality, making it suitable for extractive but not always for abstractive tasks.\n",
    "\n",
    "- **METEOR (Metric for Evaluation of Translation with Explicit ORdering)**: Extends BLEU with synonyms and stemming, often used in machine translation.\n",
    "   - **Interpretation**: Slightly better for capturing ‚Äúsemantic‚Äù similarity than BLEU, making it useful for **translation** and **summarization** where meaning preservation is essential.\n",
    "\n",
    "- **BLEURT**: An **evaluation model** trained to score responses for **semantic similarity** with reference answers. Unlike BLEU, it focuses on the *meaning* behind words, making it suitable for **summarization** and **open-ended** tasks.\n",
    "   - **Interpretation**: Higher BLEURT scores mean the model output is closer in meaning to human-provided references. It‚Äôs beneficial for tasks like **QA** and **paraphrasing**.\n",
    "\n",
    "\n",
    "- **BERTScore**: Computes similarity at the word level using embeddings, which can capture **semantic closeness** rather than exact word matches.\n",
    "   - **Interpretation**: This is more flexible than BLEU or ROUGE, as it considers synonyms and contextual relevance. High BERTScore indicates semantically similar outputs, making it suitable for **summarization** and **text generation** tasks.\n",
    "\n",
    "- **Exact Match (EM)**: Common in **QA** tasks, it measures if the answer is exactly correct.\n",
    "   - **Interpretation**: High EM means the model can precisely identify correct answers. However, it‚Äôs strict and may not reflect near-correct answers.\n",
    "\n",
    "\n",
    "### Task-Specific Benchmarks\n",
    "\n",
    "Evaluating \"how well\" a model generates data is quite hard. Therefore the industry came up with metrics that focuses on specific tasks to evaluate a model on. Among them are:\n",
    "\n",
    "- **HellaSwag**: Tests **common-sense reasoning**. It presents scenarios followed by multiple possible endings, and the model must choose the most plausible one.\n",
    "   - **Evaluation**: Accuracy is the main metric, as the task is multiple-choice. Higher accuracy indicates a better understanding of common-sense knowledge.\n",
    "   \n",
    "- **TruthfulQA**: Evaluates **truthfulness** in responses, specifically targeting the model's propensity to avoid false or misleading information. The dataset contains challenging questions where common misconceptions may lead the model astray.\n",
    "   - **Evaluation**: Scored with accuracy but with a strong emphasis on \"truthfulness.\" Low scores often indicate the model‚Äôs tendency to repeat misconceptions or fabricate plausible-sounding but incorrect information.\n",
    "\n",
    "- **MMLU (Massive Multitask Language Understanding)**: Measures **general knowledge** and **domain-specific expertise** across fields like history, math, and science.\n",
    "   - **Evaluation**: Accuracy on MMLU reflects the model‚Äôs factual knowledge and understanding across diverse subjects, with higher scores indicating broader and deeper knowledge.\n",
    "\n",
    "### Open-Ended Text Generation Evaluation\n",
    "\n",
    "For tasks like story generation, dialogue, and creative writing, classic metrics may fall short. Instead, we rely on qualitative and advanced quantitative measures:\n",
    "\n",
    "- **Human Evaluation**: Direct human assessment is the most reliable for open-ended generation. Annotators rate output on criteria like **relevance**, **coherence**, **factual accuracy**, and **fluency**.\n",
    "   - **Interpretation**: While costly, human evaluations are often conducted in tandem with automated metrics to provide a well-rounded assessment.\n",
    "\n",
    "- **Specific Qualitative Metrics**:\n",
    "   - **Relevance**: Does the output address the prompt directly?\n",
    "   - **Coherence**: Does the text flow logically?\n",
    "   - **Consistency**: Are statements within the response internally consistent?\n",
    "   - **Fluency**: Is the language natural and free of grammatical errors?\n",
    "\n",
    "### LLM Based Evaluation \n",
    "\n",
    "Now the final evaluation method that is getting really popular is to use an LLM to evaluate another LLM's answer. The idea is often to take a bigger (or at least a complete other model) and trust its capacity to evaluate answer. Even though it is quite hard to say for sure that evaluation is 100% accurate, it is remarkably efficient and scalable!\n",
    "\n",
    "\n",
    "<Note type=\"tip\" title=\"Best way to evaluate LLMs\">\n",
    "\n",
    "For LLMs, no single metric tells the full story. Here‚Äôs how to approach evaluation based on task needs:\n",
    "\n",
    "- **For Knowledge and Reasoning Tasks** (like MMLU and HellaSwag):\n",
    "   - **Accuracy** is key, but **MMLU‚Äôs diverse subjects** provide insights into specialized knowledge. **HellaSwag accuracy** also measures the model‚Äôs capacity for common-sense reasoning.\n",
    "\n",
    "- **For Conversational and Creative Tasks**:\n",
    "   - **Human ratings** are crucial, assessing fluency, relevance, and engagement.\n",
    "   - **BLEURT** and **BERTScore** give insights into how closely generated responses match expected content while allowing some creativity in wording.\n",
    "\n",
    "- **For Factuality and Trustworthiness**:\n",
    "   - **TruthfulQA** accuracy checks the model's adherence to truthful responses.\n",
    "   - **Human evaluators** often rate output for factual accuracy, especially in complex or nuanced topics.\n",
    "\n",
    "</Note>\n",
    "\n",
    "\n",
    "## Demo\n",
    "\n",
    "For this course we will do a demo of LLM-based evaluation. We'll take back a fine-tuned model and evaluate it by Llama-3B model. \n",
    "\n",
    "<Note type=\"tip\" title=\"Wanna follow along?\">\n",
    "\n",
    "If you want to follow along:\n",
    "\n",
    "* Open up a LightningAI studio \n",
    "* Switch to GPUs \n",
    "\n",
    "</Note>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's import the validation data \n",
    "# We used json files for our fine-tuning so let's keep using that\n",
    "import json\n",
    "with open(\"data/val.json\", \"r\") as file:\n",
    "    test_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Can I work while attending Jedha Bootcamp part-time?\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Then you will need to apply some formatting \n",
    "# This helps your evaluation model to do its job \n",
    "# Fortunately for us, LitGPT provides a template prompt from Alpaca \n",
    "from litgpt.prompts import Alpaca\n",
    "\n",
    "prompt_style = Alpaca()\n",
    "\n",
    "print(prompt_style.apply(prompt=test_data[0][\"instruction\"], **test_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "# Now we need load our fine-tuned model\n",
    "# We will generate answer from the validation data \n",
    "from litgpt import LLM\n",
    "from tqdm import trange # This is simply to have a nice looking progress. It doesn't matter if you don't use it\n",
    "\n",
    "# Load the LLM\n",
    "llm = LLM.load(\"results/fine-tuned-llama-3.2-1B/final\")\n",
    "\n",
    "# Here we generate answers from the validation dataset\n",
    "for i in trange(len(test_data)):\n",
    "    response = llm.generate(prompt_style.apply(prompt=test_data[i][\"instruction\"], **test_data[i]))\n",
    "    test_data[i][\"response\"] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting HF_HUB_ENABLE_HF_TRANSFER=1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6e9e1db45e4d75bfdc4fc3b24dbcda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d3b6f09cea45cfb1b49202957304cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cbab392100444398d5b520b975786c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57fb1df83aa4e5cbd824b7fc92c8011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cce300afee145a0a2db2b8f51c82015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17167b0c2fe6442d97161fc8b43d0d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec1ebb448024e2694bd2a5f1b7bad89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting checkpoint files to LitGPT format.\n",
      "{'checkpoint_dir': PosixPath('checkpoints/meta-llama/Llama-3.2-3B-Instruct'),\n",
      " 'debug_mode': False,\n",
      " 'dtype': None,\n",
      " 'model_name': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: model-00002-of-00002.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 00:09<00:00, 11.01it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving converted checkpoint to checkpoints/meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Now let's load the evaluation LLM\n",
    "# We will use LLama 3.2 3B Instruct \n",
    "# This is a rather small model trained on specialized data \n",
    "# This should do the job but if you are in more production-like environment\n",
    "# The bigger the LLM, the better obviously \n",
    "del llm # delete previous `llm` to free up GPU memory\n",
    "scorer = LLM.load(\"meta-llama/Llama-3.2-3B-Instruct\", access_token=\"REPLACE_WITH_YOUR_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 10.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 4 of 4\n",
      "Average score: 67.50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's build a function that will provide a system prompt to the evaluation model \n",
    "# and ask it to grade each answer from 0 to 100 \n",
    "from tqdm import tqdm # Again this simply a progression bar. Doesn't matter if you don't use it but it's good looking\n",
    "\n",
    "def generate_model_scores(data_dict, model, response_field=\"response\", target_field=\"output\"):\n",
    "    scores = []\n",
    "    for entry in tqdm(data_dict, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"Given the input `{entry}`\"\n",
    "            f\"and correct output `{entry[target_field]}`, \"\n",
    "            f\"score the model response `{entry[response_field]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "        score = model.generate(prompt, max_new_tokens=50)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return scores\n",
    "\n",
    "# And now we can generate scores for the model\n",
    "scores = generate_model_scores(test_data, model=scorer)\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources üìöüìö\n",
    "\n",
    "* [LLM Evaluation](https://github.com/Lightning-AI/litgpt/blob/main/tutorials/evaluation.md)\n",
    "* [Evaluating Large Language Model (LLM) systems: Metrics, challenges, and best practices](https://medium.com/data-science-at-microsoft/evaluating-llm-systems-metrics-challenges-and-best-practices-664ac25be7e5)\n",
    "* [LLM Evaluation Metrics: The Ultimate LLM Evaluation Guide](https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation)\n",
    "* [A list of metrics for evaluating LLM-generated content](https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/evaluation/list-of-eval-metrics)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
