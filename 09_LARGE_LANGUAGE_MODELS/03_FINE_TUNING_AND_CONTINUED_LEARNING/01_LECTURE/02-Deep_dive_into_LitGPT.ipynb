{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Dive into LitGPT\n",
    "\n",
    "## What you will learn in this course üßêüßê\n",
    "\n",
    "In the previous lecture, we covered a way to fine-tune a model without spending much time on explaining the underlying technology involved in the process. Let's fix that in this lecture and dive deeper into Pytorch Lightning and especially LitGPT.\n",
    "\n",
    "## What is LitGPT\n",
    "\n",
    "LitGPT is a library that is part of the [LightningAI](https://lightning.ai/docs/overview/getting-started) suite which are open source tools based on Pytorch to build, scale and deploy large models, especially LLMs. \n",
    "\n",
    "LitGPT has been specifically made to train and finetune LLMs. \n",
    "\n",
    "## Demo setup \n",
    "\n",
    "To dive deeper into LitGPT, we will need to use GPUs. If you don't have a GPUs on your local machine, we definitely advise you to pull up a **LightningAI Studio**. To do so:\n",
    "\n",
    "1. (If not done already): Create an account on [LightningAI](https://lightning.ai/)\n",
    "2. Go to your Dashboard and click on \"New Studio\" \n",
    "3. Once your Studio is up, simply switch your hardware to A10 GPUs \n",
    "\n",
    "<Note type=\"note\">\n",
    "\n",
    "If you want to use VSCode locally, you can SSH into your LigthningAI studio. Follow the guideline here:\n",
    "\n",
    "* [Connect to Local IDE](https://lightning.ai/docs/overview/studios/connect-local-ide)\n",
    "\n",
    "</Note>\n",
    "\n",
    "Once you are done, open a new terminal on VSCode when you need to run command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the following library \n",
    "# Don't use % if you are already on the terminal\n",
    "%pip install \"litgpt[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: litgpt [-h] [--config CONFIG] [--print_config[=flags]]\n",
      "              {download,chat,finetune,finetune_lora,finetune_full,finetune_adapter,finetune_adapter_v2,pretrain,generate,generate_full,generate_adapter,generate_adapter_v2,generate_sequentially,generate_tp,convert_to_litgpt,convert_from_litgpt,convert_pretrained_checkpoint,merge_lora,evaluate,serve}\n",
      "              ...\n",
      "\n",
      "options:\n",
      "  -h, --help            Show this help message and exit.\n",
      "  --config CONFIG       Path to a configuration file.\n",
      "  --print_config[=flags]\n",
      "                        Print the configuration after applying all other\n",
      "                        arguments and exit. The optional flags customizes the\n",
      "                        output and are one or more keywords separated by\n",
      "                        comma. The supported flags are: comments,\n",
      "                        skip_default, skip_null.\n",
      "\n",
      "subcommands:\n",
      "  For more details of each subcommand, add it as an argument followed by\n",
      "  --help.\n",
      "\n",
      "  Available subcommands:\n",
      "    download            Download weights or tokenizer data from the Hugging\n",
      "                        Face Hub.\n",
      "    chat                Chat with a model.\n",
      "    finetune            Finetune a model using the LoRA method.\n",
      "    finetune_lora       Finetune a model using the LoRA method.\n",
      "    finetune_full       Finetune a model.\n",
      "    finetune_adapter    Finetune a model using the Adapter method.\n",
      "    finetune_adapter_v2\n",
      "                        Finetune a model using the Adapter V2 method.\n",
      "    pretrain            Pretrain a model.\n",
      "    generate            Default generation option.\n",
      "    generate_full       For models finetuned with `litgpt finetune_full`.\n",
      "    generate_adapter    For models finetuned with `litgpt finetune_adapter`.\n",
      "    generate_adapter_v2\n",
      "                        For models finetuned with `litgpt finetune\n",
      "                        adapter_v2`.\n",
      "    generate_sequentially\n",
      "                        Generation script that partitions layers across\n",
      "                        devices to be run sequentially.\n",
      "    generate_tp         Generation script that uses tensor parallelism to run\n",
      "                        across devices.\n",
      "    convert_to_litgpt   Convert a Hugging Face Transformers checkpoint into a\n",
      "                        LitGPT compatible checkpoint.\n",
      "    convert_from_litgpt\n",
      "                        Convert a LitGPT trained checkpoint into a Hugging\n",
      "                        Face Transformers checkpoint.\n",
      "    convert_pretrained_checkpoint\n",
      "                        Convert a checkpoint after pretraining.\n",
      "    merge_lora          Merges the LoRA weights with the base model.\n",
      "    evaluate            Evaluate a model with the LM Evaluation Harness.\n",
      "    serve               Serve a LitGPT model using LitServe.\n"
     ]
    }
   ],
   "source": [
    "# Restart your kernel and test if everything works\n",
    "# Don't use ! if you are in the terminal (not in Jupyter Notebook)\n",
    "!litgpt --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the output above, it means that LitGPT is correctly installed. Now reading from the output above will already help learn about `litgpt`'s capacity. In this course, we will focus on the main components: \n",
    "\n",
    "\n",
    "* Pretraining : Training from scratch an LLM like Llama. So weights are set at random but the model architecture remains the same\n",
    "* Fine-tuning : Optimizing a model's answer based on a custom dataset. This would be equivalent of Transfer Learning that you saw earlier in the program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model (with weights)\n",
    "\n",
    "If you want to fine-tune a model, you will to have one at your disposal. LitGPT includes the most popular open-source LLMs available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify --repo_id <repo_id>. Available values:\n",
      "codellama/CodeLlama-13b-hf\n",
      "codellama/CodeLlama-13b-Instruct-hf\n",
      "codellama/CodeLlama-13b-Python-hf\n",
      "codellama/CodeLlama-34b-hf\n",
      "codellama/CodeLlama-34b-Instruct-hf\n",
      "codellama/CodeLlama-34b-Python-hf\n",
      "codellama/CodeLlama-70b-hf\n",
      "codellama/CodeLlama-70b-Instruct-hf\n",
      "codellama/CodeLlama-70b-Python-hf\n",
      "codellama/CodeLlama-7b-hf\n",
      "codellama/CodeLlama-7b-Instruct-hf\n",
      "codellama/CodeLlama-7b-Python-hf\n",
      "databricks/dolly-v2-12b\n",
      "databricks/dolly-v2-3b\n",
      "databricks/dolly-v2-7b\n",
      "EleutherAI/pythia-1.4b\n",
      "EleutherAI/pythia-1.4b-deduped\n",
      "EleutherAI/pythia-12b\n",
      "EleutherAI/pythia-12b-deduped\n",
      "EleutherAI/pythia-14m\n",
      "EleutherAI/pythia-160m\n",
      "EleutherAI/pythia-160m-deduped\n",
      "EleutherAI/pythia-1b\n",
      "EleutherAI/pythia-1b-deduped\n",
      "EleutherAI/pythia-2.8b\n",
      "EleutherAI/pythia-2.8b-deduped\n",
      "EleutherAI/pythia-31m\n",
      "EleutherAI/pythia-410m\n",
      "EleutherAI/pythia-410m-deduped\n",
      "EleutherAI/pythia-6.9b\n",
      "EleutherAI/pythia-6.9b-deduped\n",
      "EleutherAI/pythia-70m\n",
      "EleutherAI/pythia-70m-deduped\n",
      "garage-bAInd/Camel-Platypus2-13B\n",
      "garage-bAInd/Camel-Platypus2-70B\n",
      "garage-bAInd/Platypus-30B\n",
      "garage-bAInd/Platypus2-13B\n",
      "garage-bAInd/Platypus2-70B\n",
      "garage-bAInd/Platypus2-70B-instruct\n",
      "garage-bAInd/Platypus2-7B\n",
      "garage-bAInd/Stable-Platypus2-13B\n",
      "google/codegemma-7b-it\n",
      "google/gemma-2-27b\n",
      "google/gemma-2-27b-it\n",
      "google/gemma-2-2b\n",
      "google/gemma-2-2b-it\n",
      "google/gemma-2-9b\n",
      "google/gemma-2-9b-it\n",
      "google/gemma-2b\n",
      "google/gemma-2b-it\n",
      "google/gemma-7b\n",
      "google/gemma-7b-it\n",
      "h2oai/h2o-danube2-1.8b-chat\n",
      "keeeeenw/MicroLlama\n",
      "lmsys/longchat-13b-16k\n",
      "lmsys/longchat-7b-16k\n",
      "lmsys/vicuna-13b-v1.3\n",
      "lmsys/vicuna-13b-v1.5\n",
      "lmsys/vicuna-13b-v1.5-16k\n",
      "lmsys/vicuna-33b-v1.3\n",
      "lmsys/vicuna-7b-v1.3\n",
      "lmsys/vicuna-7b-v1.5\n",
      "lmsys/vicuna-7b-v1.5-16k\n",
      "meta-llama/Llama-2-13b-chat-hf\n",
      "meta-llama/Llama-2-13b-hf\n",
      "meta-llama/Llama-2-70b-chat-hf\n",
      "meta-llama/Llama-2-70b-hf\n",
      "meta-llama/Llama-2-7b-chat-hf\n",
      "meta-llama/Llama-2-7b-hf\n",
      "meta-llama/Llama-3.2-1B\n",
      "meta-llama/Llama-3.2-1B-Instruct\n",
      "meta-llama/Llama-3.2-3B\n",
      "meta-llama/Llama-3.2-3B-Instruct\n",
      "meta-llama/Meta-Llama-3-70B\n",
      "meta-llama/Meta-Llama-3-70B-Instruct\n",
      "meta-llama/Meta-Llama-3-8B\n",
      "meta-llama/Meta-Llama-3-8B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-405B\n",
      "meta-llama/Meta-Llama-3.1-405B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-70B\n",
      "meta-llama/Meta-Llama-3.1-70B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-8B\n",
      "meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "microsoft/phi-1_5\n",
      "microsoft/phi-2\n",
      "microsoft/Phi-3-mini-128k-instruct\n",
      "microsoft/Phi-3-mini-4k-instruct\n",
      "microsoft/Phi-3.5-mini-instruct\n",
      "mistralai/mathstral-7B-v0.1\n",
      "mistralai/Mistral-7B-Instruct-v0.1\n",
      "mistralai/Mistral-7B-Instruct-v0.2\n",
      "mistralai/Mistral-7B-Instruct-v0.3\n",
      "mistralai/Mistral-7B-v0.1\n",
      "mistralai/Mistral-7B-v0.3\n",
      "mistralai/Mistral-Large-Instruct-2407\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "mistralai/Mixtral-8x7B-v0.1\n",
      "NousResearch/Nous-Hermes-13b\n",
      "NousResearch/Nous-Hermes-llama-2-7b\n",
      "NousResearch/Nous-Hermes-Llama2-13b\n",
      "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF\n",
      "openlm-research/open_llama_13b\n",
      "openlm-research/open_llama_3b\n",
      "openlm-research/open_llama_7b\n",
      "stabilityai/FreeWilly2\n",
      "stabilityai/stable-code-3b\n",
      "stabilityai/stablecode-completion-alpha-3b\n",
      "stabilityai/stablecode-completion-alpha-3b-4k\n",
      "stabilityai/stablecode-instruct-alpha-3b\n",
      "stabilityai/stablelm-3b-4e1t\n",
      "stabilityai/stablelm-base-alpha-3b\n",
      "stabilityai/stablelm-base-alpha-7b\n",
      "stabilityai/stablelm-tuned-alpha-3b\n",
      "stabilityai/stablelm-tuned-alpha-7b\n",
      "stabilityai/stablelm-zephyr-3b\n",
      "tiiuae/falcon-180B\n",
      "tiiuae/falcon-180B-chat\n",
      "tiiuae/falcon-40b\n",
      "tiiuae/falcon-40b-instruct\n",
      "tiiuae/falcon-7b\n",
      "tiiuae/falcon-7b-instruct\n",
      "TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\n",
      "togethercomputer/LLaMA-2-7B-32K\n",
      "togethercomputer/RedPajama-INCITE-7B-Base\n",
      "togethercomputer/RedPajama-INCITE-7B-Chat\n",
      "togethercomputer/RedPajama-INCITE-7B-Instruct\n",
      "togethercomputer/RedPajama-INCITE-Base-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Base-7B-v0.1\n",
      "togethercomputer/RedPajama-INCITE-Chat-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Chat-7B-v0.1\n",
      "togethercomputer/RedPajama-INCITE-Instruct-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Instruct-7B-v0.1\n",
      "Trelis/Llama-2-7b-chat-hf-function-calling-v2\n",
      "unsloth/Mistral-7B-v0.2\n"
     ]
    }
   ],
   "source": [
    "# List all the downloadable models\n",
    "# Don't use ! if you are in the terminal (not in Jupyter Notebook)\n",
    "!litgpt download list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download LLama-3.2-1B which is relatively lightweight but still pretty performant. \n",
    "\n",
    "<Note type=\"important\">\n",
    "\n",
    "If you are following along and want to try other models, keep in mind that the more parameters you select the heavier the model will be not just in terms of storage but also in terms of memory (RAM). Don't use too large models for your local computer otherwise it will freeze. \n",
    "\n",
    "</Note>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting HF_HUB_ENABLE_HF_TRANSFER=1\n",
      "config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 608/608 [00:00<00:00, 3.13MB/s]\n",
      "generation_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:00<00:00, 321kB/s]\n",
      "model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 2.20G/2.20G [00:27<00:00, 78.8MB/s]\n",
      "tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.84M/1.84M [00:00<00:00, 6.98MB/s]\n",
      "tokenizer.model: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500k/500k [00:00<00:00, 21.0MB/s]\n",
      "tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.29k/1.29k [00:00<00:00, 4.25MB/s]\n",
      "Converting .safetensor files to PyTorch binaries (.bin)\n",
      "checkpoints/TinyLlama/TinyLlama-1.1B-Chat-v1.0/model.safetensors --> checkpoints/TinyLlama/TinyLlama-1.1B-Chat-v1.0/model.bin\n",
      "Converting checkpoint files to LitGPT format.\n",
      "{'checkpoint_dir': PosixPath('checkpoints/TinyLlama/TinyLlama-1.1B-Chat-v1.0'),\n",
      " 'debug_mode': False,\n",
      " 'dtype': None,\n",
      " 'model_name': None}\n",
      "Loading weights: model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 00:02<00:00, 33.65it/s\n",
      "Saving converted checkpoint to checkpoints/TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
     ]
    }
   ],
   "source": [
    "# Download TinyLLama (a small copy of LLama)\n",
    "# Don't use ! if you are in the terminal (not in Jupyter Notebook)\n",
    "!litgpt download meta-llama/Llama-3.2-1B --access_token REPLACE_WITH_YOUR_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Note type=\"important\" title=\"If you want to use LLama official models\">\n",
    "\n",
    "If you want to use real LLama model from Meta, you will need to:\n",
    "\n",
    "1. Go to HuggingFace and [accept Meta's term of use](https://huggingface.co/meta-llama/Llama-3.2-1B)\n",
    "2. Wait for the approval from Meta \n",
    "3. Then provide your HuggingFace token as parameter of your bash command like this:\n",
    "\n",
    "```bash\n",
    "!litgpt download --repo_id meta-llama/Llama-3.2-1B --access_token=YOUR_HF_TOKEN\n",
    "```\n",
    "\n",
    "You can get your access token by clicking on your profile > ACCESS TOKENS\n",
    "\n",
    "It should take about 30 minutes for you to get approved. \n",
    "\n",
    "</Note>\n",
    "\n",
    "\n",
    "Once your command has been successfully executed, you should see a `checkpoint/NAME_OF_YOUR_MODEL` directory that should be at the same place as where you executed the command. \n",
    "\n",
    "Now you should be able to interact with your model the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'access_token': None,\n",
      " 'checkpoint_dir': PosixPath('checkpoints/TinyLlama/TinyLlama-1.1B-Chat-v1.0'),\n",
      " 'compile': False,\n",
      " 'max_new_tokens': 50,\n",
      " 'multiline': False,\n",
      " 'precision': None,\n",
      " 'quantize': None,\n",
      " 'temperature': 0.8,\n",
      " 'top_k': 50,\n",
      " 'top_p': 1.0}\n",
      "Now chatting with tiny-llama-1.1b-chat.\n",
      "To exit, press 'Enter' on an empty prompt.\n",
      "\n",
      "Seed set to 1234\n",
      ">> Prompt: ^C\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS COMMAND IN A STANDALONE TERMINAL NOT IN JUPYTER \n",
    "# otherwise you won't be able to interact üòâ\n",
    "# Don't use ! if you are in the terminal (not in Jupyter Notebook)\n",
    "!litgpt chat checkpoints/meta-llama/Llama-3.2-1B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning \n",
    "\n",
    "Now if you want to fine-tune a model, you will need first to build a dataset that follows this structure:\n",
    "\n",
    "```json \n",
    "[\n",
    "    {\n",
    "        \"instruction\": \"THIS IS THE USER PROMPT\",\n",
    "        \"input\": \"THIS IS ADDITIONAL CONTEXT FROM THE INSTRUCTION\",\n",
    "        \"output\": \"THIS IS THE EXPECTED ANSWER\"\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "You can download the template files here:\n",
    "\n",
    "* [train.json](https://full-stack-assets.s3.eu-west-3.amazonaws.com/train.json)\n",
    "* [val.json](https://full-stack-assets.s3.eu-west-3.amazonaws.com/val.json)\n",
    "\n",
    "Then you can simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'access_token': None,\n",
      " 'checkpoint_dir': PosixPath('checkpoints/TinyLlama/TinyLlama-1.1B-Chat-v1.0'),\n",
      " 'data': JSON(json_path=PosixPath('/home/jovyan/00-Lectures/src'),\n",
      "              mask_prompt=False,\n",
      "              val_split_fraction=None,\n",
      "              prompt_style=<litgpt.prompts.Alpaca object at 0xffff49199810>,\n",
      "              ignore_index=-100,\n",
      "              seed=42,\n",
      "              num_workers=4),\n",
      " 'devices': 1,\n",
      " 'eval': EvalArgs(interval=100,\n",
      "                  max_new_tokens=100,\n",
      "                  max_iters=100,\n",
      "                  initial_validation=False,\n",
      "                  final_validation=True,\n",
      "                  evaluate_example='first'),\n",
      " 'logger_name': 'csv',\n",
      " 'lora_alpha': 16,\n",
      " 'lora_dropout': 0.05,\n",
      " 'lora_head': False,\n",
      " 'lora_key': False,\n",
      " 'lora_mlp': False,\n",
      " 'lora_projection': False,\n",
      " 'lora_query': True,\n",
      " 'lora_r': 8,\n",
      " 'lora_value': True,\n",
      " 'num_nodes': 1,\n",
      " 'optimizer': 'AdamW',\n",
      " 'out_dir': PosixPath('results/fine-tuned-tiny-llama'),\n",
      " 'precision': None,\n",
      " 'quantize': None,\n",
      " 'seed': 1337,\n",
      " 'train': TrainArgs(save_interval=1000,\n",
      "                    log_interval=1,\n",
      "                    global_batch_size=32,\n",
      "                    micro_batch_size=1,\n",
      "                    lr_warmup_steps=100,\n",
      "                    lr_warmup_fraction=None,\n",
      "                    epochs=5,\n",
      "                    max_tokens=None,\n",
      "                    max_steps=None,\n",
      "                    max_seq_length=None,\n",
      "                    tie_embeddings=None,\n",
      "                    max_norm=None,\n",
      "                    min_lr=6e-05)}\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "Seed set to 1337\n",
      "Number of trainable parameters: 1,126,400\n",
      "Number of non-trainable parameters: 1,100,048,384\n",
      "The longest sequence length in the train data is 124, the model's maximum sequence length is 124 and context length is 2048\n",
      "Verifying settings ...\n",
      "Epoch 1 | iter 1 step 0 | loss train: 3.028, val: n/a | iter time: 363842.23 ms\n"
     ]
    }
   ],
   "source": [
    "# Define the path of your model in the first line \n",
    "# 2nd line: Set the data format to JSON (for custom data)\n",
    "# 3rd line: Path to the training data in JSONL format\n",
    "# 4th line: Path for saving the fine-tuned model output\n",
    "# 5th line: Define the Batch size \n",
    "# 6th line: Define the number of Epochs\n",
    "# Don't use ! if you are in the terminal (not in Jupyter Notebook)\n",
    "!litgpt finetune_lora checkpoints/meta-llama/Llama-3.2-1B \\\n",
    "--data JSON \\\n",
    "--data.json_path $(pwd)/data \\\n",
    "--out_dir results/fine-tuned-llama-3.2-1B \\\n",
    "--train.global_batch_size=32 \\\n",
    "--train.epochs=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further fine-tune models\n",
    "\n",
    "If you want to further train a model, you can actually use the same commands as before, the only thing that will change will be the directory where the new model is stored:\n",
    "\n",
    "```bash\n",
    "# 1st line: Change the path to the directory where the results are\n",
    "litgpt finetune_lora results/fine-tuned-llama-3.2-1B/final \\\n",
    "--data JSON \\\n",
    "--data.json_path $(pwd)/00-Lectures/src \\\n",
    "--out_dir results/fine-tuned-llama-3.2-1B \\\n",
    "--train.global_batch_size=32 \\\n",
    "--train.epochs=5\n",
    "```\n",
    "\n",
    "If you need to further configure your model, I advise you to create a `config.yaml` file and then add it as a flag in your bash command:\n",
    "\n",
    "```bash\n",
    "litgpt results/fine-tuned-llama-3.2-1B/final/ \\\n",
    "--config config.yaml\n",
    "```\n",
    "\n",
    "You can use the following template for your `config.yaml`:\n",
    "\n",
    "* [Configuration template](https://full-stack-assets.s3.eu-west-3.amazonaws.com/config.yaml)\n",
    "\n",
    "<Note type=\"tip\">\n",
    "\n",
    "Keep the `results` folder somewhere. This is what you will use whenever you want to stage your model into production later on üòâ\n",
    "\n",
    "</Note>\n",
    "\n",
    "## Pretraining\n",
    "\n",
    "Now the only thing with these commands is that when you launch a job, you will see this output:\n",
    "\n",
    "```bash \n",
    "Number of trainable parameters: 1,126,400\n",
    "Number of non-trainable parameters: 1,100,048,384\n",
    "```\n",
    "\n",
    "This means that most of the model's weights won't be further trained. If that's what you want to do, you will need to use another method called `pretrain`. Let's see an example:\n",
    "\n",
    "<Note type=\"important\" type=\"disclaimer\">\n",
    "\n",
    "As of today, Pretraining (whether with Python or with the command-line) is pretty limited as custom dataset is not extremely well supported (see the code below for more info). This will be fixed over time. \n",
    "\n",
    "However, again I want to stress out the fact that Pretraining LLMs from scratch is very costly and therefore, you will see poor performance unless you have:\n",
    "\n",
    "* A 1TB dataset to train on \n",
    "* A large model with large GPUs \n",
    "\n",
    "\n",
    "The example below is mainly to illustrate this point, and unless you will be working in tech company that works on foundational models, you most likely won't need to train your own LLM from scratch (and we definitely don't advise you to!)\n",
    "\n",
    "</Note>\n",
    "\n",
    "### Prepare dataset \n",
    "\n",
    "Before running a training job, we need to prepare a dataset. LitGPT accepts two type of data:\n",
    "\n",
    "- `.txt` local files \n",
    "- [LitData](https://github.com/Lightning-AI/litdata): These are datasets optimized via LitData and hosted on an S3\n",
    "\n",
    "For this course we will cover the first option. You can download the whole dataset here:\n",
    "\n",
    "- [Star Wars Text Dataset](https://full-stack-assets.s3.eu-west-3.amazonaws.com/Text_files.zip)\n",
    "\n",
    "Now all you have to do is to run:\n",
    "\n",
    "```bash \n",
    "# 1st line: This is a limitation but you absolutely need to specify a \"base\" model from the litgpt pretrain list \n",
    "# 2nd line: Since we fine tuned a model based on tiny llama, let's try to further pretrain it without locking any weights\n",
    "# 3nd line: Define the output directory\n",
    "# 4th line: Define the type of data to receive a .txt files \n",
    "# 5th line: This define the path where the .txt files should be (here at ./Text_files)\n",
    "# 6th line: Set a maximum number of tokens \n",
    "# 7th line: Give a maximum sequence length of tokens. AS OF TODAY there is a bug on the library and you need to add a number that is below 2048\n",
    "# See more here: https://github.com/Lightning-AI/litgpt/issues/1450\n",
    "litgpt pretrain meta-llama/Llama-3.2-1B \\\n",
    "   --initial_checkpoint_dir results/fine-tuned-tiny-llama/final \\\n",
    "   --tokenizer_dir results/fine-tuned-llama-3.2-1B/final \\\n",
    "   --out_dir ./new_pretrained_checkpoint \\\n",
    "   --data TextFiles \\\n",
    "   --data.train_data_path Text_files \\\n",
    "   --train.max_tokens 1_000_000 \\\n",
    "   --train.max_seq_length 1000\n",
    "```\n",
    "\n",
    "By default, the training job will run for 6 epochs. Once it is done, you can try your new model:\n",
    "\n",
    "```bash \n",
    "litgpt chat new_pretrained_checkpoint/final\n",
    "```\n",
    "\n",
    "You might be a bit disappointed with the results as you might see something along the lines below:\n",
    "\n",
    "```\n",
    "him ! ! him very ... oh!l a h...!.! .!. him. oh!  !  !'.... !..! oh... it..! him  !\n",
    "```\n",
    "\n",
    "This is normal, you have two phenomenon coming into play:\n",
    "\n",
    "1. The dataset is relatively small and there are a lot of `him` / `very` / `oh` tokens. So the model is blindingly repeating those words \n",
    "2. When training from scratch, there is a possibility of forgetting previous training epochs : the loss is becoming higher as we go through more epochs \n",
    "\n",
    "A solution would be to increase the size of the dataset and increase the number of epochs for the model to learn better the subtleties of the dataset. This is a great example to illustrate that continued learning is the costliest option when it comes to training LLMs \n",
    "\n",
    "<Note type=\"important\">\n",
    "\n",
    "If you want to follow along and run the above pretraining script, you will need to change your Studio to **L40S** GPUs as it requires a lot of RAMs to load the full model and start training it.\n",
    "\n",
    "</Note>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python API\n",
    "\n",
    "Playing on the terminal is great but you might want to have further control and use litgpt in an API. If that's the case, you should use LitGPT Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The answer is simple: Darth Vader. He is the most powerful Jedi in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the"
     ]
    }
   ],
   "source": [
    "from litgpt import LLM\n",
    "\n",
    "llm = LLM.load(\"checkpoints/meta-llama/Llama-3.2-1B\") # You can also add the path of any of your checkpoints\n",
    "text = llm.generate(\"What is the name of the most powerful Jedi in the galaxy?\", top_k=1, max_new_tokens=300)\n",
    "\n",
    "\n",
    "# For text generation in streaming \n",
    "for token in text:\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Note type=\"note\">\n",
    "\n",
    "LitGPT technically also support pretraining in Python but it is pretty limited as of today as you won't be able to use custom dataset (documentation will show you how to, but the code won't work). However, if you are curious, feel free to checkout the documentation:\n",
    "\n",
    "- [Python API for Pretraining jobs](https://github.com/Lightning-AI/litgpt/blob/main/tutorials/python-api.md)\n",
    "\n",
    "</Note>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources üìöüìö\n",
    "\n",
    "* [LightiningAI](https://lightning.ai/docs/overview/getting-started)\n",
    "* [Connect to Local IDE](https://lightning.ai/docs/overview/studios/connect-local-ide)\n",
    "* [LitData](https://github.com/Lightning-AI/litdata)\n",
    "* [Python API](https://github.com/Lightning-AI/litgpt/blob/main/tutorials/python-api.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
