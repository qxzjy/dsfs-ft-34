{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Dive into LitGPT\n",
    "\n",
    "## What you will learn in this course üßêüßê\n",
    "\n",
    "In the previous lecture, we covered a way to fine-tune a model without spending much time on explaining the underlying technology involved in the process. Let's fix that in this lecture and dive deeper into Pytorch Lightning and especially LitGPT.\n",
    "\n",
    "## What is LitGPT\n",
    "\n",
    "LitGPT is a library that is part of the [LightningAI](https://lightning.ai/docs/overview/getting-started) suite which are open source tools based on Pytorch to build, scale and deploy large models, especially LLMs. \n",
    "\n",
    "LitGPT has been specifically made to train and finetune LLMs. \n",
    "\n",
    "## Demo setup \n",
    "\n",
    "To dive deeper into LitGPT, we will need to use GPUs. If you don't have a GPUs on your local machine, we definitely advise you to pull up a **LightningAI Studio**. To do so:\n",
    "\n",
    "1. (If not done already): Create an account on [LightningAI](https://lightning.ai/)\n",
    "2. Go to your Dashboard and click on \"New Studio\" \n",
    "3. Once your Studio is up, simply switch your hardware to A10 GPUs \n",
    "\n",
    "<Note type=\"note\">\n",
    "\n",
    "If you want to use VSCode locally, you can SSH into your LigthningAI studio. Follow the guideline here:\n",
    "\n",
    "* [Connect to Local IDE](https://lightning.ai/docs/overview/studios/connect-local-ide)\n",
    "\n",
    "</Note>\n",
    "\n",
    "Once you are done, open a new terminal on VSCode when you need to run command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the following library \n",
    "# Don't use % if you are already on the terminal\n",
    "%pip install \"litgpt[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: litgpt [-h] [--config CONFIG] [--print_config[=flags]]\n",
      "              {download,chat,finetune,finetune_lora,finetune_full,finetune_adapter,finetune_adapter_v2,pretrain,generate,generate_full,generate_adapter,generate_adapter_v2,generate_sequentially,generate_tp,convert_to_litgpt,convert_from_litgpt,convert_pretrained_checkpoint,merge_lora,evaluate,serve}\n",
      "              ...\n",
      "\n",
      "options:\n",
      "  -h, --help            Show this help message and exit.\n",
      "  --config CONFIG       Path to a configuration file.\n",
      "  --print_config[=flags]\n",
      "                        Print the configuration after applying all other\n",
      "                        arguments and exit. The optional flags customizes the\n",
      "                        output and are one or more keywords separated by\n",
      "                        comma. The supported flags are: comments,\n",
      "                        skip_default, skip_null.\n",
      "\n",
      "subcommands:\n",
      "  For more details of each subcommand, add it as an argument followed by\n",
      "  --help.\n",
      "\n",
      "  Available subcommands:\n",
      "    download            Download weights or tokenizer data from the Hugging\n",
      "                        Face Hub.\n",
      "    chat                Chat with a model.\n",
      "    finetune            Finetune a model using the LoRA method.\n",
      "    finetune_lora       Finetune a model using the LoRA method.\n",
      "    finetune_full       Finetune a model.\n",
      "    finetune_adapter    Finetune a model using the Adapter method.\n",
      "    finetune_adapter_v2\n",
      "                        Finetune a model using the Adapter V2 method.\n",
      "    pretrain            Pretrain a model.\n",
      "    generate            Default generation option.\n",
      "    generate_full       For models finetuned with `litgpt finetune_full`.\n",
      "    generate_adapter    For models finetuned with `litgpt finetune_adapter`.\n",
      "    generate_adapter_v2\n",
      "                        For models finetuned with `litgpt finetune\n",
      "                        adapter_v2`.\n",
      "    generate_sequentially\n",
      "                        Generation script that partitions layers across\n",
      "                        devices to be run sequentially.\n",
      "    generate_tp         Generation script that uses tensor parallelism to run\n",
      "                        across devices.\n",
      "    convert_to_litgpt   Convert a Hugging Face Transformers checkpoint into a\n",
      "                        LitGPT compatible checkpoint.\n",
      "    convert_from_litgpt\n",
      "                        Convert a LitGPT trained checkpoint into a Hugging\n",
      "                        Face Transformers checkpoint.\n",
      "    convert_pretrained_checkpoint\n",
      "                        Convert a checkpoint after pretraining.\n",
      "    merge_lora          Merges the LoRA weights with the base model.\n",
      "    evaluate            Evaluate a model with the LM Evaluation Harness.\n",
      "    serve               Serve a LitGPT model using LitServe.\n"
     ]
    }
   ],
   "source": [
    "# Restart your kernel and test if everything works\n",
    "# Don't use ! if you are in the terminal (not in Jupyter Notebook)\n",
    "!litgpt --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the output above, it means that LitGPT is correctly installed. Now reading from the output above will already help learn about `litgpt`'s capacity. In this course, we will focus on the main components: \n",
    "\n",
    "\n",
    "* Pretraining : Training from scratch an LLM like Llama. So weights are set at random but the model architecture remains the same\n",
    "* Fine-tuning : Optimizing a model's answer based on a custom dataset. This would be equivalent of Transfer Learning that you saw earlier in the program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model (with weights)\n",
    "\n",
    "If you want to fine-tune a model, you will to have one at your disposal. LitGPT includes the most popular open-source LLMs available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify --repo_id <repo_id>. Available values:\n",
      "allenai/OLMo-1B-hf\n",
      "allenai/OLMo-7B-hf\n",
      "allenai/OLMo-7B-Instruct-hf\n",
      "BSC-LT/salamandra-2b\n",
      "BSC-LT/salamandra-2b-instruct\n",
      "BSC-LT/salamandra-7b\n",
      "BSC-LT/salamandra-7b-instruct\n",
      "codellama/CodeLlama-13b-hf\n",
      "codellama/CodeLlama-13b-Instruct-hf\n",
      "codellama/CodeLlama-13b-Python-hf\n",
      "codellama/CodeLlama-34b-hf\n",
      "codellama/CodeLlama-34b-Instruct-hf\n",
      "codellama/CodeLlama-34b-Python-hf\n",
      "codellama/CodeLlama-70b-hf\n",
      "codellama/CodeLlama-70b-Instruct-hf\n",
      "codellama/CodeLlama-70b-Python-hf\n",
      "codellama/CodeLlama-7b-hf\n",
      "codellama/CodeLlama-7b-Instruct-hf\n",
      "codellama/CodeLlama-7b-Python-hf\n",
      "deepseek-ai/DeepSeek-R1-Distill-Llama-70B\n",
      "deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
      "EleutherAI/pythia-1.4b\n",
      "EleutherAI/pythia-1.4b-deduped\n",
      "EleutherAI/pythia-12b\n",
      "EleutherAI/pythia-12b-deduped\n",
      "EleutherAI/pythia-14m\n",
      "EleutherAI/pythia-160m\n",
      "EleutherAI/pythia-160m-deduped\n",
      "EleutherAI/pythia-1b\n",
      "EleutherAI/pythia-1b-deduped\n",
      "EleutherAI/pythia-2.8b\n",
      "EleutherAI/pythia-2.8b-deduped\n",
      "EleutherAI/pythia-31m\n",
      "EleutherAI/pythia-410m\n",
      "EleutherAI/pythia-410m-deduped\n",
      "EleutherAI/pythia-6.9b\n",
      "EleutherAI/pythia-6.9b-deduped\n",
      "EleutherAI/pythia-70m\n",
      "EleutherAI/pythia-70m-deduped\n",
      "garage-bAInd/Camel-Platypus2-13B\n",
      "garage-bAInd/Camel-Platypus2-70B\n",
      "garage-bAInd/Platypus-30B\n",
      "garage-bAInd/Platypus2-13B\n",
      "garage-bAInd/Platypus2-70B\n",
      "garage-bAInd/Platypus2-70B-instruct\n",
      "garage-bAInd/Platypus2-7B\n",
      "garage-bAInd/Stable-Platypus2-13B\n",
      "google/codegemma-7b-it\n",
      "google/gemma-2-27b\n",
      "google/gemma-2-27b-it\n",
      "google/gemma-2-2b\n",
      "google/gemma-2-2b-it\n",
      "google/gemma-2-9b\n",
      "google/gemma-2-9b-it\n",
      "google/gemma-2b\n",
      "google/gemma-2b-it\n",
      "google/gemma-3-12b-it\n",
      "google/gemma-3-1b-it\n",
      "google/gemma-3-27b-it\n",
      "google/gemma-3-4b-it\n",
      "google/gemma-7b\n",
      "google/gemma-7b-it\n",
      "HuggingFaceTB/SmolLM2-1.7B\n",
      "HuggingFaceTB/SmolLM2-1.7B-Instruct\n",
      "HuggingFaceTB/SmolLM2-135M\n",
      "HuggingFaceTB/SmolLM2-135M-Instruct\n",
      "HuggingFaceTB/SmolLM2-360M\n",
      "HuggingFaceTB/SmolLM2-360M-Instruct\n",
      "keeeeenw/MicroLlama\n",
      "meta-llama/Llama-2-13b-chat-hf\n",
      "meta-llama/Llama-2-13b-hf\n",
      "meta-llama/Llama-2-70b-chat-hf\n",
      "meta-llama/Llama-2-70b-hf\n",
      "meta-llama/Llama-2-7b-chat-hf\n",
      "meta-llama/Llama-2-7b-hf\n",
      "meta-llama/Llama-3.2-1B\n",
      "meta-llama/Llama-3.2-1B-Instruct\n",
      "meta-llama/Llama-3.2-3B\n",
      "meta-llama/Llama-3.2-3B-Instruct\n",
      "meta-llama/Llama-3.3-70B-Instruct\n",
      "meta-llama/Meta-Llama-3-70B\n",
      "meta-llama/Meta-Llama-3-70B-Instruct\n",
      "meta-llama/Meta-Llama-3-8B\n",
      "meta-llama/Meta-Llama-3-8B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-405B\n",
      "meta-llama/Meta-Llama-3.1-405B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-70B\n",
      "meta-llama/Meta-Llama-3.1-70B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-8B\n",
      "meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "microsoft/phi-1_5\n",
      "microsoft/phi-2\n",
      "microsoft/Phi-3-mini-128k-instruct\n",
      "microsoft/Phi-3-mini-4k-instruct\n",
      "microsoft/Phi-3.5-mini-instruct\n",
      "microsoft/phi-4\n",
      "microsoft/Phi-4-mini-instruct\n",
      "mistralai/mathstral-7B-v0.1\n",
      "mistralai/Mistral-7B-Instruct-v0.1\n",
      "mistralai/Mistral-7B-Instruct-v0.2\n",
      "mistralai/Mistral-7B-Instruct-v0.3\n",
      "mistralai/Mistral-7B-v0.1\n",
      "mistralai/Mistral-7B-v0.3\n",
      "mistralai/Mistral-Large-Instruct-2407\n",
      "mistralai/Mistral-Large-Instruct-2411\n",
      "mistralai/Mixtral-8x22B-Instruct-v0.1\n",
      "mistralai/Mixtral-8x22B-v0.1\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "mistralai/Mixtral-8x7B-v0.1\n",
      "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF\n",
      "openlm-research/open_llama_13b\n",
      "openlm-research/open_llama_3b\n",
      "openlm-research/open_llama_7b\n",
      "Qwen/Qwen2.5-0.5B\n",
      "Qwen/Qwen2.5-0.5B-Instruct\n",
      "Qwen/Qwen2.5-1.5B\n",
      "Qwen/Qwen2.5-1.5B-Instruct\n",
      "Qwen/Qwen2.5-14B\n",
      "Qwen/Qwen2.5-14B-Instruct\n",
      "Qwen/Qwen2.5-32B\n",
      "Qwen/Qwen2.5-32B-Instruct\n",
      "Qwen/Qwen2.5-3B\n",
      "Qwen/Qwen2.5-3B-Instruct\n",
      "Qwen/Qwen2.5-72B\n",
      "Qwen/Qwen2.5-72B-Instruct\n",
      "Qwen/Qwen2.5-7B\n",
      "Qwen/Qwen2.5-7B-Instruct\n",
      "Qwen/Qwen2.5-Coder-0.5B\n",
      "Qwen/Qwen2.5-Coder-0.5B-Instruct\n",
      "Qwen/Qwen2.5-Coder-1.5B\n",
      "Qwen/Qwen2.5-Coder-1.5B-Instruct\n",
      "Qwen/Qwen2.5-Coder-14B\n",
      "Qwen/Qwen2.5-Coder-14B-Instruct\n",
      "Qwen/Qwen2.5-Coder-32B\n",
      "Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "Qwen/Qwen2.5-Coder-3B\n",
      "Qwen/Qwen2.5-Coder-3B-Instruct\n",
      "Qwen/Qwen2.5-Coder-7B\n",
      "Qwen/Qwen2.5-Coder-7B-Instruct\n",
      "Qwen/Qwen2.5-Math-1.5B\n",
      "Qwen/Qwen2.5-Math-1.5B-Instruct\n",
      "Qwen/Qwen2.5-Math-72B\n",
      "Qwen/Qwen2.5-Math-72B-Instruct\n",
      "Qwen/Qwen2.5-Math-7B\n",
      "Qwen/Qwen2.5-Math-7B-Instruct\n",
      "Qwen/QwQ-32B\n",
      "Qwen/QwQ-32B-Preview\n",
      "stabilityai/FreeWilly2\n",
      "stabilityai/stable-code-3b\n",
      "stabilityai/stablecode-completion-alpha-3b\n",
      "stabilityai/stablecode-completion-alpha-3b-4k\n",
      "stabilityai/stablecode-instruct-alpha-3b\n",
      "stabilityai/stablelm-3b-4e1t\n",
      "stabilityai/stablelm-base-alpha-3b\n",
      "stabilityai/stablelm-base-alpha-7b\n",
      "stabilityai/stablelm-tuned-alpha-3b\n",
      "stabilityai/stablelm-tuned-alpha-7b\n",
      "stabilityai/stablelm-zephyr-3b\n",
      "tiiuae/falcon-180B\n",
      "tiiuae/falcon-180B-chat\n",
      "tiiuae/falcon-40b\n",
      "tiiuae/falcon-40b-instruct\n",
      "tiiuae/falcon-7b\n",
      "tiiuae/falcon-7b-instruct\n",
      "tiiuae/Falcon3-10B-Base\n",
      "tiiuae/Falcon3-10B-Instruct\n",
      "tiiuae/Falcon3-1B-Base\n",
      "tiiuae/Falcon3-1B-Instruct\n",
      "tiiuae/Falcon3-3B-Base\n",
      "tiiuae/Falcon3-3B-Instruct\n",
      "tiiuae/Falcon3-7B-Base\n",
      "tiiuae/Falcon3-7B-Instruct\n",
      "TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\n",
      "togethercomputer/LLaMA-2-7B-32K\n",
      "Trelis/Llama-2-7b-chat-hf-function-calling-v2\n",
      "unsloth/Mistral-7B-v0.2\n"
     ]
    }
   ],
   "source": [
    "!litgpt download list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify --repo_id <repo_id>. Available values:\n",
      "codellama/CodeLlama-13b-hf\n",
      "codellama/CodeLlama-13b-Instruct-hf\n",
      "codellama/CodeLlama-13b-Python-hf\n",
      "codellama/CodeLlama-34b-hf\n",
      "codellama/CodeLlama-34b-Instruct-hf\n",
      "codellama/CodeLlama-34b-Python-hf\n",
      "codellama/CodeLlama-70b-hf\n",
      "codellama/CodeLlama-70b-Instruct-hf\n",
      "codellama/CodeLlama-70b-Python-hf\n",
      "codellama/CodeLlama-7b-hf\n",
      "codellama/CodeLlama-7b-Instruct-hf\n",
      "codellama/CodeLlama-7b-Python-hf\n",
      "databricks/dolly-v2-12b\n",
      "databricks/dolly-v2-3b\n",
      "databricks/dolly-v2-7b\n",
      "EleutherAI/pythia-1.4b\n",
      "EleutherAI/pythia-1.4b-deduped\n",
      "EleutherAI/pythia-12b\n",
      "EleutherAI/pythia-12b-deduped\n",
      "EleutherAI/pythia-14m\n",
      "EleutherAI/pythia-160m\n",
      "EleutherAI/pythia-160m-deduped\n",
      "EleutherAI/pythia-1b\n",
      "EleutherAI/pythia-1b-deduped\n",
      "EleutherAI/pythia-2.8b\n",
      "EleutherAI/pythia-2.8b-deduped\n",
      "EleutherAI/pythia-31m\n",
      "EleutherAI/pythia-410m\n",
      "EleutherAI/pythia-410m-deduped\n",
      "EleutherAI/pythia-6.9b\n",
      "EleutherAI/pythia-6.9b-deduped\n",
      "EleutherAI/pythia-70m\n",
      "EleutherAI/pythia-70m-deduped\n",
      "garage-bAInd/Camel-Platypus2-13B\n",
      "garage-bAInd/Camel-Platypus2-70B\n",
      "garage-bAInd/Platypus-30B\n",
      "garage-bAInd/Platypus2-13B\n",
      "garage-bAInd/Platypus2-70B\n",
      "garage-bAInd/Platypus2-70B-instruct\n",
      "garage-bAInd/Platypus2-7B\n",
      "garage-bAInd/Stable-Platypus2-13B\n",
      "google/codegemma-7b-it\n",
      "google/gemma-2-27b\n",
      "google/gemma-2-27b-it\n",
      "google/gemma-2-2b\n",
      "google/gemma-2-2b-it\n",
      "google/gemma-2-9b\n",
      "google/gemma-2-9b-it\n",
      "google/gemma-2b\n",
      "google/gemma-2b-it\n",
      "google/gemma-7b\n",
      "google/gemma-7b-it\n",
      "h2oai/h2o-danube2-1.8b-chat\n",
      "keeeeenw/MicroLlama\n",
      "lmsys/longchat-13b-16k\n",
      "lmsys/longchat-7b-16k\n",
      "lmsys/vicuna-13b-v1.3\n",
      "lmsys/vicuna-13b-v1.5\n",
      "lmsys/vicuna-13b-v1.5-16k\n",
      "lmsys/vicuna-33b-v1.3\n",
      "lmsys/vicuna-7b-v1.3\n",
      "lmsys/vicuna-7b-v1.5\n",
      "lmsys/vicuna-7b-v1.5-16k\n",
      "meta-llama/Llama-2-13b-chat-hf\n",
      "meta-llama/Llama-2-13b-hf\n",
      "meta-llama/Llama-2-70b-chat-hf\n",
      "meta-llama/Llama-2-70b-hf\n",
      "meta-llama/Llama-2-7b-chat-hf\n",
      "meta-llama/Llama-2-7b-hf\n",
      "meta-llama/Llama-3.2-1B\n",
      "meta-llama/Llama-3.2-1B-Instruct\n",
      "meta-llama/Llama-3.2-3B\n",
      "meta-llama/Llama-3.2-3B-Instruct\n",
      "meta-llama/Meta-Llama-3-70B\n",
      "meta-llama/Meta-Llama-3-70B-Instruct\n",
      "meta-llama/Meta-Llama-3-8B\n",
      "meta-llama/Meta-Llama-3-8B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-405B\n",
      "meta-llama/Meta-Llama-3.1-405B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-70B\n",
      "meta-llama/Meta-Llama-3.1-70B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-8B\n",
      "meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "microsoft/phi-1_5\n",
      "microsoft/phi-2\n",
      "microsoft/Phi-3-mini-128k-instruct\n",
      "microsoft/Phi-3-mini-4k-instruct\n",
      "microsoft/Phi-3.5-mini-instruct\n",
      "mistralai/mathstral-7B-v0.1\n",
      "mistralai/Mistral-7B-Instruct-v0.1\n",
      "mistralai/Mistral-7B-Instruct-v0.2\n",
      "mistralai/Mistral-7B-Instruct-v0.3\n",
      "mistralai/Mistral-7B-v0.1\n",
      "mistralai/Mistral-7B-v0.3\n",
      "mistralai/Mistral-Large-Instruct-2407\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "mistralai/Mixtral-8x7B-v0.1\n",
      "NousResearch/Nous-Hermes-13b\n",
      "NousResearch/Nous-Hermes-llama-2-7b\n",
      "NousResearch/Nous-Hermes-Llama2-13b\n",
      "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF\n",
      "openlm-research/open_llama_13b\n",
      "openlm-research/open_llama_3b\n",
      "openlm-research/open_llama_7b\n",
      "stabilityai/FreeWilly2\n",
      "stabilityai/stable-code-3b\n",
      "stabilityai/stablecode-completion-alpha-3b\n",
      "stabilityai/stablecode-completion-alpha-3b-4k\n",
      "stabilityai/stablecode-instruct-alpha-3b\n",
      "stabilityai/stablelm-3b-4e1t\n",
      "stabilityai/stablelm-base-alpha-3b\n",
      "stabilityai/stablelm-base-alpha-7b\n",
      "stabilityai/stablelm-tuned-alpha-3b\n",
      "stabilityai/stablelm-tuned-alpha-7b\n",
      "stabilityai/stablelm-zephyr-3b\n",
      "tiiuae/falcon-180B\n",
      "tiiuae/falcon-180B-chat\n",
      "tiiuae/falcon-40b\n",
      "tiiuae/falcon-40b-instruct\n",
      "tiiuae/falcon-7b\n",
      "tiiuae/falcon-7b-instruct\n",
      "TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\n",
      "togethercomputer/LLaMA-2-7B-32K\n",
      "togethercomputer/RedPajama-INCITE-7B-Base\n",
      "togethercomputer/RedPajama-INCITE-7B-Chat\n",
      "togethercomputer/RedPajama-INCITE-7B-Instruct\n",
      "togethercomputer/RedPajama-INCITE-Base-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Base-7B-v0.1\n",
      "togethercomputer/RedPajama-INCITE-Chat-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Chat-7B-v0.1\n",
      "togethercomputer/RedPajama-INCITE-Instruct-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Instruct-7B-v0.1\n",
      "Trelis/Llama-2-7b-chat-hf-function-calling-v2\n",
      "unsloth/Mistral-7B-v0.2\n"
     ]
    }
   ],
   "source": [
    "# List all the downloadable models\n",
    "# Don't use ! if you are in the terminal (not in Jupyter Notebook)\n",
    "!litgpt download list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download LLama-3.2-1B which is relatively lightweight but still pretty performant. \n",
    "\n",
    "<Note type=\"important\">\n",
    "\n",
    "If you are following along and want to try other models, keep in mind that the more parameters you select the heavier the model will be not just in terms of storage but also in terms of memory (RAM). Don't use too large models for your local computer otherwise it will freeze. \n",
    "\n",
    "</Note>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting HF_HUB_ENABLE_HF_TRANSFER=1\n",
      "config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 843/843 [00:00<00:00, 8.42MB/s]\n",
      "generation_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 185/185 [00:00<00:00, 2.35MB/s]\n",
      "model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 2.47G/2.47G [00:04<00:00, 527MB/s]\n",
      "tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.09M/9.09M [00:00<00:00, 13.5MB/s]\n",
      "tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50.5k/50.5k [00:00<00:00, 90.4MB/s]\n",
      "Converting checkpoint files to LitGPT format.\n",
      "{'checkpoint_dir': PosixPath('checkpoints/meta-llama/Llama-3.2-1B'),\n",
      " 'debug_mode': False,\n",
      " 'dtype': None,\n",
      " 'model_name': None}\n",
      "Loading weights: model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 00:39<00:00,  2.54it/s\n",
      "Saving converted checkpoint to checkpoints/meta-llama/Llama-3.2-1B\n"
     ]
    }
   ],
   "source": [
    "# Download TinyLLama (a small copy of LLama)\n",
    "# Don't use ! if you are in the terminal (not in Jupyter Notebook)\n",
    "!litgpt download meta-llama/Llama-3.2-1B --access_token <read_token>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Note type=\"important\" title=\"If you want to use LLama official models\">\n",
    "\n",
    "If you want to use real LLama model from Meta, you will need to:\n",
    "\n",
    "1. Go to HuggingFace and [accept Meta's term of use](https://huggingface.co/meta-llama/Llama-3.2-1B)\n",
    "2. Wait for the approval from Meta \n",
    "3. Then provide your HuggingFace token as parameter of your bash command like this:\n",
    "\n",
    "```bash\n",
    "!litgpt download --repo_id meta-llama/Llama-3.2-1B --access_token YOUR_HF_TOKEN\n",
    "```\n",
    "\n",
    "You can get your access token by clicking on your profile > ACCESS TOKENS\n",
    "\n",
    "It should take about 30 minutes for you to get approved. \n",
    "\n",
    "</Note>\n",
    "\n",
    "\n",
    "Once your command has been successfully executed, you should see a `checkpoint/NAME_OF_YOUR_MODEL` directory that should be at the same place as where you executed the command. \n",
    "\n",
    "Now you should be able to interact with your model the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'access_token': None,\n",
      " 'checkpoint_dir': PosixPath('checkpoints/meta-llama/Llama-3.2-1B'),\n",
      " 'compile': False,\n",
      " 'max_new_tokens': 50,\n",
      " 'multiline': False,\n",
      " 'precision': None,\n",
      " 'quantize': None,\n",
      " 'temperature': 0.8,\n",
      " 'top_k': 50,\n",
      " 'top_p': 1.0}\n",
      "Now chatting with Llama-3.2-1B.\n",
      "To exit, press 'Enter' on an empty prompt.\n",
      "\n",
      "Seed set to 1234\n",
      ">> Prompt: ^C\n",
      "\n",
      "Memory used: 3.10 GB\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS COMMAND IN A STANDALONE TERMINAL NOT IN JUPYTER \n",
    "# otherwise you won't be able to interact üòâ\n",
    "# Don't use ! if you are in the terminal (not in Jupyter Notebook)\n",
    "!litgpt chat checkpoints/meta-llama/Llama-3.2-1B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning \n",
    "\n",
    "Now if you want to fine-tune a model, you will need first to build a dataset that follows this structure:\n",
    "\n",
    "```json \n",
    "[\n",
    "    {\n",
    "        \"instruction\": \"THIS IS THE USER PROMPT\",\n",
    "        \"input\": \"THIS IS ADDITIONAL CONTEXT FROM THE INSTRUCTION\",\n",
    "        \"output\": \"THIS IS THE EXPECTED ANSWER\"\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "You can download the template files here:\n",
    "\n",
    "* [train.json](https://full-stack-assets.s3.eu-west-3.amazonaws.com/train.json)\n",
    "* [val.json](https://full-stack-assets.s3.eu-west-3.amazonaws.com/val.json)\n",
    "\n",
    "Then you can simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'access_token': None,\n",
      " 'checkpoint_dir': PosixPath('checkpoints/meta-llama/Llama-3.2-1B'),\n",
      " 'data': JSON(json_path=PosixPath('/teamspace/studios/this_studio/data'),\n",
      "              mask_prompt=False,\n",
      "              val_split_fraction=None,\n",
      "              prompt_style=<litgpt.prompts.Alpaca object at 0x7ff54c929270>,\n",
      "              ignore_index=-100,\n",
      "              seed=42,\n",
      "              num_workers=4),\n",
      " 'devices': 1,\n",
      " 'eval': EvalArgs(interval=100,\n",
      "                  max_new_tokens=100,\n",
      "                  max_iters=100,\n",
      "                  initial_validation=False,\n",
      "                  final_validation=True,\n",
      "                  evaluate_example='first'),\n",
      " 'logger_name': 'csv',\n",
      " 'lora_alpha': 16,\n",
      " 'lora_dropout': 0.05,\n",
      " 'lora_head': False,\n",
      " 'lora_key': False,\n",
      " 'lora_mlp': False,\n",
      " 'lora_projection': False,\n",
      " 'lora_query': True,\n",
      " 'lora_r': 8,\n",
      " 'lora_value': True,\n",
      " 'num_nodes': 1,\n",
      " 'optimizer': 'AdamW',\n",
      " 'out_dir': PosixPath('results/fine-tuned-llama-3.2-1B'),\n",
      " 'precision': None,\n",
      " 'quantize': None,\n",
      " 'seed': 1337,\n",
      " 'train': TrainArgs(save_interval=1000,\n",
      "                    log_interval=1,\n",
      "                    global_batch_size=32,\n",
      "                    micro_batch_size=1,\n",
      "                    lr_warmup_steps=100,\n",
      "                    lr_warmup_fraction=None,\n",
      "                    epochs=5,\n",
      "                    max_tokens=None,\n",
      "                    max_steps=None,\n",
      "                    max_seq_length=None,\n",
      "                    tie_embeddings=None,\n",
      "                    max_norm=None,\n",
      "                    min_lr=6e-05)}\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "Seed set to 1337\n",
      "Number of trainable parameters: 851,968\n",
      "Number of non-trainable parameters: 1,498,482,688\n",
      "The longest sequence length in the train data is 78, the model's maximum sequence length is 78 and context length is 131072\n",
      "Verifying settings ...\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "Epoch 1 | iter 1 step 0 | loss train: 3.364, val: n/a | iter time: 435.68 ms\n",
      "Epoch 1 | iter 2 step 0 | loss train: 3.262, val: n/a | iter time: 83.80 ms\n",
      "Epoch 1 | iter 3 step 0 | loss train: 3.286, val: n/a | iter time: 71.39 ms\n",
      "Epoch 1 | iter 4 step 0 | loss train: 3.231, val: n/a | iter time: 81.10 ms\n",
      "Epoch 1 | iter 5 step 0 | loss train: 3.231, val: n/a | iter time: 73.58 ms\n",
      "Epoch 1 | iter 6 step 0 | loss train: 3.242, val: n/a | iter time: 73.24 ms\n",
      "Epoch 1 | iter 7 step 0 | loss train: 3.246, val: n/a | iter time: 66.01 ms\n",
      "Epoch 1 | iter 8 step 0 | loss train: 3.204, val: n/a | iter time: 77.80 ms\n",
      "Epoch 1 | iter 9 step 0 | loss train: 3.191, val: n/a | iter time: 64.92 ms\n",
      "Epoch 1 | iter 10 step 0 | loss train: 3.222, val: n/a | iter time: 67.83 ms\n",
      "Epoch 1 | iter 11 step 0 | loss train: 3.222, val: n/a | iter time: 66.39 ms\n",
      "Epoch 1 | iter 12 step 0 | loss train: 3.225, val: n/a | iter time: 74.73 ms\n",
      "Epoch 1 | iter 13 step 0 | loss train: 3.217, val: n/a | iter time: 72.01 ms\n",
      "Epoch 1 | iter 14 step 0 | loss train: 3.239, val: n/a | iter time: 87.86 ms\n",
      "Epoch 1 | iter 15 step 0 | loss train: 3.223, val: n/a | iter time: 75.38 ms\n",
      "Epoch 1 | iter 16 step 0 | loss train: 3.216, val: n/a | iter time: 67.88 ms\n",
      "Epoch 1 | iter 17 step 0 | loss train: 3.217, val: n/a | iter time: 67.34 ms\n",
      "Epoch 1 | iter 18 step 0 | loss train: 3.226, val: n/a | iter time: 67.10 ms\n",
      "Epoch 1 | iter 19 step 0 | loss train: 3.234, val: n/a | iter time: 67.05 ms\n",
      "Epoch 1 | iter 20 step 0 | loss train: 3.238, val: n/a | iter time: 66.46 ms\n",
      "Epoch 1 | iter 21 step 0 | loss train: 3.239, val: n/a | iter time: 65.88 ms\n",
      "Epoch 1 | iter 22 step 0 | loss train: 3.237, val: n/a | iter time: 65.94 ms\n",
      "Epoch 1 | iter 23 step 0 | loss train: 3.236, val: n/a | iter time: 65.16 ms\n",
      "Epoch 1 | iter 24 step 0 | loss train: 3.241, val: n/a | iter time: 73.84 ms\n",
      "Epoch 1 | iter 25 step 0 | loss train: 3.235, val: n/a | iter time: 66.85 ms\n",
      "Epoch 1 | iter 26 step 0 | loss train: 3.244, val: n/a | iter time: 66.77 ms\n",
      "Epoch 1 | iter 27 step 0 | loss train: 3.244, val: n/a | iter time: 74.40 ms\n",
      "Epoch 1 | iter 28 step 0 | loss train: 3.232, val: n/a | iter time: 67.83 ms\n",
      "Epoch 1 | iter 29 step 0 | loss train: 3.230, val: n/a | iter time: 66.36 ms\n",
      "Epoch 1 | iter 30 step 0 | loss train: 3.227, val: n/a | iter time: 66.88 ms\n",
      "Epoch 1 | iter 31 step 0 | loss train: 3.222, val: n/a | iter time: 75.08 ms\n",
      "Epoch 1 | iter 32 step 1 | loss train: 3.223, val: n/a | iter time: 153.95 ms (step)\n",
      "Epoch 1 | iter 33 step 1 | loss train: 3.217, val: n/a | iter time: 67.80 ms\n",
      "Epoch 1 | iter 34 step 1 | loss train: 3.230, val: n/a | iter time: 76.83 ms\n",
      "Epoch 1 | iter 35 step 1 | loss train: 3.228, val: n/a | iter time: 68.46 ms\n",
      "Epoch 1 | iter 36 step 1 | loss train: 3.233, val: n/a | iter time: 67.64 ms\n",
      "Epoch 1 | iter 37 step 1 | loss train: 3.227, val: n/a | iter time: 68.25 ms\n",
      "Epoch 1 | iter 38 step 1 | loss train: 3.229, val: n/a | iter time: 66.79 ms\n",
      "Epoch 1 | iter 39 step 1 | loss train: 3.231, val: n/a | iter time: 67.54 ms\n",
      "Epoch 1 | iter 40 step 1 | loss train: 3.248, val: n/a | iter time: 67.34 ms\n",
      "Epoch 1 | iter 41 step 1 | loss train: 3.249, val: n/a | iter time: 67.39 ms\n",
      "Epoch 1 | iter 42 step 1 | loss train: 3.244, val: n/a | iter time: 69.56 ms\n",
      "Epoch 1 | iter 43 step 1 | loss train: 3.253, val: n/a | iter time: 75.15 ms\n",
      "Epoch 1 | iter 44 step 1 | loss train: 3.250, val: n/a | iter time: 68.53 ms\n",
      "Epoch 1 | iter 45 step 1 | loss train: 3.259, val: n/a | iter time: 69.09 ms\n",
      "Epoch 1 | iter 46 step 1 | loss train: 3.247, val: n/a | iter time: 68.00 ms\n",
      "Epoch 1 | iter 47 step 1 | loss train: 3.261, val: n/a | iter time: 67.41 ms\n",
      "Epoch 1 | iter 48 step 1 | loss train: 3.268, val: n/a | iter time: 68.44 ms\n",
      "Epoch 1 | iter 49 step 1 | loss train: 3.270, val: n/a | iter time: 66.32 ms\n",
      "Epoch 1 | iter 50 step 1 | loss train: 3.273, val: n/a | iter time: 67.34 ms\n",
      "Epoch 1 | iter 51 step 1 | loss train: 3.272, val: n/a | iter time: 66.98 ms\n",
      "Epoch 1 | iter 52 step 1 | loss train: 3.272, val: n/a | iter time: 67.04 ms\n",
      "Epoch 1 | iter 53 step 1 | loss train: 3.276, val: n/a | iter time: 67.09 ms\n",
      "Epoch 1 | iter 54 step 1 | loss train: 3.265, val: n/a | iter time: 75.29 ms\n",
      "Epoch 1 | iter 55 step 1 | loss train: 3.272, val: n/a | iter time: 66.69 ms\n",
      "Epoch 1 | iter 56 step 1 | loss train: 3.274, val: n/a | iter time: 66.64 ms\n",
      "Epoch 1 | iter 57 step 1 | loss train: 3.279, val: n/a | iter time: 66.45 ms\n",
      "Epoch 2 | iter 58 step 1 | loss train: 3.266, val: n/a | iter time: 253.20 ms\n",
      "Epoch 2 | iter 59 step 1 | loss train: 3.277, val: n/a | iter time: 63.50 ms\n",
      "Epoch 2 | iter 60 step 1 | loss train: 3.283, val: n/a | iter time: 66.06 ms\n",
      "Epoch 2 | iter 61 step 1 | loss train: 3.287, val: n/a | iter time: 70.50 ms\n",
      "Epoch 2 | iter 62 step 1 | loss train: 3.284, val: n/a | iter time: 64.13 ms\n",
      "Epoch 2 | iter 63 step 1 | loss train: 3.293, val: n/a | iter time: 64.62 ms\n",
      "Epoch 2 | iter 64 step 2 | loss train: 3.298, val: n/a | iter time: 64.46 ms (step)\n",
      "Epoch 2 | iter 65 step 2 | loss train: 3.298, val: n/a | iter time: 66.64 ms\n",
      "Epoch 2 | iter 66 step 2 | loss train: 3.287, val: n/a | iter time: 63.68 ms\n",
      "Epoch 2 | iter 67 step 2 | loss train: 3.290, val: n/a | iter time: 65.95 ms\n",
      "Epoch 2 | iter 68 step 2 | loss train: 3.293, val: n/a | iter time: 65.79 ms\n",
      "Epoch 2 | iter 69 step 2 | loss train: 3.308, val: n/a | iter time: 65.61 ms\n",
      "Epoch 2 | iter 70 step 2 | loss train: 3.307, val: n/a | iter time: 65.15 ms\n",
      "Epoch 2 | iter 71 step 2 | loss train: 3.301, val: n/a | iter time: 65.12 ms\n",
      "Epoch 2 | iter 72 step 2 | loss train: 3.296, val: n/a | iter time: 65.43 ms\n",
      "Epoch 2 | iter 73 step 2 | loss train: 3.294, val: n/a | iter time: 65.72 ms\n",
      "Epoch 2 | iter 74 step 2 | loss train: 3.292, val: n/a | iter time: 64.75 ms\n",
      "Epoch 2 | iter 75 step 2 | loss train: 3.279, val: n/a | iter time: 65.42 ms\n",
      "Epoch 2 | iter 76 step 2 | loss train: 3.282, val: n/a | iter time: 65.39 ms\n",
      "Epoch 2 | iter 77 step 2 | loss train: 3.277, val: n/a | iter time: 65.89 ms\n",
      "Epoch 2 | iter 78 step 2 | loss train: 3.287, val: n/a | iter time: 65.83 ms\n",
      "Epoch 2 | iter 79 step 2 | loss train: 3.268, val: n/a | iter time: 65.48 ms\n",
      "Epoch 2 | iter 80 step 2 | loss train: 3.268, val: n/a | iter time: 65.90 ms\n",
      "Epoch 2 | iter 81 step 2 | loss train: 3.273, val: n/a | iter time: 65.67 ms\n",
      "Epoch 2 | iter 82 step 2 | loss train: 3.265, val: n/a | iter time: 64.56 ms\n",
      "Epoch 2 | iter 83 step 2 | loss train: 3.260, val: n/a | iter time: 65.71 ms\n",
      "Epoch 2 | iter 84 step 2 | loss train: 3.256, val: n/a | iter time: 66.31 ms\n",
      "Epoch 2 | iter 85 step 2 | loss train: 3.251, val: n/a | iter time: 64.68 ms\n",
      "Epoch 2 | iter 86 step 2 | loss train: 3.270, val: n/a | iter time: 64.26 ms\n",
      "Epoch 2 | iter 87 step 2 | loss train: 3.261, val: n/a | iter time: 65.70 ms\n",
      "Epoch 2 | iter 88 step 2 | loss train: 3.263, val: n/a | iter time: 66.67 ms\n",
      "Epoch 2 | iter 89 step 2 | loss train: 3.263, val: n/a | iter time: 64.90 ms\n",
      "Epoch 2 | iter 90 step 2 | loss train: 3.271, val: n/a | iter time: 65.86 ms\n",
      "Epoch 2 | iter 91 step 2 | loss train: 3.261, val: n/a | iter time: 65.72 ms\n",
      "Epoch 2 | iter 92 step 2 | loss train: 3.255, val: n/a | iter time: 65.90 ms\n",
      "Epoch 2 | iter 93 step 2 | loss train: 3.258, val: n/a | iter time: 64.99 ms\n",
      "Epoch 2 | iter 94 step 2 | loss train: 3.265, val: n/a | iter time: 65.35 ms\n",
      "Epoch 2 | iter 95 step 2 | loss train: 3.261, val: n/a | iter time: 65.81 ms\n",
      "Epoch 2 | iter 96 step 3 | loss train: 3.260, val: n/a | iter time: 66.30 ms (step)\n",
      "Epoch 2 | iter 97 step 3 | loss train: 3.258, val: n/a | iter time: 65.49 ms\n",
      "Epoch 2 | iter 98 step 3 | loss train: 3.264, val: n/a | iter time: 65.47 ms\n",
      "Epoch 2 | iter 99 step 3 | loss train: 3.257, val: n/a | iter time: 65.57 ms\n",
      "Epoch 2 | iter 100 step 3 | loss train: 3.251, val: n/a | iter time: 66.09 ms\n",
      "Epoch 2 | iter 101 step 3 | loss train: 3.243, val: n/a | iter time: 67.64 ms\n",
      "Epoch 2 | iter 102 step 3 | loss train: 3.236, val: n/a | iter time: 65.82 ms\n",
      "Epoch 2 | iter 103 step 3 | loss train: 3.245, val: n/a | iter time: 65.84 ms\n",
      "Epoch 2 | iter 104 step 3 | loss train: 3.248, val: n/a | iter time: 64.60 ms\n",
      "Epoch 2 | iter 105 step 3 | loss train: 3.262, val: n/a | iter time: 65.52 ms\n",
      "Epoch 2 | iter 106 step 3 | loss train: 3.264, val: n/a | iter time: 65.72 ms\n",
      "Epoch 2 | iter 107 step 3 | loss train: 3.269, val: n/a | iter time: 64.51 ms\n",
      "Epoch 2 | iter 108 step 3 | loss train: 3.274, val: n/a | iter time: 65.17 ms\n",
      "Epoch 2 | iter 109 step 3 | loss train: 3.277, val: n/a | iter time: 65.50 ms\n",
      "Epoch 2 | iter 110 step 3 | loss train: 3.264, val: n/a | iter time: 65.29 ms\n",
      "Epoch 2 | iter 111 step 3 | loss train: 3.284, val: n/a | iter time: 65.65 ms\n",
      "Epoch 2 | iter 112 step 3 | loss train: 3.279, val: n/a | iter time: 65.20 ms\n",
      "Epoch 2 | iter 113 step 3 | loss train: 3.265, val: n/a | iter time: 65.36 ms\n",
      "Epoch 2 | iter 114 step 3 | loss train: 3.255, val: n/a | iter time: 65.35 ms\n",
      "Epoch 3 | iter 115 step 3 | loss train: 3.255, val: n/a | iter time: 254.61 ms\n",
      "Epoch 3 | iter 116 step 3 | loss train: 3.253, val: n/a | iter time: 65.34 ms\n",
      "Epoch 3 | iter 117 step 3 | loss train: 3.253, val: n/a | iter time: 67.20 ms\n",
      "Epoch 3 | iter 118 step 3 | loss train: 3.255, val: n/a | iter time: 65.81 ms\n",
      "Epoch 3 | iter 119 step 3 | loss train: 3.257, val: n/a | iter time: 62.96 ms\n",
      "Epoch 3 | iter 120 step 3 | loss train: 3.251, val: n/a | iter time: 62.67 ms\n",
      "Epoch 3 | iter 121 step 3 | loss train: 3.252, val: n/a | iter time: 62.92 ms\n",
      "Epoch 3 | iter 122 step 3 | loss train: 3.248, val: n/a | iter time: 66.57 ms\n",
      "Epoch 3 | iter 123 step 3 | loss train: 3.234, val: n/a | iter time: 63.44 ms\n",
      "Epoch 3 | iter 124 step 3 | loss train: 3.248, val: n/a | iter time: 64.23 ms\n",
      "Epoch 3 | iter 125 step 3 | loss train: 3.247, val: n/a | iter time: 64.44 ms\n",
      "Epoch 3 | iter 126 step 3 | loss train: 3.247, val: n/a | iter time: 63.90 ms\n",
      "Epoch 3 | iter 127 step 3 | loss train: 3.257, val: n/a | iter time: 64.61 ms\n",
      "Epoch 3 | iter 128 step 4 | loss train: 3.259, val: n/a | iter time: 65.82 ms (step)\n",
      "Epoch 3 | iter 129 step 4 | loss train: 3.259, val: n/a | iter time: 65.19 ms\n",
      "Epoch 3 | iter 130 step 4 | loss train: 3.254, val: n/a | iter time: 64.05 ms\n",
      "Epoch 3 | iter 131 step 4 | loss train: 3.247, val: n/a | iter time: 64.34 ms\n",
      "Epoch 3 | iter 132 step 4 | loss train: 3.246, val: n/a | iter time: 64.36 ms\n",
      "Epoch 3 | iter 133 step 4 | loss train: 3.241, val: n/a | iter time: 64.32 ms\n",
      "Epoch 3 | iter 134 step 4 | loss train: 3.242, val: n/a | iter time: 64.62 ms\n",
      "Epoch 3 | iter 135 step 4 | loss train: 3.237, val: n/a | iter time: 65.29 ms\n",
      "Epoch 3 | iter 136 step 4 | loss train: 3.233, val: n/a | iter time: 65.07 ms\n",
      "Epoch 3 | iter 137 step 4 | loss train: 3.217, val: n/a | iter time: 64.79 ms\n",
      "Epoch 3 | iter 138 step 4 | loss train: 3.209, val: n/a | iter time: 64.81 ms\n",
      "Epoch 3 | iter 139 step 4 | loss train: 3.210, val: n/a | iter time: 65.03 ms\n",
      "Epoch 3 | iter 140 step 4 | loss train: 3.206, val: n/a | iter time: 65.46 ms\n",
      "Epoch 3 | iter 141 step 4 | loss train: 3.212, val: n/a | iter time: 65.17 ms\n",
      "Epoch 3 | iter 142 step 4 | loss train: 3.218, val: n/a | iter time: 65.25 ms\n",
      "Epoch 3 | iter 143 step 4 | loss train: 3.212, val: n/a | iter time: 66.52 ms\n",
      "Epoch 3 | iter 144 step 4 | loss train: 3.214, val: n/a | iter time: 64.93 ms\n",
      "Epoch 3 | iter 145 step 4 | loss train: 3.227, val: n/a | iter time: 65.24 ms\n",
      "Epoch 3 | iter 146 step 4 | loss train: 3.234, val: n/a | iter time: 64.99 ms\n",
      "Epoch 3 | iter 147 step 4 | loss train: 3.236, val: n/a | iter time: 65.01 ms\n",
      "Epoch 3 | iter 148 step 4 | loss train: 3.243, val: n/a | iter time: 65.34 ms\n",
      "Epoch 3 | iter 149 step 4 | loss train: 3.248, val: n/a | iter time: 65.38 ms\n",
      "Epoch 3 | iter 150 step 4 | loss train: 3.242, val: n/a | iter time: 65.43 ms\n",
      "Epoch 3 | iter 151 step 4 | loss train: 3.239, val: n/a | iter time: 65.19 ms\n",
      "Epoch 3 | iter 152 step 4 | loss train: 3.236, val: n/a | iter time: 66.32 ms\n",
      "Epoch 3 | iter 153 step 4 | loss train: 3.242, val: n/a | iter time: 66.13 ms\n",
      "Epoch 3 | iter 154 step 4 | loss train: 3.235, val: n/a | iter time: 66.28 ms\n",
      "Epoch 3 | iter 155 step 4 | loss train: 3.242, val: n/a | iter time: 66.17 ms\n",
      "Epoch 3 | iter 156 step 4 | loss train: 3.241, val: n/a | iter time: 68.08 ms\n",
      "Epoch 3 | iter 157 step 4 | loss train: 3.238, val: n/a | iter time: 66.89 ms\n",
      "Epoch 3 | iter 158 step 4 | loss train: 3.243, val: n/a | iter time: 66.28 ms\n",
      "Epoch 3 | iter 159 step 4 | loss train: 3.237, val: n/a | iter time: 67.57 ms\n",
      "Epoch 3 | iter 160 step 5 | loss train: 3.238, val: n/a | iter time: 69.04 ms (step)\n",
      "Epoch 3 | iter 161 step 5 | loss train: 3.230, val: n/a | iter time: 65.39 ms\n",
      "Epoch 3 | iter 162 step 5 | loss train: 3.232, val: n/a | iter time: 66.15 ms\n",
      "Epoch 3 | iter 163 step 5 | loss train: 3.240, val: n/a | iter time: 65.75 ms\n",
      "Epoch 3 | iter 164 step 5 | loss train: 3.250, val: n/a | iter time: 65.65 ms\n",
      "Epoch 3 | iter 165 step 5 | loss train: 3.254, val: n/a | iter time: 65.48 ms\n",
      "Epoch 3 | iter 166 step 5 | loss train: 3.264, val: n/a | iter time: 65.44 ms\n",
      "Epoch 3 | iter 167 step 5 | loss train: 3.259, val: n/a | iter time: 65.49 ms\n",
      "Epoch 3 | iter 168 step 5 | loss train: 3.262, val: n/a | iter time: 65.95 ms\n",
      "Epoch 3 | iter 169 step 5 | loss train: 3.275, val: n/a | iter time: 66.14 ms\n",
      "Epoch 3 | iter 170 step 5 | loss train: 3.283, val: n/a | iter time: 65.89 ms\n",
      "Epoch 3 | iter 171 step 5 | loss train: 3.279, val: n/a | iter time: 65.48 ms\n",
      "Epoch 4 | iter 172 step 5 | loss train: 3.272, val: n/a | iter time: 261.94 ms\n",
      "Epoch 4 | iter 173 step 5 | loss train: 3.259, val: n/a | iter time: 68.08 ms\n",
      "Epoch 4 | iter 174 step 5 | loss train: 3.267, val: n/a | iter time: 71.52 ms\n",
      "Epoch 4 | iter 175 step 5 | loss train: 3.254, val: n/a | iter time: 69.31 ms\n",
      "Epoch 4 | iter 176 step 5 | loss train: 3.259, val: n/a | iter time: 66.04 ms\n",
      "Epoch 4 | iter 177 step 5 | loss train: 3.257, val: n/a | iter time: 64.88 ms\n",
      "Epoch 4 | iter 178 step 5 | loss train: 3.255, val: n/a | iter time: 66.36 ms\n",
      "Epoch 4 | iter 179 step 5 | loss train: 3.253, val: n/a | iter time: 69.28 ms\n",
      "Epoch 4 | iter 180 step 5 | loss train: 3.259, val: n/a | iter time: 67.07 ms\n",
      "Epoch 4 | iter 181 step 5 | loss train: 3.257, val: n/a | iter time: 67.47 ms\n",
      "Epoch 4 | iter 182 step 5 | loss train: 3.255, val: n/a | iter time: 68.38 ms\n",
      "Epoch 4 | iter 183 step 5 | loss train: 3.250, val: n/a | iter time: 68.08 ms\n",
      "Epoch 4 | iter 184 step 5 | loss train: 3.249, val: n/a | iter time: 67.16 ms\n",
      "Epoch 4 | iter 185 step 5 | loss train: 3.246, val: n/a | iter time: 69.00 ms\n",
      "Epoch 4 | iter 186 step 5 | loss train: 3.255, val: n/a | iter time: 66.91 ms\n",
      "Epoch 4 | iter 187 step 5 | loss train: 3.262, val: n/a | iter time: 67.36 ms\n",
      "Epoch 4 | iter 188 step 5 | loss train: 3.258, val: n/a | iter time: 64.84 ms\n",
      "Epoch 4 | iter 189 step 5 | loss train: 3.257, val: n/a | iter time: 65.98 ms\n",
      "Epoch 4 | iter 190 step 5 | loss train: 3.256, val: n/a | iter time: 65.66 ms\n",
      "Epoch 4 | iter 191 step 5 | loss train: 3.255, val: n/a | iter time: 65.79 ms\n",
      "Epoch 4 | iter 192 step 6 | loss train: 3.254, val: n/a | iter time: 67.91 ms (step)\n",
      "Epoch 4 | iter 193 step 6 | loss train: 3.266, val: n/a | iter time: 66.18 ms\n",
      "Epoch 4 | iter 194 step 6 | loss train: 3.267, val: n/a | iter time: 65.64 ms\n",
      "Epoch 4 | iter 195 step 6 | loss train: 3.268, val: n/a | iter time: 66.88 ms\n",
      "Epoch 4 | iter 196 step 6 | loss train: 3.264, val: n/a | iter time: 66.74 ms\n",
      "Epoch 4 | iter 197 step 6 | loss train: 3.272, val: n/a | iter time: 65.89 ms\n",
      "Epoch 4 | iter 198 step 6 | loss train: 3.261, val: n/a | iter time: 66.07 ms\n",
      "Epoch 4 | iter 199 step 6 | loss train: 3.265, val: n/a | iter time: 66.15 ms\n",
      "Epoch 4 | iter 200 step 6 | loss train: 3.263, val: n/a | iter time: 66.99 ms\n",
      "Epoch 4 | iter 201 step 6 | loss train: 3.251, val: n/a | iter time: 66.46 ms\n",
      "Epoch 4 | iter 202 step 6 | loss train: 3.240, val: n/a | iter time: 66.63 ms\n",
      "Epoch 4 | iter 203 step 6 | loss train: 3.244, val: n/a | iter time: 66.25 ms\n",
      "Epoch 4 | iter 204 step 6 | loss train: 3.245, val: n/a | iter time: 66.29 ms\n",
      "Epoch 4 | iter 205 step 6 | loss train: 3.246, val: n/a | iter time: 67.01 ms\n",
      "Epoch 4 | iter 206 step 6 | loss train: 3.243, val: n/a | iter time: 66.16 ms\n",
      "Epoch 4 | iter 207 step 6 | loss train: 3.256, val: n/a | iter time: 66.79 ms\n",
      "Epoch 4 | iter 208 step 6 | loss train: 3.249, val: n/a | iter time: 66.71 ms\n",
      "Epoch 4 | iter 209 step 6 | loss train: 3.241, val: n/a | iter time: 65.76 ms\n",
      "Epoch 4 | iter 210 step 6 | loss train: 3.242, val: n/a | iter time: 66.76 ms\n",
      "Epoch 4 | iter 211 step 6 | loss train: 3.250, val: n/a | iter time: 65.73 ms\n",
      "Epoch 4 | iter 212 step 6 | loss train: 3.238, val: n/a | iter time: 66.66 ms\n",
      "Epoch 4 | iter 213 step 6 | loss train: 3.243, val: n/a | iter time: 66.84 ms\n",
      "Epoch 4 | iter 214 step 6 | loss train: 3.249, val: n/a | iter time: 66.92 ms\n",
      "Epoch 4 | iter 215 step 6 | loss train: 3.243, val: n/a | iter time: 68.17 ms\n",
      "Epoch 4 | iter 216 step 6 | loss train: 3.248, val: n/a | iter time: 67.53 ms\n",
      "Epoch 4 | iter 217 step 6 | loss train: 3.249, val: n/a | iter time: 69.21 ms\n",
      "Epoch 4 | iter 218 step 6 | loss train: 3.236, val: n/a | iter time: 69.14 ms\n",
      "Epoch 4 | iter 219 step 6 | loss train: 3.229, val: n/a | iter time: 65.16 ms\n",
      "Epoch 4 | iter 220 step 6 | loss train: 3.229, val: n/a | iter time: 65.34 ms\n",
      "Epoch 4 | iter 221 step 6 | loss train: 3.230, val: n/a | iter time: 65.22 ms\n",
      "Epoch 4 | iter 222 step 6 | loss train: 3.223, val: n/a | iter time: 64.66 ms\n",
      "Epoch 4 | iter 223 step 6 | loss train: 3.219, val: n/a | iter time: 64.67 ms\n",
      "Epoch 4 | iter 224 step 7 | loss train: 3.212, val: n/a | iter time: 66.30 ms (step)\n",
      "Epoch 4 | iter 225 step 7 | loss train: 3.218, val: n/a | iter time: 63.44 ms\n",
      "Epoch 4 | iter 226 step 7 | loss train: 3.216, val: n/a | iter time: 65.03 ms\n",
      "Epoch 4 | iter 227 step 7 | loss train: 3.220, val: n/a | iter time: 64.69 ms\n",
      "Epoch 4 | iter 228 step 7 | loss train: 3.212, val: n/a | iter time: 66.00 ms\n",
      "Epoch 5 | iter 229 step 7 | loss train: 3.210, val: n/a | iter time: 267.16 ms\n",
      "Epoch 5 | iter 230 step 7 | loss train: 3.216, val: n/a | iter time: 66.79 ms\n",
      "Epoch 5 | iter 231 step 7 | loss train: 3.220, val: n/a | iter time: 68.58 ms\n",
      "Epoch 5 | iter 232 step 7 | loss train: 3.220, val: n/a | iter time: 70.96 ms\n",
      "Epoch 5 | iter 233 step 7 | loss train: 3.225, val: n/a | iter time: 66.27 ms\n",
      "Epoch 5 | iter 234 step 7 | loss train: 3.226, val: n/a | iter time: 65.40 ms\n",
      "Epoch 5 | iter 235 step 7 | loss train: 3.220, val: n/a | iter time: 66.48 ms\n",
      "Epoch 5 | iter 236 step 7 | loss train: 3.221, val: n/a | iter time: 68.91 ms\n",
      "Epoch 5 | iter 237 step 7 | loss train: 3.227, val: n/a | iter time: 65.61 ms\n",
      "Epoch 5 | iter 238 step 7 | loss train: 3.220, val: n/a | iter time: 67.07 ms\n",
      "Epoch 5 | iter 239 step 7 | loss train: 3.208, val: n/a | iter time: 66.88 ms\n",
      "Epoch 5 | iter 240 step 7 | loss train: 3.206, val: n/a | iter time: 67.07 ms\n",
      "Epoch 5 | iter 241 step 7 | loss train: 3.212, val: n/a | iter time: 67.38 ms\n",
      "Epoch 5 | iter 242 step 7 | loss train: 3.204, val: n/a | iter time: 67.34 ms\n",
      "Epoch 5 | iter 243 step 7 | loss train: 3.197, val: n/a | iter time: 66.20 ms\n",
      "Epoch 5 | iter 244 step 7 | loss train: 3.199, val: n/a | iter time: 66.10 ms\n",
      "Epoch 5 | iter 245 step 7 | loss train: 3.194, val: n/a | iter time: 68.40 ms\n",
      "Epoch 5 | iter 246 step 7 | loss train: 3.190, val: n/a | iter time: 68.40 ms\n",
      "Epoch 5 | iter 247 step 7 | loss train: 3.208, val: n/a | iter time: 66.42 ms\n",
      "Epoch 5 | iter 248 step 7 | loss train: 3.197, val: n/a | iter time: 66.87 ms\n",
      "Epoch 5 | iter 249 step 7 | loss train: 3.201, val: n/a | iter time: 66.52 ms\n",
      "Epoch 5 | iter 250 step 7 | loss train: 3.206, val: n/a | iter time: 67.69 ms\n",
      "Epoch 5 | iter 251 step 7 | loss train: 3.200, val: n/a | iter time: 67.89 ms\n",
      "Epoch 5 | iter 252 step 7 | loss train: 3.197, val: n/a | iter time: 68.50 ms\n",
      "Epoch 5 | iter 253 step 7 | loss train: 3.193, val: n/a | iter time: 67.31 ms\n",
      "Epoch 5 | iter 254 step 7 | loss train: 3.190, val: n/a | iter time: 67.30 ms\n",
      "Epoch 5 | iter 255 step 7 | loss train: 3.190, val: n/a | iter time: 66.67 ms\n",
      "Epoch 5 | iter 256 step 8 | loss train: 3.194, val: n/a | iter time: 68.32 ms (step)\n",
      "Epoch 5 | iter 257 step 8 | loss train: 3.188, val: n/a | iter time: 66.35 ms\n",
      "Epoch 5 | iter 258 step 8 | loss train: 3.189, val: n/a | iter time: 67.53 ms\n",
      "Epoch 5 | iter 259 step 8 | loss train: 3.184, val: n/a | iter time: 66.50 ms\n",
      "Epoch 5 | iter 260 step 8 | loss train: 3.195, val: n/a | iter time: 67.16 ms\n",
      "Epoch 5 | iter 261 step 8 | loss train: 3.189, val: n/a | iter time: 66.63 ms\n",
      "Epoch 5 | iter 262 step 8 | loss train: 3.179, val: n/a | iter time: 66.66 ms\n",
      "Epoch 5 | iter 263 step 8 | loss train: 3.162, val: n/a | iter time: 67.38 ms\n",
      "Epoch 5 | iter 264 step 8 | loss train: 3.156, val: n/a | iter time: 67.16 ms\n",
      "Epoch 5 | iter 265 step 8 | loss train: 3.153, val: n/a | iter time: 67.14 ms\n",
      "Epoch 5 | iter 266 step 8 | loss train: 3.154, val: n/a | iter time: 68.24 ms\n",
      "Epoch 5 | iter 267 step 8 | loss train: 3.152, val: n/a | iter time: 73.37 ms\n",
      "Epoch 5 | iter 268 step 8 | loss train: 3.159, val: n/a | iter time: 66.71 ms\n",
      "Epoch 5 | iter 269 step 8 | loss train: 3.160, val: n/a | iter time: 66.84 ms\n",
      "Epoch 5 | iter 270 step 8 | loss train: 3.163, val: n/a | iter time: 66.73 ms\n",
      "Epoch 5 | iter 271 step 8 | loss train: 3.179, val: n/a | iter time: 66.38 ms\n",
      "Epoch 5 | iter 272 step 8 | loss train: 3.179, val: n/a | iter time: 67.56 ms\n",
      "Epoch 5 | iter 273 step 8 | loss train: 3.178, val: n/a | iter time: 66.81 ms\n",
      "Epoch 5 | iter 274 step 8 | loss train: 3.192, val: n/a | iter time: 66.63 ms\n",
      "Epoch 5 | iter 275 step 8 | loss train: 3.198, val: n/a | iter time: 66.46 ms\n",
      "Epoch 5 | iter 276 step 8 | loss train: 3.198, val: n/a | iter time: 67.31 ms\n",
      "Epoch 5 | iter 277 step 8 | loss train: 3.201, val: n/a | iter time: 68.38 ms\n",
      "Epoch 5 | iter 278 step 8 | loss train: 3.199, val: n/a | iter time: 66.99 ms\n",
      "Epoch 5 | iter 279 step 8 | loss train: 3.200, val: n/a | iter time: 67.33 ms\n",
      "Epoch 5 | iter 280 step 8 | loss train: 3.206, val: n/a | iter time: 67.50 ms\n",
      "Epoch 5 | iter 281 step 8 | loss train: 3.196, val: n/a | iter time: 66.90 ms\n",
      "Epoch 5 | iter 282 step 8 | loss train: 3.200, val: n/a | iter time: 65.98 ms\n",
      "Epoch 5 | iter 283 step 8 | loss train: 3.204, val: n/a | iter time: 66.33 ms\n",
      "Epoch 5 | iter 284 step 8 | loss train: 3.205, val: n/a | iter time: 67.22 ms\n",
      "Epoch 5 | iter 285 step 8 | loss train: 3.208, val: n/a | iter time: 65.57 ms\n",
      "\n",
      "| ------------------------------------------------------\n",
      "| Token Counts\n",
      "| - Input Tokens              :   6125\n",
      "| - Tokens w/ Prompt          :  19620\n",
      "| - Total Tokens (w/ Padding) :  19620\n",
      "| -----------------------------------------------------\n",
      "| Performance\n",
      "| - Training Time             :  22.41 s\n",
      "| - Tok/sec                   :  875.44 tok/s\n",
      "| -----------------------------------------------------\n",
      "| Memory Usage                                                                 \n",
      "| - Memory Used               :  8.77 GB                                        \n",
      "-------------------------------------------------------\n",
      "\n",
      "Validating ...\n",
      "Final evaluation | val loss: 3.217 | val ppl: 24.963\n",
      "Saving LoRA weights to '/teamspace/studios/this_studio/results/fine-tuned-llama-3.2-1B/final/lit_model.pth.lora'\n",
      "{'checkpoint_dir': PosixPath('/teamspace/studios/this_studio/results/fine-tuned-llama-3.2-1B/final'),\n",
      " 'precision': None,\n",
      " 'pretrained_checkpoint_dir': None}\n",
      "Saved merged weights to '/teamspace/studios/this_studio/results/fine-tuned-llama-3.2-1B/final/lit_model.pth'\n"
     ]
    }
   ],
   "source": [
    "# Define the path of your model in the first line \n",
    "# 2nd line: Set the data format to JSON (for custom data)\n",
    "# 3rd line: Path to the training data in JSONL format\n",
    "# 4th line: Path for saving the fine-tuned model output\n",
    "# 5th line: Define the Batch size \n",
    "# 6th line: Define the number of Epochs\n",
    "# Don't use ! if you are in the terminal (not in Jupyter Notebook)\n",
    "!litgpt finetune_lora checkpoints/meta-llama/Llama-3.2-1B \\\n",
    "--data JSON \\\n",
    "--data.json_path $(pwd)/data \\\n",
    "--out_dir results/fine-tuned-llama-3.2-1B \\\n",
    "--train.global_batch_size=32 \\\n",
    "--train.epochs=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further fine-tune models\n",
    "\n",
    "If you want to further train a model, you can actually use the same commands as before, the only thing that will change will be the directory where the new model is stored:\n",
    "\n",
    "```bash\n",
    "# 1st line: Change the path to the directory where the results are\n",
    "litgpt finetune_lora results/fine-tuned-llama-3.2-1B/final \\\n",
    "--data JSON \\\n",
    "--data.json_path $(pwd)/00-Lectures/src \\\n",
    "--out_dir results/fine-tuned-llama-3.2-1B \\\n",
    "--train.global_batch_size=32 \\\n",
    "--train.epochs=5\n",
    "```\n",
    "\n",
    "If you need to further configure your model, I advise you to create a `config.yaml` file and then add it as a flag in your bash command:\n",
    "\n",
    "```bash\n",
    "litgpt results/fine-tuned-llama-3.2-1B/final/ \\\n",
    "--config config.yaml\n",
    "```\n",
    "\n",
    "You can use the following template for your `config.yaml`:\n",
    "\n",
    "* [Configuration template](https://full-stack-assets.s3.eu-west-3.amazonaws.com/config.yaml)\n",
    "\n",
    "<Note type=\"tip\">\n",
    "\n",
    "Keep the `results` folder somewhere. This is what you will use whenever you want to stage your model into production later on üòâ\n",
    "\n",
    "</Note>\n",
    "\n",
    "## Pretraining\n",
    "\n",
    "Now the only thing with these commands is that when you launch a job, you will see this output:\n",
    "\n",
    "```bash \n",
    "Number of trainable parameters: 1,126,400\n",
    "Number of non-trainable parameters: 1,100,048,384\n",
    "```\n",
    "\n",
    "This means that most of the model's weights won't be further trained. If that's what you want to do, you will need to use another method called `pretrain`. Let's see an example:\n",
    "\n",
    "<Note type=\"important\" type=\"disclaimer\">\n",
    "\n",
    "As of today, Pretraining (whether with Python or with the command-line) is pretty limited as custom dataset is not extremely well supported (see the code below for more info). This will be fixed over time. \n",
    "\n",
    "However, again I want to stress out the fact that Pretraining LLMs from scratch is very costly and therefore, you will see poor performance unless you have:\n",
    "\n",
    "* A 1TB dataset to train on \n",
    "* A large model with large GPUs \n",
    "\n",
    "\n",
    "The example below is mainly to illustrate this point, and unless you will be working in tech company that works on foundational models, you most likely won't need to train your own LLM from scratch (and we definitely don't advise you to!)\n",
    "\n",
    "</Note>\n",
    "\n",
    "### Prepare dataset \n",
    "\n",
    "Before running a training job, we need to prepare a dataset. LitGPT accepts two type of data:\n",
    "\n",
    "- `.txt` local files \n",
    "- [LitData](https://github.com/Lightning-AI/litdata): These are datasets optimized via LitData and hosted on an S3\n",
    "\n",
    "For this course we will cover the first option. You can download the whole dataset here:\n",
    "\n",
    "- [Star Wars Text Dataset](https://full-stack-assets.s3.eu-west-3.amazonaws.com/Text_files.zip)\n",
    "\n",
    "Now all you have to do is to run:\n",
    "\n",
    "```bash \n",
    "# 1st line: This is a limitation but you absolutely need to specify a \"base\" model from the litgpt pretrain list \n",
    "# 2nd line: Since we fine tuned a model based on tiny llama, let's try to further pretrain it without locking any weights\n",
    "# 3nd line: Define the output directory\n",
    "# 4th line: Define the type of data to receive a .txt files \n",
    "# 5th line: This define the path where the .txt files should be (here at ./Text_files)\n",
    "# 6th line: Set a maximum number of tokens \n",
    "# 7th line: Give a maximum sequence length of tokens. AS OF TODAY there is a bug on the library and you need to add a number that is below 2048\n",
    "# See more here: https://github.com/Lightning-AI/litgpt/issues/1450\n",
    "litgpt pretrain meta-llama/Llama-3.2-1B \\\n",
    "   --initial_checkpoint_dir results/fine-tuned-tiny-llama/final \\\n",
    "   --tokenizer_dir results/fine-tuned-llama-3.2-1B/final \\\n",
    "   --out_dir ./new_pretrained_checkpoint \\\n",
    "   --data TextFiles \\\n",
    "   --data.train_data_path Text_files \\\n",
    "   --train.max_tokens 1_000_000 \\\n",
    "   --train.max_seq_length 1000\n",
    "```\n",
    "\n",
    "By default, the training job will run for 6 epochs. Once it is done, you can try your new model:\n",
    "\n",
    "```bash \n",
    "litgpt chat new_pretrained_checkpoint/final\n",
    "```\n",
    "\n",
    "You might be a bit disappointed with the results as you might see something along the lines below:\n",
    "\n",
    "```\n",
    "him ! ! him very ... oh!l a h...!.! .!. him. oh!  !  !'.... !..! oh... it..! him  !\n",
    "```\n",
    "\n",
    "This is normal, you have two phenomenon coming into play:\n",
    "\n",
    "1. The dataset is relatively small and there are a lot of `him` / `very` / `oh` tokens. So the model is blindingly repeating those words \n",
    "2. When training from scratch, there is a possibility of forgetting previous training epochs : the loss is becoming higher as we go through more epochs \n",
    "\n",
    "A solution would be to increase the size of the dataset and increase the number of epochs for the model to learn better the subtleties of the dataset. This is a great example to illustrate that continued learning is the costliest option when it comes to training LLMs \n",
    "\n",
    "<Note type=\"important\">\n",
    "\n",
    "If you want to follow along and run the above pretraining script, you will need to change your Studio to **L40S** GPUs as it requires a lot of RAMs to load the full model and start training it.\n",
    "\n",
    "</Note>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python API\n",
    "\n",
    "Playing on the terminal is great but you might want to have further control and use litgpt in an API. If that's the case, you should use LitGPT Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The answer is simple: Darth Vader. He is the most powerful Jedi in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the galaxy. He is the most powerful Sith in the galaxy, and he is the most powerful Sith in the"
     ]
    }
   ],
   "source": [
    "from litgpt import LLM\n",
    "\n",
    "llm = LLM.load(\"checkpoints/meta-llama/Llama-3.2-1B\") # You can also add the path of any of your checkpoints\n",
    "text = llm.generate(\"What is the name of the most powerful Jedi in the galaxy?\", top_k=1, max_new_tokens=300)\n",
    "\n",
    "\n",
    "# For text generation in streaming \n",
    "for token in text:\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Note type=\"note\">\n",
    "\n",
    "LitGPT technically also support pretraining in Python but it is pretty limited as of today as you won't be able to use custom dataset (documentation will show you how to, but the code won't work). However, if you are curious, feel free to checkout the documentation:\n",
    "\n",
    "- [Python API for Pretraining jobs](https://github.com/Lightning-AI/litgpt/blob/main/tutorials/python-api.md)\n",
    "\n",
    "</Note>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources üìöüìö\n",
    "\n",
    "* [LightiningAI](https://lightning.ai/docs/overview/getting-started)\n",
    "* [Connect to Local IDE](https://lightning.ai/docs/overview/studios/connect-local-ide)\n",
    "* [LitData](https://github.com/Lightning-AI/litdata)\n",
    "* [Python API](https://github.com/Lightning-AI/litgpt/blob/main/tutorials/python-api.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
