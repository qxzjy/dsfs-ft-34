{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Langchain\n",
    "\n",
    "## What you will learn in this course üßêüßê\n",
    "\n",
    "Once you know how to manipulate Documents and store them in a VectorDB, you have everything you need to perform RAG. We will still use Langchain to do so. In this course, you will learn:\n",
    "\n",
    "* How to query a VectorDB \n",
    "* Retrievers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Setup \n",
    "\n",
    "For this demo, you will need to: \n",
    "\n",
    "* Run a Weaviate Database \n",
    "```bash \n",
    "docker run -p 8080:8080 -p 50051:50051 cr.weaviate.io/semitechnologies/weaviate:1.27.0\n",
    "```\n",
    "\n",
    "* Populate the VectorDB with the code we wrote in the previous lesson. For standard environment setup, you can use docker:\n",
    "\n",
    "```bash \n",
    "docker run -v $(pwd):/home/jovyan -p 8888:8888 jupyter/datascience-notebook\n",
    "```\n",
    "\n",
    "<Note type=\"note\">\n",
    "\n",
    "Yes you will have two containers running at the same time. That's completely fine üëå\n",
    "\n",
    "</Note>\n",
    "\n",
    "Then follow the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install package\n",
    "%pip install -Uqq langchain-weaviate\n",
    "%pip install langchain langchain_mistralai -q\n",
    "%pip install -qU langchain-community beautifulsoup4\n",
    "%pip install -qU weaviate-client\n",
    "%pip install sentence-transformers -q \n",
    "%pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/qrbwlc0x6fj2rs4chrxkrm_m0000gn/T/ipykernel_49761/2788632218.py:66: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings()\n",
      "/var/folders/yt/qrbwlc0x6fj2rs4chrxkrm_m0000gn/T/ipykernel_49761/2788632218.py:66: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2025-May-16 12:07 PM - langchain_weaviate.vectorstores - INFO - Tenant Wookieepedia does not exist in index LangChain_59d4f6a4963e4befa58a225f7ce05e4c. Creating tenant.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "import weaviate\n",
    "\n",
    "# Add a BeautifulSoup Extractor \n",
    "# This function will be used to read the HTML extracted from our Loader\n",
    "# and parsed in a more readable manner\n",
    "def bs4_extractor(html: str) -> str:\n",
    "    \"\"\"Extract only titles and paragraphs of an HTML content\"\"\"\n",
    "    try:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Extract the title\n",
    "        title = soup.title.string if soup.title else \"No title found\"\n",
    "        \n",
    "        # Extract all paragraphs\n",
    "        paragraphs = [p.get_text() for p in soup.find_all('p')]\n",
    "        \n",
    "        # Combine title and paragraphs into a single string\n",
    "        extracted_content = title + \"\\n\" + \"\\n\".join(paragraphs)\n",
    "    \n",
    "        return extracted_content\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# This instanciate a loader\n",
    "loader = RecursiveUrlLoader(\n",
    "    \"https://starwars.fandom.com/wiki/Jedha\", # Everything about Jedha\n",
    "    max_depth=1, # How deep crawler will follow links (here we technically don't follow any links to retrieve limited amount of data)\n",
    "    use_async=False,\n",
    "    extractor=bs4_extractor, # This can be replaced by a function to extract HTML from the web page (let's say you might want to only extract <table></table> you could create a function for that)\n",
    "    metadata_extractor=None, # Same as the above\n",
    "    timeout=10, # Maximum time in seconds before raises a TimeOut error\n",
    "    continue_on_failure=True, # Continue to crawl even if there are some parsing errors\n",
    "    prevent_outside=True, # Prevent from loading URLs which are not children of the root URL -> Good to prevent attacks\n",
    "    # check out full documentation if you want to read about all arguments - https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html#langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.__init__\n",
    ")\n",
    "\n",
    "# Now we need to load the actual documents \n",
    "docs = loader.load()\n",
    "\n",
    "# Here we use pretrained Tokenizer offered by hugging face. This gives us definitely more \n",
    "# accurate splitting\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "# Instanciate a splitter \n",
    "# There are plenty of different splitters see below to learn more\n",
    "splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer), # Maximum of 1000 characters in each splitted documents)\n",
    "\n",
    "# Now create splits \n",
    "splitted_docs = splitter[0].split_documents(docs)\n",
    "\n",
    "client = weaviate.connect_to_local(\n",
    "    #host=\"host.docker.internal\",  # Use host.docker.internal if you are running it inside a docker container\n",
    "    port=8080,\n",
    "    grpc_port=50051,\n",
    ")\n",
    "\n",
    "# Instanciate Embeddings\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "# Now we can load our documents into our Database \n",
    "# Depending on the amount of data \n",
    "# The time necessary to execute the cell will vary\n",
    "vectorstore = WeaviateVectorStore.from_documents(\n",
    "    splitted_docs, \n",
    "    embeddings, \n",
    "    client=client, \n",
    "    by_text=False, \n",
    "    tenant=\"Wookieepedia\", # This is the name of the collection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Note type=\"note\">\n",
    "\n",
    "The above code is just the full code from the previous lecture. If you want to have more details, feel free to refer to it. \n",
    "\n",
    "</Note>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## What are Retrievers ü´¥\n",
    "\n",
    "When you are querying a VectorDB, you will need to use what Langchain calls a **Retriever**. This is simply the tool that is here to *retrieve* relevant data from your database. \n",
    "\n",
    "There various algorithms behind evevery retrievers. Some of them are using Unsupervised Machine Learning, others are using LLMs and some others are using just simple word match. You can find the list of all retrievers here:\n",
    "\n",
    "* [All Langchain Retrievers](https://python.langchain.com/docs/integrations/retrievers/)\n",
    "\n",
    "\n",
    "You can either use one of the above or VectorDBs often come with pre-built retrievers that you can use as well! Let's see how that works right now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve Mistral API key from .env\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The initial name of Jedha was NiJedha. Jedha, also known as the Pilgrim Moon, the Cold Moon, or the Kyber Heart, was formerly known as NiJedha. NiJedha was the Holy City of Jedha, a spiritual hub for many faiths.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain import hub\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\")\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2, \"tenant\": \"Wookieepedia\"})\n",
    "\n",
    "# Create prompt. \n",
    "# This can also be found at hub.pull(\"rlm/rag-prompt\")\n",
    "prompt = \"\"\"\n",
    "You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context} \n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# This is the basic chat prompt template \n",
    "# You can then add a MessagePlaceHolder etc. \n",
    "# to add memory to your LLM app!\n",
    "prompt = ChatPromptTemplate(\n",
    "    (\"system\", prompt)\n",
    ")\n",
    "\n",
    "# This is a helper function to join all the documents that will be retrieved\n",
    "# by the retriever and then just concatenated as one big string that will placed at {context} in the prompt above \n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain will first receive a question from the user\n",
    "# This will populate the \"context\" that will retrieve all document based on the {question} thanks to `retriever`\n",
    "# After context is retrieved by the retriever it will directly go to `format_docs` function \n",
    "# At the same time \"question\" will be passed through the next phase of the chain (the `prompt`) \n",
    "# This is done by `RunnablePassthrough` which purpose is to pass information through the chain\n",
    "# Finally the output is parsed as string\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What was the initial name of Jedha?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources üìöüìö\n",
    "\n",
    "* [Weaviate - Langchain](https://python.langchain.com/docs/integrations/vectorstores/weaviate/)\n",
    "* [`RecursiveUrlLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html#langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.__init__)\n",
    "* [HuggingFace Tokenizers](https://huggingface.co/docs/transformers/en/main_classes/tokenizer)\n",
    "* [All Langchain Splitters](https://python.langchain.com/api_reference/text_splitters/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
