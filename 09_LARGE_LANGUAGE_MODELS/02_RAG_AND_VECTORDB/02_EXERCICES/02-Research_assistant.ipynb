{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Assistant \n",
    "\n",
    "In the previous exercise, we were using a vector database to retrieve records containing relevant information based on natural language queries\n",
    "Now let's create an AI application that will answer questions based on what is inside our database:\n",
    "\n",
    "\n",
    "## Step 0 - Setup \n",
    "\n",
    "Make sure you have:\n",
    "\n",
    "* A running Weaviate DB with data from the previous exercise \n",
    "* A running jupyter notebook with everything below installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install package\n",
    "%pip install -Uqq langchain-weaviate\n",
    "%pip install langchain langchain_mistralai langchain_huggingface -q\n",
    "%pip install -qU weaviate-client\n",
    "%pip install sentence-transformers -q \n",
    "%pip install transformers -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step I - Create your VectorStore client\n",
    "\n",
    "First you need to create a VectorStore client that will call your Weaviate DB.\n",
    "\n",
    "Define an environment variable for your mistral api key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve Mistral API key from .env\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to your weaviate database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "import os\n",
    "\n",
    "weaviate_url = os.environ[\"WEAVIATE_URL\"]\n",
    "weaviate_api_key = os.environ[\"WEAVIATE_API_KEY\"]\n",
    "\n",
    "# Connect to Weaviate Cloud\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=weaviate_url,\n",
    "    auth_credentials=Auth.api_key(weaviate_api_key),\n",
    ")\n",
    "\n",
    "# client = weaviate.connect_to_local(\n",
    "#     #host=\"host.docker.internal\",  # Use host.docker.internal if you are running it inside a docker container\n",
    "#     port=8080,\n",
    "#     grpc_port=50051,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all existing collections in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LangChain_5bc7e27ecd0747218db36fbf82ce55b8': _CollectionConfigSimple(name='LangChain_5bc7e27ecd0747218db36fbf82ce55b8', description=None, generative_config=None, properties=[_Property(name='text', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=None, vectorizer='none', vectorizer_configs=None), _Property(name='sources', description=\"This property was generated by Weaviate's auto-schema feature on Fri May 16 13:54:08 2025\", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=None, vectorizer='none', vectorizer_configs=None), _Property(name='chunk_id', description=\"This property was generated by Weaviate's auto-schema feature on Fri May 16 13:54:08 2025\", data_type=<DataType.NUMBER: 'number'>, index_filterable=True, index_range_filters=False, index_searchable=False, nested_properties=None, tokenization=None, vectorizer_config=None, vectorizer='none', vectorizer_configs=None)], references=[], reranker_config=None, vectorizer_config=None, vectorizer=<Vectorizers.NONE: 'none'>, vector_config=None)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.collections.list_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the documents into a vector store object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/qrbwlc0x6fj2rs4chrxkrm_m0000gn/T/ipykernel_4715/2890979363.py:4: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Instanciate Embeddings\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "# Now we can load our documents into our Database \n",
    "# Depending on the amount of data \n",
    "# The time necessary to execute the cell will vary\n",
    "vectorstore = WeaviateVectorStore.from_documents(\n",
    "    [],\n",
    "    client=client,\n",
    "    embedding=embeddings,\n",
    "    index_name=\"LangChain_5bc7e27ecd0747218db36fbf82ce55b8\",\n",
    "    use_multi_tenancy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP II - Build your RAG Application \n",
    "\n",
    "Alright now you have your client, let's build your AI application with RAG here are the steps:\n",
    "\n",
    "1. Load a llm model\n",
    "2. Define a retriver object using yout vector store based on your llm\n",
    "3. Prepare a prompt in which you could insert a question, and context\n",
    "4. Prepare a RAG chain with the retriever, the prompte, the llm, and an output parser\n",
    "5. Invoke the chain on some example input such as: \"Tell me everything I need to know about LLMs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLMs, or Large Language Models, are a type of neural network trained on vast amounts of text data to understand and generate natural language. They differ from traditional programming by learning to perform tasks rather than following explicit instructions. LLMs are built using transformers, which employ an attention mechanism to understand the context of words within a sentence. These models are trained through a process involving tokenization, embedding, and transforming tokens into vectors to predict the next word based on context.\\n\\nKey points about LLMs include their ability to handle tasks like summarization, text generation, and question-answering. They improve through fine-tuning, where pre-trained models are adapted to specific tasks or domains. Ethical considerations, such as bias and safety, are also crucial in their development and deployment.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\")\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5, \"tenant\": \"knowledge_base_llm\"})\n",
    "\n",
    "# Create prompt. \n",
    "# This can also be found at hub.pull(\"rlm/rag-prompt\")\n",
    "prompt = \"\"\"\n",
    "You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context} \n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# This is the basic chat prompt template \n",
    "# You can then add a MessagePlaceHolder etc. \n",
    "# to add memory to your LLM app!\n",
    "prompt = ChatPromptTemplate(\n",
    "    (\"system\", prompt)\n",
    ")\n",
    "\n",
    "# This is a helper function to join all the documents that will be retrieved\n",
    "# by the retriever and then just concatenated as one big string that will placed at {context} in the prompt above \n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain will first receive a question from the user\n",
    "# This will populate the \"context\" that will retrieve all document based on the {question} thanks to `retriever`\n",
    "# After context is retrieved by the retriever it will directly go to `format_docs` function \n",
    "# At the same time \"question\" will be passed through the next phase of the chain (the `prompt`) \n",
    "# This is done by `RunnablePassthrough` which purpose is to pass information through the chain\n",
    "# Finally the output is parsed as string\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"Tell me everything I need to know about LLMs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP III - Verify that you model actually used the knowledge base \n",
    "\n",
    "Just to make sure, try to verify that your model used the knowledge base when formulating its answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'chunk_id': 0.0, 'sources': 'Large Language Models (LLMs) - Everything You NEED To Know.m4a'}, page_content=\"This video is going to give you everything you need to go from knowing absolutely nothing about artificial intelligence and large language models to having a solid foundation of how these revolutionary technologies work. Over the past year, artificial intelligence has completely changed the world, with products like ChatGPT potentially appending every single industry and how people interact with technology in general. And in this video, I will be focusing on LLMs, how they work, ethical considerations, applications, and so much more. And this video was created in collaboration with an incredible program called AI Camp, in which high school students learn all about artificial intelligence. And I'll talk more about that later in the video. Let's go. So first, what is an LLM? Is it different from AI? And how is ChatGPT related to all of this? LLMs stand for large language models, which is a type of neural network that's trained on massive amounts of text data. It's generally trained on data that can be found online. Everything from web scraping to books to transcripts, anything that is text-based can be trained into a large language model. And taking a step back, what is a neural network? A neural network is essentially a series of algorithms that try to recognize patterns in data. And really what they're trying to do is simulate how the human brain works. And LLMs are a specific type of neural network that focus on understanding natural language. And as mentioned, LLMs learn by reading tons of books, articles, internet text, and there's really no limitation there. And so how do LLMs differ from traditional programming? Well, with traditional programming, it's instruction-based, which means if X, then Y. You're explicitly telling the computer what to do. You're giving it a set of instructions to execute. But with LLMs, it's a completely different story. You're teaching the computer not how to do things, but how to learn how to do things. And this is a much more flexible approach and is really good for a lot of different applications where previously traditional coding could not accomplish them. So one example application is image recognition. With image recognition, traditional programming would require you to hard code every single rule for how to, let's say, identify different letters. So A, B, C, D. But if you're handwriting these letters, everybody's handwritten letters look different. So how do you use traditional programming to identify every single possible variation? Well, that's where this AI approach comes in. Instead of giving a computer explicit instructions for how to identify a handwritten letter, you instead give it a bunch of examples of what handwritten letters look like, and then it can infer what a new handwritten letter looks like based on all of the examples that it has. What also sets machine learning and large language models apart in this new approach to programming is that they are much more flexible, much more adaptable, meaning they can learn from their mistakes and inaccuracies, and are thus so much more scalable than traditional programming. LLMs are incredibly powerful at a wide range of tasks, including summarization, text generation, creative writing, question and answer, programming, and if you've watched any of my videos, you know how powerful these large language models can be. And they're only getting better. Know that right now, large language models and AI in general are the worst they'll ever be. And as we're generating more data on the internet, and as we use synthetic data, which means data created by other large language models, these models are going to get better rapidly. And it's super exciting to think about what the future holds. Now let's talk a little bit about the history and evolution of large language models. We're going to cover just a few of the large language models today in this section. The history of LLMs traces all the way back to the ELISA model, which was from 1966, which was really the first language model. It had pre-programmed answers based on keywords. It had a very limited understanding of the English language. And like many early language models, you started to see holes in its logic after a few back and forths in a conversation. And then after that, language models really didn't evolve for a very long time. Although technically the first recurrent neural network was created in 1924 or RNN, they weren't really able to learn until 1972. And these new learning language models are a series of neural networks with layers and weights and a whole bunch of stuff that I'm not going to get into in this video. And RNNs were really the first technology that was able to predict the next word in a sentence rather than having everything pre-programmed for it. And that was really the basis for how current large language models work. And even after this and the advent of deep learning in the early 2000s, the field of AI evolved very slowly, with language models far behind what we see today. This all changed in 2017, where the Google DeepMind team released a research paper about a new technology called Transformers. And this paper was called Attention is All You Need. And a quick side note, I don't think Google even knew quite what they had published at that time. But that same paper is what led OpenAI to develop ChatGPT. So obviously other computer scientists saw the potential for the Transformers architecture. With this new Transformers architecture, it was far more advanced. It required decreased training time, and it had many other features like self-attention, which I'll cover later in this video. Transformers allowed for pre-trained large language models like GPT-1, which was developed by OpenAI in 2018. It had 117 million parameters, and it was completely revolutionary, but soon to be outclassed by other LLMs. Then after that, BERT was released, B-E-R-T, in 2018. That had 340 million parameters, and had bidirectionality, which means it had the ability to process text in both directions, which helped it have a better understanding of context. And as comparison, a unidirectional model only has an understanding of the words that came before the target text. And after this, LLMs didn't develop a lot of new technology, but they did increase greatly in scale. GPT-2 was released in early 2019, and had 2.5 billion parameters. Then GPT-3 in June of 2020, with 175 billion parameters. And it was at this point that the public started noticing large language models. GPT had a much better understanding of natural language than any of its predecessors. And this is the type of model that powers ChatGPT, which is probably the model that you're most familiar with. And ChatGPT became so popular because it was so much more accurate than anything anyone had ever seen before. And it was really because of its size. And because it was now built into this chatbot format, anybody could jump in and really understand how to interact with this model. ChatGPT 3.5 came out in December of 2022, and started this current wave of AI that we see today. Then in March 2023, GPT-4 was released, and it was incredible, and still is incredible to this day. It had a whopping reported 1.76 trillion parameters, and uses likely a mixture of experts approach, which means it has multiple models that are all fine-tuned for specific use cases. And then when somebody asks a question to it, it chooses which of those models to use. And then they added multi-modality and a bunch of other features. And that brings us to where we are today. All right, now let's talk about how LLMs actually work in a little bit more detail. The process of how large language models work can be split into three steps. The first of these steps is called tokenization. And there are neural networks that are trained to split long text into individual tokens. And a token is essentially about three fourths of a word. So if it's a shorter word like hi, or that, or there, it's probably just one token. But if you have a longer word like summarization, it's going to be split into multiple pieces. And the way that tokenization happens is actually different for every model. Some of them separate prefixes and suffixes. Let's look at an example. What is the tallest building? So what is the tallest building? Are all separate tokens. And so that separates the suffix off of tallest, but not building because it is taking the context into account. And this step is done so models can understand each word individually, just like humans. We understand each word individually and as groupings of words. And then the second step of LLMs is something called embeddings. The large language models turns those tokens into embedding vectors, turning those tokens into essentially a bunch of numerical representations of those tokens numbers. And this makes it significantly easier for the computer to read and understand each word and how the different words relate to each other. And these numbers all correspond with the position in an embeddings vector database. And then the final step in the process is transformers, which we'll get to in a little bit. But first, let's talk about vector databases. And I'm going to use the terms word and token interchangeably. So just keep that in mind because they're almost the same thing, not quite, but almost. And so these word embeddings that I've been talking about are placed into something called a vector database. These databases are storage and retrieval mechanisms that are highly optimized for vectors. And again, those are just numbers, long series of numbers. Because they're converted into these vectors, they can easily see which words are related to other words based on how similar they are, how close they are based on their embeddings. And that is how the large language model is able to predict the next word based on the previous words. Vector databases capture the relationship between data as vectors in multi-dimensional space. I know that sounds complicated, but it's really just a lot of numbers. Vectors are objects with a magnitude and a direction, which both influence how similar one vector is to another. And that is how LLMs represent words based on those numbers. Each word gets turned into a vector, capturing semantic meaning and its relationship to other words. So here's an example. The words book and worm, which independently might not look like they're related to each other, but they are related concepts because they frequently appear together. A bookworm, somebody who likes to read a lot. And because of that, they will have embeddings that look close to each other. And so models build up an understanding of natural language using these embeddings and looking for similarity of different words, terms, groupings of words, and all of these nuanced relationships. And the vector format helps models understand natural language better than other formats. And you can kind of think of all this like a map. If you have a map with two landmarks that are close to each other, they're likely going to have very similar coordinates. So it's kind of like that. Okay, now let's talk about transformers. Matrix representations can be made out of those vectors that we were just talking about. This is done by extracting some information out of the numbers and placing all of the information into a matrix through an algorithm called multi-head attention. The output of the multi-head attention algorithm is a set of numbers which tells the model how much the words and its order are contributing to the sentence as a whole. We transform the input matrix into an output matrix which will then correspond with a word having the same values as that output matrix. So basically we're taking that input matrix, converting it into an output matrix, and then converting it into natural language. And the word is the final output of this whole process. This transformation is done by the algorithm that was created during the training process. So the model's understanding of how to do this transformation is based on all of its knowledge that it was trained with, all of that text data from the internet, from books, from articles, etc. And it learned which sequences of words go together and their corresponding next words based on the weights determined during training. Transformers use an attention mechanism to understand the context of words within a sentence. It involves calculations with the dot product, which is essentially a number representing how much the word contributed to the sentence. It will find the difference between the dot products of words and give it correspondingly large values for attention. And it will take that word into account more if it has higher attention. Now let's talk about how large language models actually get trained. The first step of training a large language model is collecting the data. You need a lot of data. When I say billions of parameters, that is just a measure of how much data is actually going into training these models. And you need to find a really good data set. If you have really bad data going into a model, then you're going to have a really bad model. Garbage in, garbage out. So if a data set is incomplete or biased, the large language model will be also. And data sets are huge. We're talking about massive, massive amounts of data. They take data in from web pages, from books, from conversations, from reddit posts, from x posts, from youtube transcriptions. Basically anywhere where we can get some text data, that data is becoming so valuable. Let me put into context how massive the data sets we're talking about really are. So here's a little bit of text which is 276 tokens. That's it. Now if we zoom out, that one pixel is that many tokens. And now here's a representation of 285 million tokens, which is 0.02% of the 1.3 trillion tokens that some large language models take to train. And there's an entire science behind data pre-processing, which prepares the data to be used to train a model. Everything from looking at the data quality, to labeling consistency, data cleaning, data transformation, and data reduction. But I'm not going to go too deep into that. And this pre-processing can take a long time. And it depends on the type of machine being used, how much processing power you have, the size of the data set, the number of pre-processing steps, and a whole bunch of other factors that make it really difficult to know exactly how long pre-processing is going to take. But one thing that we know takes a long time is the actual training. Companies like NVIDIA are building hardware specifically tailored for the math behind large language models. And this hardware is constantly getting better. The software used to process these models are getting better also. And so the total time to process models is decreasing, but the size of the models is increasing. And to train these models, it is extremely expensive because you need a lot of processing power, electricity, and these chips are not cheap. And that is why NVIDIA stock price has skyrocketed. Their revenue growth has been extraordinary. And so with the process of training, we take this pre-processed text data that we talked about earlier, and it's fed into the model. And then using transformers, or whatever technology a model is actually based on, but most likely transformers, it will try to predict the next word based on the context of that data. And it's going to adjust the weights of the model to get the best possible output. And this process repeats millions and millions of times over and over again until we reach some optimal quality. And then the final step is evaluation. A small amount of the data is set aside for evaluation. And the model is tested on this data set for performance. And then the model is adjusted if necessary. The metric used to determine the effectiveness of the model is called perplexity. It will compare two words based on their similarity. And it will give a good score if the words are related and a bad score if it's not. And then we also use RLHF, reinforcement learning through human feedback. And that's when users or testers actually test the model and provide positive or negative scores based on the output. And then once again, the model is adjusted as necessary. All right, let's talk about fine-tuning now, which I think a lot of you are going to be interested in because it's something that the average person can get into quite easily. So we have these popular large language models that are trained on massive sets of data to build general language capabilities. And these pre-trained models, like BERT, like GPT, give developers a head start versus training models from scratch. But then in comes fine-tuning, which allows us to take these raw models, these foundation models, and fine-tune them for our specific use cases. So let's think about an example. Let's say you want to fine-tune a model to be able to take pizza orders, to be able to have conversations, answer questions about pizza, and finally be able to allow customers to buy pizza. You can take a pre-existing set of conversations that exemplify the back and forth between a pizza shop and a customer, load that in, fine-tune a model, and then all of a sudden that model is going to be much better at having conversations about pizza ordering. The model updates the weights to be better at understanding certain pizza terminology, questions, responses, tone, everything. And fine-tuning is much faster than a full training, and it produces much higher accuracy. And fine-tuning allows pre-trained models to be fine-tuned for real-world use cases. And finally, you can take a single foundational model and fine-tune it any number of times for any number of use cases. And there are a lot of great services out there that allow you to do that. And again, it's all about the quality of your data. So if you have a really good data set that you're going to fine-tune a model on, the model is going to be really, really good. And conversely, if you have a poor quality data set, it's not going to perform as well. All right, let me pause for a second and talk about AI Camp. So as mentioned earlier, this video, all of its content, the animations, have been created in collaboration with students from AI Camp. AI Camp is a learning experience for students that are age 13 and above. You work in small, personalized groups with experienced mentors. You work together to create an AI product using NLP, computer vision, and data science. AI Camp has both a three-week and a one-week program during summer that requires zero programming experience. And they also have a new program which is 10 weeks long during the school\"),\n",
       " Document(metadata={'chunk_id': 0.0, 'sources': 'Stanford CS229 I Machine Learning I Building Large Language Models (LLMs).m4a'}, page_content=\"So, let's get started, so I'll be talking about building LLMs today, so I think a lot of you have heard of LLMs before, but just as a quick recap, LLMs, standing for Large Language Models, are basically all the chatbots that you've been hearing about recently, so ChatGPT from OpenAI, Claude from Entropiq, Gemini, and Lama, and other type of models like this, and today we'll be talking about how do they actually work, so it's going to be an overview because it's only one lecture, and it's hard to compress everything, but hopefully I'll touch a little bit about all the components that are needed to train some of these LLMs. Also, if you have questions, please interrupt me and ask. If you have a question, most likely other people in the room or on Zoom have the same question, so please ask. Great, so what matters when training LLMs? So, there are a few key components that matter. One is the architecture, so as you probably all know, LLMs are neural networks, and when you think about neural networks, you have to think about what architecture you're using. Another component which is really important is the training loss and the training algorithm, so how you actually train these models. Then it's data, so what do you train these models on? The evaluation, which is how do you know whether you're actually making progress towards the goal of LLMs, and then the system component, so that is like how do you actually make these models run on modern hardware, which is really important because these models are really large, so now more than ever, systems are actually really an important topic for LLMs. So, those five components, you probably all know that LLMs, and if you don't know, LLMs are all based on transformers, or at least some version of transformers. I'm actually not going to talk about the architecture today, one, because I gave a lecture on transformers a few weeks ago, and two, because you can find so much information online on transformers, but I think you can- there's much less information about the other four topics, so I really want to talk about those. Another thing to say is that most of academia actually focuses on architecture and training algorithm and losses. As academics, and I've done that for a lot- a big part of my career, is simply we like thinking that this is like we make new architectures, new models, and it seems like it's very important, but in reality, honestly, what matters in practice is mostly the three other topics, so data, evaluation, and systems, which is what most of industry actually focuses on. So that's also one of the reasons why I don't want to talk too much about the architecture, because really the rest is super important. Great, so overview of the lecture, I'll be talking about pre-training. So pre-training, you probably heard that word, this is the general word, this is kind of the classical language modeling paradigm, where you basically train your language model to essentially model all of internet. And then there's a post-training, which is a more recent paradigm, which is taking these large language models and making them essentially AI assistants. So this is more of a recent trend since ChatGPT. So if you ever heard of GPT-3 or GPT-2, that's really pre-training land. If you heard of ChatGPT, which you probably have, this is really post-training land. So I'll be talking about both, but I'll start with pre-training. And specifically, I'll talk about what is the task of pre-training LLMs and what is the laws that people actually use. So language modeling, this is a quick recap. Language models at a high level are simply models of probability distribution over sequences of tokens or of words. So it's basically some model of p of x1 to xl, where x1 is basically word 1 and xl is the last word in the sequence or in the sentence. So very concretely, if you have a sentence like the mouse ate the cheese, what the language model gives you is simply a probability of this sentence being uttered by a human or being found online. So if you have another sentence like the mouse ate cheese, here there's grammatical mistakes. So the model should know that this should have some syntactic knowledge. So it should know that this has less likelihood of appearing online. If you have another sentence like the cheese ate the mouse, then the model should hopefully know about the fact that usually cheese don't eat mouse. So there's some semantic knowledge and this is less likely than the first sentence. So this is basically at a high level what language models are. One word that you probably have been hearing a lot in the news are generative models. So this is just something that can generate models that can generate sentences or can generate some data. The reason why we say language models are generative models is that once you have a model of a distribution, you can simply sample from this model and then we can generate data. So you can generate sentences using a language model. So the type of models that people are all currently using are what we call autoregressive language models. And the key idea of autoregressive language models is that you take this distribution over words and you basically decompose it into the distribution of the first word, multiply it by the distribution of the likelihood of the distribution of the second word given the first word, and multiply it by p of the third word given the first two words. So there's no approximation here. This is just the chain rule of probability, which hopefully you all know about, really no approximation. This is just one way of modeling a distribution. So slightly more concisely, you can write it as a product of p's of the next word given everything which happened in the past, so of the context. So this is what we call autoregressive language models. Again, this is really not the only way of modeling distribution, this is just one way. It has some benefits and some downsides. One downside of autoregressive language models is that when you actually sample from this autoregressive language model, you basically have a for loop which generates the next word, then conditions on that next word, and then regenerate the other word. So basically, if you have a longer sentence that you want to generate, it takes more time to generate it. So there are some downsides of this current paradigm, but that's what we currently have, so I'm going to talk about this one. Great. So autoregressive language models. At a high level, what the task of autoregressive language model is, is simply predicting the next word, as I just said. So if you have a sentence like she likely prefers, one potential next word might be dogs. And the way we do it is that we first tokenize. So you take these words or sub words, you tokenize them, and then you give an ID for each token. So here I have one, two, three. Then you pass it through this black box. As I already said, we're not going to talk about the architecture. You just pass it through a model, and you then get a distribution, a probability distribution over the next word, over the next token. And then you sample from this distribution, you get a new token, and then you de-tokenize. So you get a new ID, you de-tokenize, and that's how you basically sample from a language model. One thing which is important to note is that the last two steps are actually only needed during inference. When you do training, you just need to predict the most likely token, and you can just compare to the real token, which happened in practice, and then you basically change the weights of your model to increase the probability of generating that token. Great. So autoregressive neural language models. So to be slightly more specific, still without talking about the architecture, the first thing we do is that we have all of these- oh, sorry, yes? On the previous slide, when you're predicting the probability of the next token, does this mean that your final output vector has to be the same dimensionality as the number of tokens that you have? Yes. How do you deal with if you're adding more tokens to your co-presenters? Yeah. So we're going to talk about tokenization actually later, so you will get some sense of this. You basically can't deal with adding new tokens. I'm kind of exaggerating. There are methods for doing it, but essentially people don't do it. So it's really important to think about how you tokenize your text, and that's why we'll talk about that later. But it's a very good point to note is that basically the vocabulary size, so the number of tokens that you have, is essentially the output of your language model. So it's actually pretty large. Okay, so autoregressive neural language models. First thing you do is that you take every word or every token. You embed them, so you get some vector representation for each of these tokens. You pass them through some neural network, as we said, it's a transformer. Then you get a representation for all the words in the context. So it's basically a representation of the entire sentence. You pass it through a linear layer, as you just said, to basically map it to the number so that the output, the number of outputs is the number of tokens. You then pass it through some softmax, and you basically get a probability distribution over the next words given every word in the context. And the laws that you use is basically, it's essentially a task of classifying the next token. So it's a very simple kind of machine learning task. So you use the cross-entropy laws, where you basically look at the actual target that happened, which is a target distribution, which is a one-hot encoding, which here in this case says, I saw the real word that happened is cat. So that's a one-hot distribution over cat. And here, this is the distribution that you generated. And basically, you do cross-entropy, which really just increases the probability of generating cat and decreases the probability of generating all the other tokens. One thing to notice is that, as you all know, again, this is just equivalent to maximizing the text log likelihood, because you can just rewrite the max over the probability of this autoregressive language modeling task as just being this minimum of, I just added the log here and minus, which is just the minimum of the loss, which is the cross-entropy loss. So basically, minimizing the loss is the same thing as maximizing the likelihood of your text. Any questions? OK, tokenizer. So this is one thing that people usually don't talk that much about. Tokenizers are extremely important. So it's really important that you understand, at least, what they do at a high level. So why do we need tokenizers in the first place? First, it's more general than words. So one simple thing that you might think is, oh, we're just going to take every word that we will have. And you just say, every word is a token in its own. But then what happens is, if there's a typo in your word, then you might not have any token associated with this word with a typo. And then you don't know how to actually pass this word with a typo into a large language model. So what do you do next? And also, even if you think about words, words are fine with Latin-based languages. But if you think about a language like Thai, you won't have a simple way of tokenizing by spaces, because there are no spaces between words. So really, tokens are much more general than words. First thing. Second thing that you might think is that you might tokenize every sentence, character by character. You might say, A is one token, B is another token. That would actually work, and probably very well. The issue is that then your sequence becomes super long. And as you probably remember from the lecture on transformers, the complexity grows quadratically with the length of sequences. So you really don't want to have a super long sequence. So tokenizers basically try to deal with those two problems and give common sub-sequences a certain token. And usually, how you should be thinking about it is around an average of every token is around three, four letters. And there are many algorithms for tokenization. I'll just talk about one of them to give you a high level, which is what we call byte-pair encoding, which is actually pretty common, one of the two most common tokenizers. And the way that you train a tokenizer is that first, you start with a very large corpus of text. And here, I'm really not talking about training a large language model yet. This is purely for the tokenization step. So this is my large corpus of text with these five words. Then you associate every character in this corpus of text a different token. So here, I just split up every character with a different token, and I just color coded all of those tokens. And then what you do is that you go through your text, and every time you see pairs of tokens that are very common, the most common pair of token, you just merge them. So here, you see three times the tokens T and O next to each other, so you're just going to say this is a new token. And then you continue. You repeat that. So now you have T-O-K, TOK, which happens three times, TOK with an E, that happens two times, and TOKEN, which happens twice, and then EX, which also happens twice. So this is that if you were to train a tokenizer on this corpus of text, which is very small, that's how you would finish with a trained tokenizer. In reality, you do it on much larger corpuses of text. And this is the real tokenizer of, actually, I think this is GPT-3 or ChatGPT. And here, you see how it would actually separate these words. So basically, you see the same thing as what we gave in the previous example, TOKEN becomes its own token. So tokenizer is actually split up into two tokens, TOKEN and EIZER. So yeah, that's all about tokenizers. Any question on that? Yeah? How do you deal with spaces, and how do you deal with introduction? Yeah. So actually, there's a step before tokenizers, which is what we call pre-tokenizers, which is exactly what you just said. So this is mostly, in theory, there's no reason to deal with spaces and punctuation separately. You could just say every space gets its own token, every punctuation gets its own token, and you could just do all the merging. The problem is that, so there's an efficiency question. Actually, training these tokenizers takes a long time. So you're better off, because you have to consider every pair of token. So what you end up doing is saying, if there's a space, this is very, like pre-tokenizers are very English-specific. So you say, if there's a space, we're not going to start looking at the token that came before and the token that came afterwards. So you're not merging in between spaces. But this is just like a computation optimization. You could theoretically just deal with it the same way as you deal with any other character. Yeah? When you merge tokens, do you delete the tokens that you merged away, or do you keep the smaller tokens that you merged? You actually keep the smaller tokens. I mean, in reality, it doesn't matter much, because usually on large corpus of text, you will have actually everything. But you usually keep the small ones. And the reason why you want to do that is because if, in case there's a, as we said before, you have some grammatical mistakes or some typos, you still want to be able to represent these words by character. So yeah. Yes? Are the tokens unique? So, I mean, say, in this case, T-O-K-E-N, is there only one occurrence, or do you need to leave multiple occurrence so they can take on different meanings or something? Oh, oh, I see what you say. No, no, no. It's every token has its own unique ID. So a usual, this is a great question. For example, if you think about a bank, which could be bank for money or bank like water, they will have the same token, but the model will learn, the transformer will learn that based on the words that are around it, it should associate that, I'm saying, I'm being very hand-wavy here, but associate that with a representation that is either more like the bank money side or the bank water side. But that's the transformer that does that. It's not a tokenizer. Yes? Yeah, so you mentioned during tokenization, you keep the smaller tokens to start with, right? So if you start with a T, you keep the T, and then you tell your tokenizer to expand that amount of token. So let's say maybe you didn't train on token, but in your data, you are trying to encode token. So how does the tokenizer know to encode it with token or to do it with T? Yeah, it's a great question. You basically, when you, so when you tokenize, so that's after training of the tokenizer, when you actually apply the tokenizer, you basically always choose the largest token that you can apply. So if you can do token, you will never do T. You will always do token. But it's actually, so people don't usually talk that much about tokenizers, but there's a lot of computational benefits or computational tricks that you can do for making these things faster. So I really don't think we, and honestly, I think a lot of people think that we should just get away from tokenizers and just kind of tokenize\"),\n",
       " Document(metadata={'chunk_id': 0.0, 'sources': \"＂okay, but I want Llama 3 for my specific use case＂ - Here's how.m4a\"}, page_content=\"My name is David Andrzej and in this video, I'll teach you how to fine-tune Lama3 so that it performs 10 times better for your specific use case. Let's start with what even is fine-tuning and I made this explanation in plain English so that anybody can understand. Fine-tuning is adapting a pre-trained LLM like GPT-4 or in this case Lama3 to a specific task or domain. It involves adjusting a small portion of the parameters on a more focused dataset. So, you know, when a new model releases, what everybody needs to know is how many parameters it has. We have Lama3 8B and always that number like 8B or 70B, that's the number of parameters. So we're adjusting just a small number of them to make it more focused on a specific thing. Fine-tuning customizes the outputs to be more relevant and accurate for your use case. Here's the power of fine-tuning. Cost-effectiveness. It leverages the power of pre-trained LLMs which cost tens of millions of dollars, if not hundreds of millions, to train and we can just, you know, run a GPU for a few hours and fine-tune something for, I don't know, like cents, a few cents or a few dollars at most, which is just amazing. It gives you improved performance because you can enhance the LLM on your dataset and improve accuracy for specific tasks. And it also is more data efficient. You can achieve excellent results even with smaller datasets. So, you know, maybe even like 300, 500 entries while, you know, Lama3 was trained on 15 trillion tokens. I don't know about you, but I don't have nearly as much data as Zack. So that's why fine-tuning is great for people like you and me. So how does LLM fine-tuning actually work? First, you need to prepare your dataset. And this, you know, depending on how hardcore you want to go, this can take anywhere from 20 minutes to a few hours to weeks, potentially. Depends how far you want to take it. So you create a smaller, high-quality dataset tailored to your specific use case and label it appropriately, which I'll teach you in a bit. The pre-trained LLMs weights are updated incrementally using the optimization algorithms like gradient descent based on the new dataset. So we can only fine-tune LLMs that we have access to the weights, meaning open-source, open-weights LLMs. You cannot fine-tune GPT-4 if you are not OpenAI. OpenAI can do it, obviously, but me and you, we probably don't have GPT-4 just laying on our computer. Then you monitor and refine. You evaluate the model's performance on a validation set, preventing overfitting and guide adjustments. Now, here are some real-world use cases for fine-tuning. Fine-tuning an LLM on customer service transcripts can create a chatbot, like this one, that can address issue in a way specific to your company. So let's say you have a specific product, very niche, that there is not much data about it on the internet. And if somebody messages your customer support email, you want your chatbot to respond in a specific way based on the information of your product. And that data is proprietary. It's private. Only you have it. And you can fine-tune an LLM to respond based on that data. So, like, technically, if you have enough scripts, you can fine-tune an LLM to respond like you. And, you know, if you try chat GPT, if you even give, like, chat GPT some writing and tell it, continue in this writing style, it's terrible. So this is where fine-tuning could be better. Tailored content generation. So you can fine-tune an LLM on your posts and descriptions to create engaging summaries or marketing copy, again, in your writing style, tailored to your audience. Domain-specific analysis. So fine-tuning LLM on legal or medical text can make it much better for those specific benchmarks. So you might have a model that, let's say, it reaches 50 on some arbitrary benchmark. With fine-tuning, it can reach 70 or 80. Now let's dive into how to actually implement this on Llama3. So I created this Google collab. Well, actually, most of it was created by Anslov team. A huge shout out to Anslov because they did all the heavy lifting. So I'm going to also link their GitHub below. Now, first off, I added a component that's only available in April to the community. So if you join during April, you will get a personalized AI strategy to future-proof yourself and your business. So if you want to be among people who are building the future, if you want access to all the different courses, modules, and everything else in the community, and to two weekly calls, then consider joining. And especially if you want me to give you a personalized AI strategy to future-proof yourself. So if that sounds interesting to you, make sure to join the community. It's the first link in the description. Now let's fine-tune LLama3, shall we? So first thing, we check the GPU version available in the environment and install specific dependencies that are comparable with the detected GPU to prevent conflicts. So this is this cell. By the way, if you don't know how Google Colab works, which is, you know, the software I'm using right now, it's super simple. It's basically splitting the code into cells. It's called the Jupyter Notebook, but it's like much more easier to see. You can add text, you can add graphics, and it's great for like tutorials and explaining, right? So if you never use this, it's great because it's free. And Google actually gives you a GPU so you can use this T4 GPU to train this model for free. And if you want faster, you can obviously upgrade it, right? So I'm going to link this Colab below the video as well. So we run this cell, which does what I just explained. The next cell, we need to prepare to load a range of quantized language models, including the new 15 trillion LLama3 model, so trained on 15 trillion tokens. And it's optimized for efficiency with 4-bit quantization. I mean, I'm not going to even pretend I know everything about fine-tuning because I don't. So if it seems like I have gaps in my knowledge, it's because it is. I do have those gaps in my knowledge. So I try to make it as simple as possible. But if this proves something, it proves that you don't have to be a machine learning expert to fine-tune models. So just follow along. So here, this is the max sequence length. Obviously, LLama3 is up to 8,000. So I mean, 2,000 is plenty for this demonstration, but you can do anything. You can do 4,000 or 8,000. Here, I use 4-bit quantization to reduce memory usage, but it can be false as well. So here are the models. We can see, like, we have Mistral7B, LLama2, which is the old one, Gemma from Google. But obviously, we're interested in LLama3 8B. And by the way, we can also use LLama3 70B if you want, which obviously will take longer because it's a much bigger model. So in that case, you might want to buy the premium version of Collab or just wait for a while. But yeah, I mean, everything is the same. Just here, you would change the model to LLama3 70B. And if you want to use, like, gated models from HuggingFace, which gated means that you have to usually agree to some, you know, license or whatever, then here, just remove the comment and then put your HuggingFace token here. Super simple. Now, by the way, you always have to run this. So what do you do when you go to Google Collab? You click on runtime and click run all. That way, all of the cells run. But you can also do it one by one by clicking this button right here next to each cell. And it needs to have this little tick, green tick. That way, it was executed. Here, it's not because I, you know, removed the... I changed this. So anytime you make any change, it disappears. But that doesn't matter. It was still executed. So it's stored in the runtime. Next up, we integrate LoRa. Again, you don't have to understand what this is, but it's basically a way of fine-tuning into our model, which allows us to efficiently update just a fraction of the parameters, enhancing training speed and reducing computation load. So again, we are not training the model from scratch. We're just fine-tuning a few parameters for our specific use case. And here, you can change the R to any number greater than 0, 8, 16, 32, 64, up to you. And your goals, what you want to do with it. By the way, on Sloth, the reason I'm using it is because it makes fine-tuning much faster and consuming less memory. So it's actually a great framework for this. Dataprep. We now use the Alpaca dataset from Yama, which is this one, which has 50,000 rows. And I have it loaded in VS code here. Just that way, you see how it looks like in JSON formatting. So, you know, it's a lot of lines because for everyone, it's basically times five. Yeah, so like 250,000 lines. And it's like every one of them has an instruction. I should probably zoom it in. So every one entry has an instruction. Give three tips for staying healthy. Input, this is not mandatory because instruction is already enough context. And then output, this is what the LLM should say. And you do this enough times and the LLM learns. It basically learns, right? So we can see it probably better here. And if you want to use your own dataset, you have to format it the same way. So, you know, just having output, input and instructions, these three parameters. But yeah, just look at this. Not all of them have the input, which is fine. I mean, probably like 20% or 15% have the input. And that's just extra context. So yeah, I'm also going to link this dataset below. But if you want your own dataset, which, you know, if you want your own use case, just make sure to format it the same way. So, you know, instruction, some text, input, some extra context or empty, and output, how the model should respond. And, you know, if you're getting creative, you can definitely use LLMs to generate these large datasets much faster. I mean, maybe you create really like 20 high quality examples by hand. And then you run a team of agents for creating that dataset that can just, you know, use those 20 examples to create 50,000. Like in this dataset. But yeah, that's a topic for a whole nother video. So if you want me to make a video on how to make datasets for fine tuning, then let me know. But let's go back to our Colab. So then we define a system prompt, which is, you know, custom instruction system prompt, which you already know, hopefully. That formats tasks into instruction inputs and responses. So this has to fit with our dataset. And we apply it to our dataset for the model. And we add the EOS token to signal completion. So this token right here, here we define it. And here we add it because without this, the token generation continues forever. So we don't want that, obviously. So let's look at the system prompt. It's very simple. It says below is an instruction that describes a task paired with an input that provides further context. Write a response that appropriately completes the request. And that's our system prompt. And then we feed it the instruction, the input and response. And obviously you can change the system prompt if you want. Now, train the model. We do a 60 step. We do only 60 steps here to speed things up. You can, like, this is obviously very small because it's not even one epoch, training epoch. So if you want to like actually use something for production or your business, you probably want to train it for longer than 60 steps. And I'm going to show you how in this bit. So if you do multiple epochs, you have to turn max steps none. So here, okay. Number of trained epochs is not included in here. So what you would do is you would copy this and you would go in here and look at the steps, right? So we have the steps here. You would add this. Maybe you would do four or whatever, however many you want. The more, the better. But at a certain point, it starts to not yield better result. So max steps, you have to change it to none, right? So it's like 60 right now. So you do none. And this is where you would do like proper fine tuning. But, you know, I just added that 60 for demonstration. That way it's faster. And it still took like eight minutes. So I'm not going to replicate it. I'm just going to show you everything. But yeah, basically, you know, this is what you do. You decide how many epochs you want. And then at this stage, we're configuring our model's training setup where we define things like batch size and learning rate to teach our model effectively with the data we've prepared. So obviously you can like mess with stuff here. Again, I'm not going to pretend I understand everything. But the main things are, you know, packing like this can make it five times faster for short sequences. Obviously, the steps and the epochs. But yeah, I mean, if you're confused something, just take a screenshot. Boom, like this. And ask ShedGPD. Now, this is the current memory stats, right? So we're using the Tesla T4 GPU provided from Google for free. And the max memory is 14 gigabytes. And this is where the training begins. This is the magical part, right? So here we do this line of code, trainer stats, trainer.train. And this will give us the statistics as the model trains. So again, this is only 60 steps, which is like zero epochs. But yeah, you can see the training loss going down. So like basically, smaller number is better here. So you can see like at the start, we have 1.8. Like 1.9, and then it quickly starts dropping to like 0.9, you know, around 1, 0.8. So it fluctuates a bit, but it consistently goes down 0.7. But you can see it's reaching like a asymptote, right? Obviously, it's only 60 steps. So it really doesn't mean anything. But yeah, like we ended up like 0.8 from like 2. So it shows you like if the model is actually improving. So it shows you like if the model is actually improving. And this took like eight minutes. So you can see the stats here, right? So 476 seconds, almost exactly eight minutes. Peak reserve memory was 8.9 gigabytes. And for training was 3.3 gigabytes. So not like this is the power of Unslof. It's like really optimized for this to use, to run faster and to use less memory. So that way we can fine tune GPUs for cheaper. I mean, you know, I'm using a free T4 GPU from Google. So it's free, but it's faster. Like if you didn't use Unslof, it would be a lot slower. So, okay, so 60% of, we used 60% of max memory. So that's good because we didn't like hit the limit. So we still have like 40% reserved. And for training, it was only 22%, which is even better. Inference, which means here we actually run our new model that we fine tuned. And okay, so this data set is for like instructions. And this is basically when you see a model that is like instruct at the end of it, this is what they mean. It's just trained on a large data set of instructions. Because usually the models are more for like chatting, for text generation. You know, you give it some input and it's like gives you some output. It's, you know, for more conversational. Here for instructions, for the instruct models is to follow instructions. You give it a task and it completes it. So like we can see it probably here in VS code, like rewrite the sentence to change its meaning and then output the thief escaped. Compared to data subs, so this is like all tasks. It's all in instructions. And then it shows how the model should do it. So let's look at it, right? So now we've trained the model. This took like eight minutes to do. So all of you can do this. The beauty of using a Google Cloud is that obviously it doesn't matter what machine you have. Even if you have a terrible computer, this will take the exact same time because you're using the GPU and cloud. So obviously here you can change your prompt. I mean, this is, you know, I changed the prompts here. So this is my prompt. But always make sure to leave the output blank. So here, the first one is the instruction. Then this is the input, like the extra added context and the output, leave it blank because the model will generate it, right? So list the prime numbers contained within\"),\n",
       " Document(metadata={'chunk_id': 0.0, 'sources': 'Large Language Models (LLMs) - Everything You NEED To Know.m4a'}, page_content=\"that. And again, it's all about the quality of your data. So if you have a really good data set that you're going to fine-tune a model on, the model is going to be really, really good. And conversely, if you have a poor quality data set, it's not going to perform as well. All right, let me pause for a second and talk about AI Camp. So as mentioned earlier, this video, all of its content, the animations, have been created in collaboration with students from AI Camp. AI Camp is a learning experience for students that are age 13 and above. You work in small, personalized groups with experienced mentors. You work together to create an AI product using NLP, computer vision, and data science. AI Camp has both a three-week and a one-week program during summer that requires zero programming experience. And they also have a new program which is 10 weeks long during the school year, which is less intensive than the one-week and three-week programs for those students who are really busy. AI Camp's mission is to provide students with deep knowledge in artificial intelligence, which will position them to be ready for AI in the real world. I'll link an article from USA Today in the description all about AI Camp. But if you're a student or if you're a parent of a student within this age, I would highly recommend checking out AI Camp. Go to ai-camp.org to learn more. Now let's talk about limitations and challenges of large language models. As capable as LLMs are, they still have a lot of limitations. Recent models continue to get better, but they are still flawed. They're incredibly valuable and knowledgeable in certain ways, but they're also deeply flawed in others, like math and logic and reasoning. They still struggle a lot of the time versus humans, which understand concepts like that pretty easily. Also, bias and safety continue to be a big problem. Large language models are trained on data created by humans, which is naturally flawed. Humans have opinions on everything, and those opinions trickle down into these models. These data sets may include harmful or biased information, and some companies take their models a step further and provide a level of censorship to those models. And that's an entire discussion in itself whether censorship is worthwhile or not. I know a lot of you already know my opinions on this from my previous videos. And another big limitation of LLMs historically has been that they only have knowledge up until the point where their training occurred. But that is starting to be solved with ChatGPT being able to browse the web, for example. Grok from x.ai being able to access live tweets. But there's still a lot of kinks to be worked out with this. Also, another big challenge for LLMs\"),\n",
       " Document(metadata={'chunk_id': 0.0, 'sources': 'A Practical Introduction to Large Language Models (LLMs).m4a'}, page_content=\"Hey everyone, I'm Shaw and I'm back with a new data science series. In this new series, I'm going to be talking about large language models and how to use them in practice. In this video, I will give a beginner-friendly introduction to large language models and describe three levels of working with them in practice. Future videos in this series will discuss various practical aspects of large language models, things like using OpenAI's Python API, using open-source solutions like the Hugging Phase Transformers library, how to fine-tune large language models, and of course, how to build a large language model from scratch. If you enjoyed this content, please be sure to like, subscribe, and share with others. And if you have any suggestions for me to include in this series, please share those in the comments section below. And so with that, let's get into the video. So to kick off the video series, in this video, I'm going to be giving a practical introduction to large language models. And this is meant to be very beginner-friendly and high level, and I'll leave more technical details and example code for future videos and blogs in this series. So a natural place to start is, what is a large language model, or LLM for short? So I'm sure most people are familiar with ChatGPT, however, if you are enlightened enough to not keep up with new cycles and tech hype and all this kind of stuff, ChatGPT is essentially a very impressive and advanced chatbot. So if you go to the ChatGPT website, you can ask it questions like, what's a large language model? And it will generate a response very quickly, like the one that we are seeing here. And that is really impressive. Like if you were ever on AOL, Instant Messenger, also called AIM, you know, back in early 2000s or in the early days of the internet, there were chatbots then, there have been chatbots for a long time, but this one feels different. Like the text is very impressive and it almost feels human-like. A question you might have when you hear the term large language model is, what makes it large? What's the difference between a large language model and a not large language model? And this was exactly the question I had when I first heard the term. And so one way we can put it is that large language models are a special type of language model. But what makes them so special? And I'm sure there's a lot that can be said about large language models. But to keep things simple, I'm going to talk about two distinguishing properties. The first quantitative and the second qualitative. So first, quantitatively, large language models are large. They have many, many more model parameters than past language models. And so these days, this is anywhere from tens to hundreds of billions of parameters. The model parameters are numbers that define how the model will take an input and generate the output. So it's essentially the numbers that define the model itself. Okay, so that's a quantitative perspective of what distinguishes large language models from not large language models. But there's also this qualitative perspective and these so-called emergent properties that start to show up when language models become large. And so emergent properties is the language used in this paper cited below, a survey of large language models available in the archive. Really great beginner's guide, I recommend it. But essentially what this term means is there are properties in large language models that do not appear in smaller language models. And so one example of this is zero-shot learning. One definition of zero-shot learning is the capability of a machine learning model to complete a task it was not explicitly trained to do. So while this may not sound super impressive to us very smart and sophisticated humans, this is actually a major innovation in how these state-of-the-art machine learning models are developed. So to see this, we can compare the old state-of-the-art paradigm to this new state-of-the-art paradigm. The old way, and not too long ago, we can say like about 5, 10 years ago, the way the high-performing best machine learning models were developed was strictly through supervised learning. What this would typically look like is you would train a model on thousands, if not millions, of labeled examples. And so what this might have looked like is you have some input text like, hello, hola, how's it going, esta bien, so on and so forth. And you take all these examples and you manually assign a label to each example. Here we're labeling the language, so English, Spanish, so on. And so you can imagine that this would take a tremendous amount of human effort to get thousands, if not millions, of high-quality examples. So let's compare this to the more recent innovation with large language models who use a different paradigm. They use so-called self-supervised learning. What that looks like in the context of large language models is you train a very large model on a very large corpus of data. And so what this can look like is if you're trying to build a model that can do language classification, instead of painstakingly generating this labeled data set, you can just take a corpus of English text and a corpus of Spanish text and train a model in a self-supervised way. So in contrast to supervised learning, self-supervised learning does not require manual labeling of each example in your data set. The so-called labels or targets for the model are actually defined from the inherent structure of the data or this context of the text. So you might be thinking to yourself, how does this self-supervised learning actually work? And so one of the most popular ways that this is done is the next word prediction paradigm. So suppose we have this text, listen to your, and we want to predict what the next word would be. But clearly there's not just one word that can go after this string of words. There are actually many words you can put after this text and it would make sense. In this next word prediction paradigm, what the language model is trying to do is to predict the probability distribution of the next word given the previous words. What this might look like is listen to your heart might be the most probable next word, but another likely word could be gut or listen to your body or listen to your parents and listen to your grandma. And so this is essentially the core task that these large language models are trained to do. And the way the large language model will learn these probabilities is that it'll see so many examples in this massive corpus of text that it's trained on and it has a massive number of internal parameters so it can efficiently represent all the different statistical associations with different words. And an important point here is that context matters. If we simply added the word don't to the front of this string here and it changed it to don't listen to your, then this probability distribution could look entirely different because just by adding one word before this sentence, we completely change the meaning of the sentence. And so to put this a bit more mathematically, and I promise this is the most technical thing in this video, this is an example of a auto regression task. So auto meaning self, regression meaning you're trying to predict something. So what this notation means is what is the probability of the nth text or more technically the nth token given the preceding m token. So n minus one, n minus two, n minus three, so on and so forth. And so if you really want to boil everything down, this is the core task most large language models are doing. And somehow through this very simple task of predict the next word, we get this incredible performance from tools like chat GPT and other large language models. So now with that foundation set, hopefully you have a decent understanding of what large language models are and how they work and a broader context for them. Now let's talk about how we can use these in practice. Here I will talk about three levels in which we can use large language models. These three levels are ordered by the technical expertise and computational resources required. The most accessible way to use large language models is prompt engineering. Next we have model fine tuning. And then finally we have build your own large language model. So starting from level one, prompt engineering here, I have a pretty broad definition of prompt engineering. Here I define it as just using an LLM out of the box. So more specifically, not touching any of the model parameters. So of these tens of billions or hundreds of billions of parameters that define the model, we're not going to touch any of them. We're just going to leave them as is. Here I'll talk about two ways we can do this. One is the easy way and I'm sure is the way that most people in the world have interacted with large language models, which is using things like chat GPT. These are like intuitive user interfaces. They don't require any code and they're completely free. Someone can just go to the chat GPT website, type in a prompt and it'll spit out a response. So while this is definitely the easiest way to do it, it is a bit restrictive in that you have to go to their website. This doesn't really scale well if you're trying to build a product or service around it. But for a lot of use cases, this is actually super helpful. So for applications where the easy way doesn't cut it, there is the less easy way, which is using things like the open AI API or the hugging face transformers library. And these tools provide ways to interact with large language models programmatically. So essentially using Python. In the case of the open AI API, instead of typing your request in the chat GPT user interface, you can send it over to open AI using Python and their API and then you will get a response back. Of course, their API is not free, so you have to pay per API call. Another way we can do this is via open source solutions, one of which is the hugging face transformers library, which gives you easy access to open source large language models. So it's free and you can run these models locally. So no need to send your potentially proprietary or confidential information to a third party in open AI. So future videos of the series, we'll dive into all these different aspects. I'll talk about the open AI API, what it is, how it works, share example code. I'll dive into the hugging face transformers library, same situation. What the heck is it? How does it work? And then sharing some Python example code there. I'll also do a video talking about prompt engineering more generally. How can we create prompts to get good responses from large language models? And so while prompt engineering is the most accessible way to work with large language models, just working with a model out of the box may give you suboptimal performance on a specific task or use case. Or the model has really good performance, but it's massive. It has like a hundred billion parameters. So a question might be, is there a way we can use a smaller model, but kind of tweak it in a way to have good performance on our very narrow and specific use case. And so this brings us to level two, which is model fine tuning, which here I define as adjusting at least one internal model parameter for a particular task. And so here there are just generally two steps. One, you get a pre-trained large language model, maybe from open AI, or maybe an open source model from the hugging face transformers library. And then you update the model parameters given task specific examples. Kind of going back to the supervised learning versus self-supervised learning, the pre-trained model is going to be a self-supervised model. So it'll be trained on this simple word prediction task. But in step two, here's where we're going to do supervised learning or even reinforcement learning to tweak the model parameters for a specific use case. And so this turns out to work very well. Examples like ChatGPT, you're not working with the raw pre-trained model. The model that you are interacting with in ChatGPT is actually a fine-tuned model developed using reinforcement learning. And so a reason why this might work is that in doing this self-supervised task and doing the word prediction, the base model, this pre-trained large language model is learning useful representations for a wide variety of tasks. So in a future video, I will dive in more deeply into fine-tuning techniques. Another one is low rank adaptation or LORA for short. And then another popular one is reinforcement learning with human feedback or RLHF. And of course, there is a third step here. You'll deploy your fine-tuned large language model to do some kind of service or, you know, use it in your day-to-day life and you'll profit somehow. And so my sense is between prompt engineering and model fine-tuning, you can probably handle 99% of large language model use cases and applications. However, if you're a large organization, large enterprise, and security is a big concern. So you don't want to use open source models or you don't want to send data to a third party via an API. And maybe you want your large language model to be very good at a relatively specific set of tasks. You want to customize the training data in a very specific way and you want to own all the rights, have it for commercial use, all this kind of stuff. Then it can make sense to go one step further beyond model fine-tuning and build your own large language model. And so here I define it as just coming up with all the model parameters. So I'll just talk about how to do this at a very high level here, and I'll leave technical details for a future video in the series. First, we need to get our data. And so what this might look like is you'll get a book corpus, a Wikipedia corpus, and a Python corpus. And so this is billions of tokens of text. And then you will take that and pre-process it, refine it into your training data set. And then you can take the training data set and do the model training through self-supervised learning. And then out of that comes the pre-trained large language model. So you can take this as your starting point for level two and go from there. And so if you enjoyed this video and you want to read more, be sure to check out the blog in Towards Data Science. There I share some more details that I may have missed in this video. This series is both a video and blog series. So each video will have an associated blog, and there will also be tons of example code on the GitHub repository. The goal of the series is to really just make information about large language models much more accessible. I really do think this is the technological innovation of our time, and there are so many opportunities for potential use cases, applications, products, services that can come out of large language models. And that's something that I want to support. I think we'll be better off if more people understand this technology and are applying it to solving problems. So with that, be sure to hit the subscribe button to keep up with future videos in this series. If you have any questions or suggestions for other topics I should cover in this series, please drop those in the comment section below. And as always, thank you so much for your time and thanks for watching.\")]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Tell me everything I need to know about LLMs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closing Weaviate connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
