{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmented Research Assistant ðŸ¤– ðŸš€\n",
    "\n",
    "You've built a great assistant that uses RAG to provide answers. Now let's combined what we've learned and try to implement memory inside that bot ðŸ§ \n",
    "\n",
    "<Note type=\"note\">\n",
    "\n",
    "In this exercise, you will probably learn a few more advanced concepts especially regarding chaining. Read carefully and be patient if you want to finish this exercise ðŸ˜Œ\n",
    "\n",
    "</Note>\n",
    "\n",
    "\n",
    "## Step 0 - Demo setup \n",
    "\n",
    "First remember to have:\n",
    "\n",
    "* You containers running \n",
    "* Packages below installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install package\n",
    "%pip install -Uqq langchain-weaviate\n",
    "%pip install langchain langchain_mistralai langchain_huggingface -q\n",
    "%pip install -qU langchain-community beautifulsoup4\n",
    "%pip install -qU weaviate-client\n",
    "%pip install sentence-transformers -q \n",
    "%pip install transformers -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step I - Connect to your database \n",
    "\n",
    "Now let's connect your database. \n",
    "\n",
    "<Note type=\"important\">\n",
    "\n",
    "If for some reason you deleted your Weaviate container, you will need to recreate one and repopulate your DB as everything would have been lost.\n",
    "\n",
    "Check out the previous exercises if you need to do so. \n",
    "\n",
    "</Note>\n",
    "\n",
    "1. Connect to your database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "import os\n",
    "\n",
    "weaviate_url = os.environ[\"WEAVIATE_URL\"]\n",
    "weaviate_api_key = os.environ[\"WEAVIATE_API_KEY\"]\n",
    "\n",
    "# Connect to Weaviate Cloud\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=weaviate_url,\n",
    "    auth_credentials=Auth.api_key(weaviate_api_key),\n",
    ")\n",
    "\n",
    "# client = weaviate.connect_to_local(\n",
    "#     # host=\"host.docker.internal\",  # Use host.docker.internal if you are running it inside a docker container\n",
    "#     port=8080,\n",
    "#     grpc_port=50051,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Start by displaying the available collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LangChain_5bc7e27ecd0747218db36fbf82ce55b8': _CollectionConfigSimple(name='LangChain_5bc7e27ecd0747218db36fbf82ce55b8', description=None, generative_config=None, properties=[_Property(name='text', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=None, vectorizer='none', vectorizer_configs=None), _Property(name='sources', description=\"This property was generated by Weaviate's auto-schema feature on Fri May 16 13:54:08 2025\", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=None, vectorizer='none', vectorizer_configs=None), _Property(name='chunk_id', description=\"This property was generated by Weaviate's auto-schema feature on Fri May 16 13:54:08 2025\", data_type=<DataType.NUMBER: 'number'>, index_filterable=True, index_range_filters=False, index_searchable=False, nested_properties=None, tokenization=None, vectorizer_config=None, vectorizer='none', vectorizer_configs=None)], references=[], reranker_config=None, vectorizer_config=None, vectorizer=<Vectorizers.NONE: 'none'>, vector_config=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.collections.list_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define a vector store using an embedding and the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/qrbwlc0x6fj2rs4chrxkrm_m0000gn/T/ipykernel_23730/1841365288.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings()\n",
      "/var/folders/yt/qrbwlc0x6fj2rs4chrxkrm_m0000gn/T/ipykernel_23730/1841365288.py:2: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "# Instanciate Embeddings\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "# Now we can load our documents into our Database \n",
    "# Depending on the amount of data \n",
    "# The time necessary to execute the cell will vary\n",
    "vectorstore = WeaviateVectorStore.from_documents(\n",
    "    [],\n",
    "    client= client,\n",
    "    embedding=embeddings,\n",
    "    index_name=\"LangChain_5bc7e27ecd0747218db36fbf82ce55b8\", # To know where to get that, you need to look for client.collections.list_all()\n",
    "    use_multi_tenancy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step II - Load a model & Create a retriever\n",
    "\n",
    "Now let's:\n",
    "* Import a model and\n",
    "* create a `retriever` variable based on `vectorestore`\n",
    "\n",
    "Once this is done, test your `retriever` by doing a simple search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'chunk_id': 0.0, 'sources': 'Large Language Models (LLMs) - Everything You NEED To Know.m4a'}, page_content=\"This video is going to give you everything you need to go from knowing absolutely nothing about artificial intelligence and large language models to having a solid foundation of how these revolutionary technologies work. Over the past year, artificial intelligence has completely changed the world, with products like ChatGPT potentially appending every single industry and how people interact with technology in general. And in this video, I will be focusing on LLMs, how they work, ethical considerations, applications, and so much more. And this video was created in collaboration with an incredible program called AI Camp, in which high school students learn all about artificial intelligence. And I'll talk more about that later in the video. Let's go. So first, what is an LLM? Is it different from AI? And how is ChatGPT related to all of this? LLMs stand for large language models, which is a type of neural network that's trained on massive amounts of text data. It's generally trained on data that can be found online. Everything from web scraping to books to transcripts, anything that is text-based can be trained into a large language model. And taking a step back, what is a neural network? A neural network is essentially a series of algorithms that try to recognize patterns in data. And really what they're trying to do is simulate how the human brain works. And LLMs are a specific type of neural network that focus on understanding natural language. And as mentioned, LLMs learn by reading tons of books, articles, internet text, and there's really no limitation there. And so how do LLMs differ from traditional programming? Well, with traditional programming, it's instruction-based, which means if X, then Y. You're explicitly telling the computer what to do. You're giving it a set of instructions to execute. But with LLMs, it's a completely different story. You're teaching the computer not how to do things, but how to learn how to do things. And this is a much more flexible approach and is really good for a lot of different applications where previously traditional coding could not accomplish them. So one example application is image recognition. With image recognition, traditional programming would require you to hard code every single rule for how to, let's say, identify different letters. So A, B, C, D. But if you're handwriting these letters, everybody's handwritten letters look different. So how do you use traditional programming to identify every single possible variation? Well, that's where this AI approach comes in. Instead of giving a computer explicit instructions for how to identify a handwritten letter, you instead give it a bunch of examples of what handwritten letters look like, and then it can infer what a new handwritten letter looks like based on all of the examples that it has. What also sets machine learning and large language models apart in this new approach to programming is that they are much more flexible, much more adaptable, meaning they can learn from their mistakes and inaccuracies, and are thus so much more scalable than traditional programming. LLMs are incredibly powerful at a wide range of tasks, including summarization, text generation, creative writing, question and answer, programming, and if you've watched any of my videos, you know how powerful these large language models can be. And they're only getting better. Know that right now, large language models and AI in general are the worst they'll ever be. And as we're generating more data on the internet, and as we use synthetic data, which means data created by other large language models, these models are going to get better rapidly. And it's super exciting to think about what the future holds. Now let's talk a little bit about the history and evolution of large language models. We're going to cover just a few of the large language models today in this section. The history of LLMs traces all the way back to the ELISA model, which was from 1966, which was really the first language model. It had pre-programmed answers based on keywords. It had a very limited understanding of the English language. And like many early language models, you started to see holes in its logic after a few back and forths in a conversation. And then after that, language models really didn't evolve for a very long time. Although technically the first recurrent neural network was created in 1924 or RNN, they weren't really able to learn until 1972. And these new learning language models are a series of neural networks with layers and weights and a whole bunch of stuff that I'm not going to get into in this video. And RNNs were really the first technology that was able to predict the next word in a sentence rather than having everything pre-programmed for it. And that was really the basis for how current large language models work. And even after this and the advent of deep learning in the early 2000s, the field of AI evolved very slowly, with language models far behind what we see today. This all changed in 2017, where the Google DeepMind team released a research paper about a new technology called Transformers. And this paper was called Attention is All You Need. And a quick side note, I don't think Google even knew quite what they had published at that time. But that same paper is what led OpenAI to develop ChatGPT. So obviously other computer scientists saw the potential for the Transformers architecture. With this new Transformers architecture, it was far more advanced. It required decreased training time, and it had many other features like self-attention, which I'll cover later in this video. Transformers allowed for pre-trained large language models like GPT-1, which was developed by OpenAI in 2018. It had 117 million parameters, and it was completely revolutionary, but soon to be outclassed by other LLMs. Then after that, BERT was released, B-E-R-T, in 2018. That had 340 million parameters, and had bidirectionality, which means it had the ability to process text in both directions, which helped it have a better understanding of context. And as comparison, a unidirectional model only has an understanding of the words that came before the target text. And after this, LLMs didn't develop a lot of new technology, but they did increase greatly in scale. GPT-2 was released in early 2019, and had 2.5 billion parameters. Then GPT-3 in June of 2020, with 175 billion parameters. And it was at this point that the public started noticing large language models. GPT had a much better understanding of natural language than any of its predecessors. And this is the type of model that powers ChatGPT, which is probably the model that you're most familiar with. And ChatGPT became so popular because it was so much more accurate than anything anyone had ever seen before. And it was really because of its size. And because it was now built into this chatbot format, anybody could jump in and really understand how to interact with this model. ChatGPT 3.5 came out in December of 2022, and started this current wave of AI that we see today. Then in March 2023, GPT-4 was released, and it was incredible, and still is incredible to this day. It had a whopping reported 1.76 trillion parameters, and uses likely a mixture of experts approach, which means it has multiple models that are all fine-tuned for specific use cases. And then when somebody asks a question to it, it chooses which of those models to use. And then they added multi-modality and a bunch of other features. And that brings us to where we are today. All right, now let's talk about how LLMs actually work in a little bit more detail. The process of how large language models work can be split into three steps. The first of these steps is called tokenization. And there are neural networks that are trained to split long text into individual tokens. And a token is essentially about three fourths of a word. So if it's a shorter word like hi, or that, or there, it's probably just one token. But if you have a longer word like summarization, it's going to be split into multiple pieces. And the way that tokenization happens is actually different for every model. Some of them separate prefixes and suffixes. Let's look at an example. What is the tallest building? So what is the tallest building? Are all separate tokens. And so that separates the suffix off of tallest, but not building because it is taking the context into account. And this step is done so models can understand each word individually, just like humans. We understand each word individually and as groupings of words. And then the second step of LLMs is something called embeddings. The large language models turns those tokens into embedding vectors, turning those tokens into essentially a bunch of numerical representations of those tokens numbers. And this makes it significantly easier for the computer to read and understand each word and how the different words relate to each other. And these numbers all correspond with the position in an embeddings vector database. And then the final step in the process is transformers, which we'll get to in a little bit. But first, let's talk about vector databases. And I'm going to use the terms word and token interchangeably. So just keep that in mind because they're almost the same thing, not quite, but almost. And so these word embeddings that I've been talking about are placed into something called a vector database. These databases are storage and retrieval mechanisms that are highly optimized for vectors. And again, those are just numbers, long series of numbers. Because they're converted into these vectors, they can easily see which words are related to other words based on how similar they are, how close they are based on their embeddings. And that is how the large language model is able to predict the next word based on the previous words. Vector databases capture the relationship between data as vectors in multi-dimensional space. I know that sounds complicated, but it's really just a lot of numbers. Vectors are objects with a magnitude and a direction, which both influence how similar one vector is to another. And that is how LLMs represent words based on those numbers. Each word gets turned into a vector, capturing semantic meaning and its relationship to other words. So here's an example. The words book and worm, which independently might not look like they're related to each other, but they are related concepts because they frequently appear together. A bookworm, somebody who likes to read a lot. And because of that, they will have embeddings that look close to each other. And so models build up an understanding of natural language using these embeddings and looking for similarity of different words, terms, groupings of words, and all of these nuanced relationships. And the vector format helps models understand natural language better than other formats. And you can kind of think of all this like a map. If you have a map with two landmarks that are close to each other, they're likely going to have very similar coordinates. So it's kind of like that. Okay, now let's talk about transformers. Matrix representations can be made out of those vectors that we were just talking about. This is done by extracting some information out of the numbers and placing all of the information into a matrix through an algorithm called multi-head attention. The output of the multi-head attention algorithm is a set of numbers which tells the model how much the words and its order are contributing to the sentence as a whole. We transform the input matrix into an output matrix which will then correspond with a word having the same values as that output matrix. So basically we're taking that input matrix, converting it into an output matrix, and then converting it into natural language. And the word is the final output of this whole process. This transformation is done by the algorithm that was created during the training process. So the model's understanding of how to do this transformation is based on all of its knowledge that it was trained with, all of that text data from the internet, from books, from articles, etc. And it learned which sequences of words go together and their corresponding next words based on the weights determined during training. Transformers use an attention mechanism to understand the context of words within a sentence. It involves calculations with the dot product, which is essentially a number representing how much the word contributed to the sentence. It will find the difference between the dot products of words and give it correspondingly large values for attention. And it will take that word into account more if it has higher attention. Now let's talk about how large language models actually get trained. The first step of training a large language model is collecting the data. You need a lot of data. When I say billions of parameters, that is just a measure of how much data is actually going into training these models. And you need to find a really good data set. If you have really bad data going into a model, then you're going to have a really bad model. Garbage in, garbage out. So if a data set is incomplete or biased, the large language model will be also. And data sets are huge. We're talking about massive, massive amounts of data. They take data in from web pages, from books, from conversations, from reddit posts, from x posts, from youtube transcriptions. Basically anywhere where we can get some text data, that data is becoming so valuable. Let me put into context how massive the data sets we're talking about really are. So here's a little bit of text which is 276 tokens. That's it. Now if we zoom out, that one pixel is that many tokens. And now here's a representation of 285 million tokens, which is 0.02% of the 1.3 trillion tokens that some large language models take to train. And there's an entire science behind data pre-processing, which prepares the data to be used to train a model. Everything from looking at the data quality, to labeling consistency, data cleaning, data transformation, and data reduction. But I'm not going to go too deep into that. And this pre-processing can take a long time. And it depends on the type of machine being used, how much processing power you have, the size of the data set, the number of pre-processing steps, and a whole bunch of other factors that make it really difficult to know exactly how long pre-processing is going to take. But one thing that we know takes a long time is the actual training. Companies like NVIDIA are building hardware specifically tailored for the math behind large language models. And this hardware is constantly getting better. The software used to process these models are getting better also. And so the total time to process models is decreasing, but the size of the models is increasing. And to train these models, it is extremely expensive because you need a lot of processing power, electricity, and these chips are not cheap. And that is why NVIDIA stock price has skyrocketed. Their revenue growth has been extraordinary. And so with the process of training, we take this pre-processed text data that we talked about earlier, and it's fed into the model. And then using transformers, or whatever technology a model is actually based on, but most likely transformers, it will try to predict the next word based on the context of that data. And it's going to adjust the weights of the model to get the best possible output. And this process repeats millions and millions of times over and over again until we reach some optimal quality. And then the final step is evaluation. A small amount of the data is set aside for evaluation. And the model is tested on this data set for performance. And then the model is adjusted if necessary. The metric used to determine the effectiveness of the model is called perplexity. It will compare two words based on their similarity. And it will give a good score if the words are related and a bad score if it's not. And then we also use RLHF, reinforcement learning through human feedback. And that's when users or testers actually test the model and provide positive or negative scores based on the output. And then once again, the model is adjusted as necessary. All right, let's talk about fine-tuning now, which I think a lot of you are going to be interested in because it's something that the average person can get into quite easily. So we have these popular large language models that are trained on massive sets of data to build general language capabilities. And these pre-trained models, like BERT, like GPT, give developers a head start versus training models from scratch. But then in comes fine-tuning, which allows us to take these raw models, these foundation models, and fine-tune them for our specific use cases. So let's think about an example. Let's say you want to fine-tune a model to be able to take pizza orders, to be able to have conversations, answer questions about pizza, and finally be able to allow customers to buy pizza. You can take a pre-existing set of conversations that exemplify the back and forth between a pizza shop and a customer, load that in, fine-tune a model, and then all of a sudden that model is going to be much better at having conversations about pizza ordering. The model updates the weights to be better at understanding certain pizza terminology, questions, responses, tone, everything. And fine-tuning is much faster than a full training, and it produces much higher accuracy. And fine-tuning allows pre-trained models to be fine-tuned for real-world use cases. And finally, you can take a single foundational model and fine-tune it any number of times for any number of use cases. And there are a lot of great services out there that allow you to do that. And again, it's all about the quality of your data. So if you have a really good data set that you're going to fine-tune a model on, the model is going to be really, really good. And conversely, if you have a poor quality data set, it's not going to perform as well. All right, let me pause for a second and talk about AI Camp. So as mentioned earlier, this video, all of its content, the animations, have been created in collaboration with students from AI Camp. AI Camp is a learning experience for students that are age 13 and above. You work in small, personalized groups with experienced mentors. You work together to create an AI product using NLP, computer vision, and data science. AI Camp has both a three-week and a one-week program during summer that requires zero programming experience. And they also have a new program which is 10 weeks long during the school\"),\n",
       " Document(metadata={'chunk_id': 0.0, 'sources': 'Stanford CS229 I Machine Learning I Building Large Language Models (LLMs).m4a'}, page_content=\"So, let's get started, so I'll be talking about building LLMs today, so I think a lot of you have heard of LLMs before, but just as a quick recap, LLMs, standing for Large Language Models, are basically all the chatbots that you've been hearing about recently, so ChatGPT from OpenAI, Claude from Entropiq, Gemini, and Lama, and other type of models like this, and today we'll be talking about how do they actually work, so it's going to be an overview because it's only one lecture, and it's hard to compress everything, but hopefully I'll touch a little bit about all the components that are needed to train some of these LLMs. Also, if you have questions, please interrupt me and ask. If you have a question, most likely other people in the room or on Zoom have the same question, so please ask. Great, so what matters when training LLMs? So, there are a few key components that matter. One is the architecture, so as you probably all know, LLMs are neural networks, and when you think about neural networks, you have to think about what architecture you're using. Another component which is really important is the training loss and the training algorithm, so how you actually train these models. Then it's data, so what do you train these models on? The evaluation, which is how do you know whether you're actually making progress towards the goal of LLMs, and then the system component, so that is like how do you actually make these models run on modern hardware, which is really important because these models are really large, so now more than ever, systems are actually really an important topic for LLMs. So, those five components, you probably all know that LLMs, and if you don't know, LLMs are all based on transformers, or at least some version of transformers. I'm actually not going to talk about the architecture today, one, because I gave a lecture on transformers a few weeks ago, and two, because you can find so much information online on transformers, but I think you can- there's much less information about the other four topics, so I really want to talk about those. Another thing to say is that most of academia actually focuses on architecture and training algorithm and losses. As academics, and I've done that for a lot- a big part of my career, is simply we like thinking that this is like we make new architectures, new models, and it seems like it's very important, but in reality, honestly, what matters in practice is mostly the three other topics, so data, evaluation, and systems, which is what most of industry actually focuses on. So that's also one of the reasons why I don't want to talk too much about the architecture, because really the rest is super important. Great, so overview of the lecture, I'll be talking about pre-training. So pre-training, you probably heard that word, this is the general word, this is kind of the classical language modeling paradigm, where you basically train your language model to essentially model all of internet. And then there's a post-training, which is a more recent paradigm, which is taking these large language models and making them essentially AI assistants. So this is more of a recent trend since ChatGPT. So if you ever heard of GPT-3 or GPT-2, that's really pre-training land. If you heard of ChatGPT, which you probably have, this is really post-training land. So I'll be talking about both, but I'll start with pre-training. And specifically, I'll talk about what is the task of pre-training LLMs and what is the laws that people actually use. So language modeling, this is a quick recap. Language models at a high level are simply models of probability distribution over sequences of tokens or of words. So it's basically some model of p of x1 to xl, where x1 is basically word 1 and xl is the last word in the sequence or in the sentence. So very concretely, if you have a sentence like the mouse ate the cheese, what the language model gives you is simply a probability of this sentence being uttered by a human or being found online. So if you have another sentence like the mouse ate cheese, here there's grammatical mistakes. So the model should know that this should have some syntactic knowledge. So it should know that this has less likelihood of appearing online. If you have another sentence like the cheese ate the mouse, then the model should hopefully know about the fact that usually cheese don't eat mouse. So there's some semantic knowledge and this is less likely than the first sentence. So this is basically at a high level what language models are. One word that you probably have been hearing a lot in the news are generative models. So this is just something that can generate models that can generate sentences or can generate some data. The reason why we say language models are generative models is that once you have a model of a distribution, you can simply sample from this model and then we can generate data. So you can generate sentences using a language model. So the type of models that people are all currently using are what we call autoregressive language models. And the key idea of autoregressive language models is that you take this distribution over words and you basically decompose it into the distribution of the first word, multiply it by the distribution of the likelihood of the distribution of the second word given the first word, and multiply it by p of the third word given the first two words. So there's no approximation here. This is just the chain rule of probability, which hopefully you all know about, really no approximation. This is just one way of modeling a distribution. So slightly more concisely, you can write it as a product of p's of the next word given everything which happened in the past, so of the context. So this is what we call autoregressive language models. Again, this is really not the only way of modeling distribution, this is just one way. It has some benefits and some downsides. One downside of autoregressive language models is that when you actually sample from this autoregressive language model, you basically have a for loop which generates the next word, then conditions on that next word, and then regenerate the other word. So basically, if you have a longer sentence that you want to generate, it takes more time to generate it. So there are some downsides of this current paradigm, but that's what we currently have, so I'm going to talk about this one. Great. So autoregressive language models. At a high level, what the task of autoregressive language model is, is simply predicting the next word, as I just said. So if you have a sentence like she likely prefers, one potential next word might be dogs. And the way we do it is that we first tokenize. So you take these words or sub words, you tokenize them, and then you give an ID for each token. So here I have one, two, three. Then you pass it through this black box. As I already said, we're not going to talk about the architecture. You just pass it through a model, and you then get a distribution, a probability distribution over the next word, over the next token. And then you sample from this distribution, you get a new token, and then you de-tokenize. So you get a new ID, you de-tokenize, and that's how you basically sample from a language model. One thing which is important to note is that the last two steps are actually only needed during inference. When you do training, you just need to predict the most likely token, and you can just compare to the real token, which happened in practice, and then you basically change the weights of your model to increase the probability of generating that token. Great. So autoregressive neural language models. So to be slightly more specific, still without talking about the architecture, the first thing we do is that we have all of these- oh, sorry, yes? On the previous slide, when you're predicting the probability of the next token, does this mean that your final output vector has to be the same dimensionality as the number of tokens that you have? Yes. How do you deal with if you're adding more tokens to your co-presenters? Yeah. So we're going to talk about tokenization actually later, so you will get some sense of this. You basically can't deal with adding new tokens. I'm kind of exaggerating. There are methods for doing it, but essentially people don't do it. So it's really important to think about how you tokenize your text, and that's why we'll talk about that later. But it's a very good point to note is that basically the vocabulary size, so the number of tokens that you have, is essentially the output of your language model. So it's actually pretty large. Okay, so autoregressive neural language models. First thing you do is that you take every word or every token. You embed them, so you get some vector representation for each of these tokens. You pass them through some neural network, as we said, it's a transformer. Then you get a representation for all the words in the context. So it's basically a representation of the entire sentence. You pass it through a linear layer, as you just said, to basically map it to the number so that the output, the number of outputs is the number of tokens. You then pass it through some softmax, and you basically get a probability distribution over the next words given every word in the context. And the laws that you use is basically, it's essentially a task of classifying the next token. So it's a very simple kind of machine learning task. So you use the cross-entropy laws, where you basically look at the actual target that happened, which is a target distribution, which is a one-hot encoding, which here in this case says, I saw the real word that happened is cat. So that's a one-hot distribution over cat. And here, this is the distribution that you generated. And basically, you do cross-entropy, which really just increases the probability of generating cat and decreases the probability of generating all the other tokens. One thing to notice is that, as you all know, again, this is just equivalent to maximizing the text log likelihood, because you can just rewrite the max over the probability of this autoregressive language modeling task as just being this minimum of, I just added the log here and minus, which is just the minimum of the loss, which is the cross-entropy loss. So basically, minimizing the loss is the same thing as maximizing the likelihood of your text. Any questions? OK, tokenizer. So this is one thing that people usually don't talk that much about. Tokenizers are extremely important. So it's really important that you understand, at least, what they do at a high level. So why do we need tokenizers in the first place? First, it's more general than words. So one simple thing that you might think is, oh, we're just going to take every word that we will have. And you just say, every word is a token in its own. But then what happens is, if there's a typo in your word, then you might not have any token associated with this word with a typo. And then you don't know how to actually pass this word with a typo into a large language model. So what do you do next? And also, even if you think about words, words are fine with Latin-based languages. But if you think about a language like Thai, you won't have a simple way of tokenizing by spaces, because there are no spaces between words. So really, tokens are much more general than words. First thing. Second thing that you might think is that you might tokenize every sentence, character by character. You might say, A is one token, B is another token. That would actually work, and probably very well. The issue is that then your sequence becomes super long. And as you probably remember from the lecture on transformers, the complexity grows quadratically with the length of sequences. So you really don't want to have a super long sequence. So tokenizers basically try to deal with those two problems and give common sub-sequences a certain token. And usually, how you should be thinking about it is around an average of every token is around three, four letters. And there are many algorithms for tokenization. I'll just talk about one of them to give you a high level, which is what we call byte-pair encoding, which is actually pretty common, one of the two most common tokenizers. And the way that you train a tokenizer is that first, you start with a very large corpus of text. And here, I'm really not talking about training a large language model yet. This is purely for the tokenization step. So this is my large corpus of text with these five words. Then you associate every character in this corpus of text a different token. So here, I just split up every character with a different token, and I just color coded all of those tokens. And then what you do is that you go through your text, and every time you see pairs of tokens that are very common, the most common pair of token, you just merge them. So here, you see three times the tokens T and O next to each other, so you're just going to say this is a new token. And then you continue. You repeat that. So now you have T-O-K, TOK, which happens three times, TOK with an E, that happens two times, and TOKEN, which happens twice, and then EX, which also happens twice. So this is that if you were to train a tokenizer on this corpus of text, which is very small, that's how you would finish with a trained tokenizer. In reality, you do it on much larger corpuses of text. And this is the real tokenizer of, actually, I think this is GPT-3 or ChatGPT. And here, you see how it would actually separate these words. So basically, you see the same thing as what we gave in the previous example, TOKEN becomes its own token. So tokenizer is actually split up into two tokens, TOKEN and EIZER. So yeah, that's all about tokenizers. Any question on that? Yeah? How do you deal with spaces, and how do you deal with introduction? Yeah. So actually, there's a step before tokenizers, which is what we call pre-tokenizers, which is exactly what you just said. So this is mostly, in theory, there's no reason to deal with spaces and punctuation separately. You could just say every space gets its own token, every punctuation gets its own token, and you could just do all the merging. The problem is that, so there's an efficiency question. Actually, training these tokenizers takes a long time. So you're better off, because you have to consider every pair of token. So what you end up doing is saying, if there's a space, this is very, like pre-tokenizers are very English-specific. So you say, if there's a space, we're not going to start looking at the token that came before and the token that came afterwards. So you're not merging in between spaces. But this is just like a computation optimization. You could theoretically just deal with it the same way as you deal with any other character. Yeah? When you merge tokens, do you delete the tokens that you merged away, or do you keep the smaller tokens that you merged? You actually keep the smaller tokens. I mean, in reality, it doesn't matter much, because usually on large corpus of text, you will have actually everything. But you usually keep the small ones. And the reason why you want to do that is because if, in case there's a, as we said before, you have some grammatical mistakes or some typos, you still want to be able to represent these words by character. So yeah. Yes? Are the tokens unique? So, I mean, say, in this case, T-O-K-E-N, is there only one occurrence, or do you need to leave multiple occurrence so they can take on different meanings or something? Oh, oh, I see what you say. No, no, no. It's every token has its own unique ID. So a usual, this is a great question. For example, if you think about a bank, which could be bank for money or bank like water, they will have the same token, but the model will learn, the transformer will learn that based on the words that are around it, it should associate that, I'm saying, I'm being very hand-wavy here, but associate that with a representation that is either more like the bank money side or the bank water side. But that's the transformer that does that. It's not a tokenizer. Yes? Yeah, so you mentioned during tokenization, you keep the smaller tokens to start with, right? So if you start with a T, you keep the T, and then you tell your tokenizer to expand that amount of token. So let's say maybe you didn't train on token, but in your data, you are trying to encode token. So how does the tokenizer know to encode it with token or to do it with T? Yeah, it's a great question. You basically, when you, so when you tokenize, so that's after training of the tokenizer, when you actually apply the tokenizer, you basically always choose the largest token that you can apply. So if you can do token, you will never do T. You will always do token. But it's actually, so people don't usually talk that much about tokenizers, but there's a lot of computational benefits or computational tricks that you can do for making these things faster. So I really don't think we, and honestly, I think a lot of people think that we should just get away from tokenizers and just kind of tokenize\"),\n",
       " Document(metadata={'chunk_id': 0.0, 'sources': \"ï¼‚okay, but I want Llama 3 for my specific use caseï¼‚ - Here's how.m4a\"}, page_content=\"My name is David Andrzej and in this video, I'll teach you how to fine-tune Lama3 so that it performs 10 times better for your specific use case. Let's start with what even is fine-tuning and I made this explanation in plain English so that anybody can understand. Fine-tuning is adapting a pre-trained LLM like GPT-4 or in this case Lama3 to a specific task or domain. It involves adjusting a small portion of the parameters on a more focused dataset. So, you know, when a new model releases, what everybody needs to know is how many parameters it has. We have Lama3 8B and always that number like 8B or 70B, that's the number of parameters. So we're adjusting just a small number of them to make it more focused on a specific thing. Fine-tuning customizes the outputs to be more relevant and accurate for your use case. Here's the power of fine-tuning. Cost-effectiveness. It leverages the power of pre-trained LLMs which cost tens of millions of dollars, if not hundreds of millions, to train and we can just, you know, run a GPU for a few hours and fine-tune something for, I don't know, like cents, a few cents or a few dollars at most, which is just amazing. It gives you improved performance because you can enhance the LLM on your dataset and improve accuracy for specific tasks. And it also is more data efficient. You can achieve excellent results even with smaller datasets. So, you know, maybe even like 300, 500 entries while, you know, Lama3 was trained on 15 trillion tokens. I don't know about you, but I don't have nearly as much data as Zack. So that's why fine-tuning is great for people like you and me. So how does LLM fine-tuning actually work? First, you need to prepare your dataset. And this, you know, depending on how hardcore you want to go, this can take anywhere from 20 minutes to a few hours to weeks, potentially. Depends how far you want to take it. So you create a smaller, high-quality dataset tailored to your specific use case and label it appropriately, which I'll teach you in a bit. The pre-trained LLMs weights are updated incrementally using the optimization algorithms like gradient descent based on the new dataset. So we can only fine-tune LLMs that we have access to the weights, meaning open-source, open-weights LLMs. You cannot fine-tune GPT-4 if you are not OpenAI. OpenAI can do it, obviously, but me and you, we probably don't have GPT-4 just laying on our computer. Then you monitor and refine. You evaluate the model's performance on a validation set, preventing overfitting and guide adjustments. Now, here are some real-world use cases for fine-tuning. Fine-tuning an LLM on customer service transcripts can create a chatbot, like this one, that can address issue in a way specific to your company. So let's say you have a specific product, very niche, that there is not much data about it on the internet. And if somebody messages your customer support email, you want your chatbot to respond in a specific way based on the information of your product. And that data is proprietary. It's private. Only you have it. And you can fine-tune an LLM to respond based on that data. So, like, technically, if you have enough scripts, you can fine-tune an LLM to respond like you. And, you know, if you try chat GPT, if you even give, like, chat GPT some writing and tell it, continue in this writing style, it's terrible. So this is where fine-tuning could be better. Tailored content generation. So you can fine-tune an LLM on your posts and descriptions to create engaging summaries or marketing copy, again, in your writing style, tailored to your audience. Domain-specific analysis. So fine-tuning LLM on legal or medical text can make it much better for those specific benchmarks. So you might have a model that, let's say, it reaches 50 on some arbitrary benchmark. With fine-tuning, it can reach 70 or 80. Now let's dive into how to actually implement this on Llama3. So I created this Google collab. Well, actually, most of it was created by Anslov team. A huge shout out to Anslov because they did all the heavy lifting. So I'm going to also link their GitHub below. Now, first off, I added a component that's only available in April to the community. So if you join during April, you will get a personalized AI strategy to future-proof yourself and your business. So if you want to be among people who are building the future, if you want access to all the different courses, modules, and everything else in the community, and to two weekly calls, then consider joining. And especially if you want me to give you a personalized AI strategy to future-proof yourself. So if that sounds interesting to you, make sure to join the community. It's the first link in the description. Now let's fine-tune LLama3, shall we? So first thing, we check the GPU version available in the environment and install specific dependencies that are comparable with the detected GPU to prevent conflicts. So this is this cell. By the way, if you don't know how Google Colab works, which is, you know, the software I'm using right now, it's super simple. It's basically splitting the code into cells. It's called the Jupyter Notebook, but it's like much more easier to see. You can add text, you can add graphics, and it's great for like tutorials and explaining, right? So if you never use this, it's great because it's free. And Google actually gives you a GPU so you can use this T4 GPU to train this model for free. And if you want faster, you can obviously upgrade it, right? So I'm going to link this Colab below the video as well. So we run this cell, which does what I just explained. The next cell, we need to prepare to load a range of quantized language models, including the new 15 trillion LLama3 model, so trained on 15 trillion tokens. And it's optimized for efficiency with 4-bit quantization. I mean, I'm not going to even pretend I know everything about fine-tuning because I don't. So if it seems like I have gaps in my knowledge, it's because it is. I do have those gaps in my knowledge. So I try to make it as simple as possible. But if this proves something, it proves that you don't have to be a machine learning expert to fine-tune models. So just follow along. So here, this is the max sequence length. Obviously, LLama3 is up to 8,000. So I mean, 2,000 is plenty for this demonstration, but you can do anything. You can do 4,000 or 8,000. Here, I use 4-bit quantization to reduce memory usage, but it can be false as well. So here are the models. We can see, like, we have Mistral7B, LLama2, which is the old one, Gemma from Google. But obviously, we're interested in LLama3 8B. And by the way, we can also use LLama3 70B if you want, which obviously will take longer because it's a much bigger model. So in that case, you might want to buy the premium version of Collab or just wait for a while. But yeah, I mean, everything is the same. Just here, you would change the model to LLama3 70B. And if you want to use, like, gated models from HuggingFace, which gated means that you have to usually agree to some, you know, license or whatever, then here, just remove the comment and then put your HuggingFace token here. Super simple. Now, by the way, you always have to run this. So what do you do when you go to Google Collab? You click on runtime and click run all. That way, all of the cells run. But you can also do it one by one by clicking this button right here next to each cell. And it needs to have this little tick, green tick. That way, it was executed. Here, it's not because I, you know, removed the... I changed this. So anytime you make any change, it disappears. But that doesn't matter. It was still executed. So it's stored in the runtime. Next up, we integrate LoRa. Again, you don't have to understand what this is, but it's basically a way of fine-tuning into our model, which allows us to efficiently update just a fraction of the parameters, enhancing training speed and reducing computation load. So again, we are not training the model from scratch. We're just fine-tuning a few parameters for our specific use case. And here, you can change the R to any number greater than 0, 8, 16, 32, 64, up to you. And your goals, what you want to do with it. By the way, on Sloth, the reason I'm using it is because it makes fine-tuning much faster and consuming less memory. So it's actually a great framework for this. Dataprep. We now use the Alpaca dataset from Yama, which is this one, which has 50,000 rows. And I have it loaded in VS code here. Just that way, you see how it looks like in JSON formatting. So, you know, it's a lot of lines because for everyone, it's basically times five. Yeah, so like 250,000 lines. And it's like every one of them has an instruction. I should probably zoom it in. So every one entry has an instruction. Give three tips for staying healthy. Input, this is not mandatory because instruction is already enough context. And then output, this is what the LLM should say. And you do this enough times and the LLM learns. It basically learns, right? So we can see it probably better here. And if you want to use your own dataset, you have to format it the same way. So, you know, just having output, input and instructions, these three parameters. But yeah, just look at this. Not all of them have the input, which is fine. I mean, probably like 20% or 15% have the input. And that's just extra context. So yeah, I'm also going to link this dataset below. But if you want your own dataset, which, you know, if you want your own use case, just make sure to format it the same way. So, you know, instruction, some text, input, some extra context or empty, and output, how the model should respond. And, you know, if you're getting creative, you can definitely use LLMs to generate these large datasets much faster. I mean, maybe you create really like 20 high quality examples by hand. And then you run a team of agents for creating that dataset that can just, you know, use those 20 examples to create 50,000. Like in this dataset. But yeah, that's a topic for a whole nother video. So if you want me to make a video on how to make datasets for fine tuning, then let me know. But let's go back to our Colab. So then we define a system prompt, which is, you know, custom instruction system prompt, which you already know, hopefully. That formats tasks into instruction inputs and responses. So this has to fit with our dataset. And we apply it to our dataset for the model. And we add the EOS token to signal completion. So this token right here, here we define it. And here we add it because without this, the token generation continues forever. So we don't want that, obviously. So let's look at the system prompt. It's very simple. It says below is an instruction that describes a task paired with an input that provides further context. Write a response that appropriately completes the request. And that's our system prompt. And then we feed it the instruction, the input and response. And obviously you can change the system prompt if you want. Now, train the model. We do a 60 step. We do only 60 steps here to speed things up. You can, like, this is obviously very small because it's not even one epoch, training epoch. So if you want to like actually use something for production or your business, you probably want to train it for longer than 60 steps. And I'm going to show you how in this bit. So if you do multiple epochs, you have to turn max steps none. So here, okay. Number of trained epochs is not included in here. So what you would do is you would copy this and you would go in here and look at the steps, right? So we have the steps here. You would add this. Maybe you would do four or whatever, however many you want. The more, the better. But at a certain point, it starts to not yield better result. So max steps, you have to change it to none, right? So it's like 60 right now. So you do none. And this is where you would do like proper fine tuning. But, you know, I just added that 60 for demonstration. That way it's faster. And it still took like eight minutes. So I'm not going to replicate it. I'm just going to show you everything. But yeah, basically, you know, this is what you do. You decide how many epochs you want. And then at this stage, we're configuring our model's training setup where we define things like batch size and learning rate to teach our model effectively with the data we've prepared. So obviously you can like mess with stuff here. Again, I'm not going to pretend I understand everything. But the main things are, you know, packing like this can make it five times faster for short sequences. Obviously, the steps and the epochs. But yeah, I mean, if you're confused something, just take a screenshot. Boom, like this. And ask ShedGPD. Now, this is the current memory stats, right? So we're using the Tesla T4 GPU provided from Google for free. And the max memory is 14 gigabytes. And this is where the training begins. This is the magical part, right? So here we do this line of code, trainer stats, trainer.train. And this will give us the statistics as the model trains. So again, this is only 60 steps, which is like zero epochs. But yeah, you can see the training loss going down. So like basically, smaller number is better here. So you can see like at the start, we have 1.8. Like 1.9, and then it quickly starts dropping to like 0.9, you know, around 1, 0.8. So it fluctuates a bit, but it consistently goes down 0.7. But you can see it's reaching like a asymptote, right? Obviously, it's only 60 steps. So it really doesn't mean anything. But yeah, like we ended up like 0.8 from like 2. So it shows you like if the model is actually improving. So it shows you like if the model is actually improving. And this took like eight minutes. So you can see the stats here, right? So 476 seconds, almost exactly eight minutes. Peak reserve memory was 8.9 gigabytes. And for training was 3.3 gigabytes. So not like this is the power of Unslof. It's like really optimized for this to use, to run faster and to use less memory. So that way we can fine tune GPUs for cheaper. I mean, you know, I'm using a free T4 GPU from Google. So it's free, but it's faster. Like if you didn't use Unslof, it would be a lot slower. So, okay, so 60% of, we used 60% of max memory. So that's good because we didn't like hit the limit. So we still have like 40% reserved. And for training, it was only 22%, which is even better. Inference, which means here we actually run our new model that we fine tuned. And okay, so this data set is for like instructions. And this is basically when you see a model that is like instruct at the end of it, this is what they mean. It's just trained on a large data set of instructions. Because usually the models are more for like chatting, for text generation. You know, you give it some input and it's like gives you some output. It's, you know, for more conversational. Here for instructions, for the instruct models is to follow instructions. You give it a task and it completes it. So like we can see it probably here in VS code, like rewrite the sentence to change its meaning and then output the thief escaped. Compared to data subs, so this is like all tasks. It's all in instructions. And then it shows how the model should do it. So let's look at it, right? So now we've trained the model. This took like eight minutes to do. So all of you can do this. The beauty of using a Google Cloud is that obviously it doesn't matter what machine you have. Even if you have a terrible computer, this will take the exact same time because you're using the GPU and cloud. So obviously here you can change your prompt. I mean, this is, you know, I changed the prompts here. So this is my prompt. But always make sure to leave the output blank. So here, the first one is the instruction. Then this is the input, like the extra added context and the output, leave it blank because the model will generate it, right? So list the prime numbers contained within\"),\n",
       " Document(metadata={'chunk_id': 0.0, 'sources': 'Large Language Models (LLMs) - Everything You NEED To Know.m4a'}, page_content=\"that. And again, it's all about the quality of your data. So if you have a really good data set that you're going to fine-tune a model on, the model is going to be really, really good. And conversely, if you have a poor quality data set, it's not going to perform as well. All right, let me pause for a second and talk about AI Camp. So as mentioned earlier, this video, all of its content, the animations, have been created in collaboration with students from AI Camp. AI Camp is a learning experience for students that are age 13 and above. You work in small, personalized groups with experienced mentors. You work together to create an AI product using NLP, computer vision, and data science. AI Camp has both a three-week and a one-week program during summer that requires zero programming experience. And they also have a new program which is 10 weeks long during the school year, which is less intensive than the one-week and three-week programs for those students who are really busy. AI Camp's mission is to provide students with deep knowledge in artificial intelligence, which will position them to be ready for AI in the real world. I'll link an article from USA Today in the description all about AI Camp. But if you're a student or if you're a parent of a student within this age, I would highly recommend checking out AI Camp. Go to ai-camp.org to learn more. Now let's talk about limitations and challenges of large language models. As capable as LLMs are, they still have a lot of limitations. Recent models continue to get better, but they are still flawed. They're incredibly valuable and knowledgeable in certain ways, but they're also deeply flawed in others, like math and logic and reasoning. They still struggle a lot of the time versus humans, which understand concepts like that pretty easily. Also, bias and safety continue to be a big problem. Large language models are trained on data created by humans, which is naturally flawed. Humans have opinions on everything, and those opinions trickle down into these models. These data sets may include harmful or biased information, and some companies take their models a step further and provide a level of censorship to those models. And that's an entire discussion in itself whether censorship is worthwhile or not. I know a lot of you already know my opinions on this from my previous videos. And another big limitation of LLMs historically has been that they only have knowledge up until the point where their training occurred. But that is starting to be solved with ChatGPT being able to browse the web, for example. Grok from x.ai being able to access live tweets. But there's still a lot of kinks to be worked out with this. Also, another big challenge for LLMs\"),\n",
       " Document(metadata={'chunk_id': 0.0, 'sources': 'A Practical Introduction to Large Language Models (LLMs).m4a'}, page_content=\"Hey everyone, I'm Shaw and I'm back with a new data science series. In this new series, I'm going to be talking about large language models and how to use them in practice. In this video, I will give a beginner-friendly introduction to large language models and describe three levels of working with them in practice. Future videos in this series will discuss various practical aspects of large language models, things like using OpenAI's Python API, using open-source solutions like the Hugging Phase Transformers library, how to fine-tune large language models, and of course, how to build a large language model from scratch. If you enjoyed this content, please be sure to like, subscribe, and share with others. And if you have any suggestions for me to include in this series, please share those in the comments section below. And so with that, let's get into the video. So to kick off the video series, in this video, I'm going to be giving a practical introduction to large language models. And this is meant to be very beginner-friendly and high level, and I'll leave more technical details and example code for future videos and blogs in this series. So a natural place to start is, what is a large language model, or LLM for short? So I'm sure most people are familiar with ChatGPT, however, if you are enlightened enough to not keep up with new cycles and tech hype and all this kind of stuff, ChatGPT is essentially a very impressive and advanced chatbot. So if you go to the ChatGPT website, you can ask it questions like, what's a large language model? And it will generate a response very quickly, like the one that we are seeing here. And that is really impressive. Like if you were ever on AOL, Instant Messenger, also called AIM, you know, back in early 2000s or in the early days of the internet, there were chatbots then, there have been chatbots for a long time, but this one feels different. Like the text is very impressive and it almost feels human-like. A question you might have when you hear the term large language model is, what makes it large? What's the difference between a large language model and a not large language model? And this was exactly the question I had when I first heard the term. And so one way we can put it is that large language models are a special type of language model. But what makes them so special? And I'm sure there's a lot that can be said about large language models. But to keep things simple, I'm going to talk about two distinguishing properties. The first quantitative and the second qualitative. So first, quantitatively, large language models are large. They have many, many more model parameters than past language models. And so these days, this is anywhere from tens to hundreds of billions of parameters. The model parameters are numbers that define how the model will take an input and generate the output. So it's essentially the numbers that define the model itself. Okay, so that's a quantitative perspective of what distinguishes large language models from not large language models. But there's also this qualitative perspective and these so-called emergent properties that start to show up when language models become large. And so emergent properties is the language used in this paper cited below, a survey of large language models available in the archive. Really great beginner's guide, I recommend it. But essentially what this term means is there are properties in large language models that do not appear in smaller language models. And so one example of this is zero-shot learning. One definition of zero-shot learning is the capability of a machine learning model to complete a task it was not explicitly trained to do. So while this may not sound super impressive to us very smart and sophisticated humans, this is actually a major innovation in how these state-of-the-art machine learning models are developed. So to see this, we can compare the old state-of-the-art paradigm to this new state-of-the-art paradigm. The old way, and not too long ago, we can say like about 5, 10 years ago, the way the high-performing best machine learning models were developed was strictly through supervised learning. What this would typically look like is you would train a model on thousands, if not millions, of labeled examples. And so what this might have looked like is you have some input text like, hello, hola, how's it going, esta bien, so on and so forth. And you take all these examples and you manually assign a label to each example. Here we're labeling the language, so English, Spanish, so on. And so you can imagine that this would take a tremendous amount of human effort to get thousands, if not millions, of high-quality examples. So let's compare this to the more recent innovation with large language models who use a different paradigm. They use so-called self-supervised learning. What that looks like in the context of large language models is you train a very large model on a very large corpus of data. And so what this can look like is if you're trying to build a model that can do language classification, instead of painstakingly generating this labeled data set, you can just take a corpus of English text and a corpus of Spanish text and train a model in a self-supervised way. So in contrast to supervised learning, self-supervised learning does not require manual labeling of each example in your data set. The so-called labels or targets for the model are actually defined from the inherent structure of the data or this context of the text. So you might be thinking to yourself, how does this self-supervised learning actually work? And so one of the most popular ways that this is done is the next word prediction paradigm. So suppose we have this text, listen to your, and we want to predict what the next word would be. But clearly there's not just one word that can go after this string of words. There are actually many words you can put after this text and it would make sense. In this next word prediction paradigm, what the language model is trying to do is to predict the probability distribution of the next word given the previous words. What this might look like is listen to your heart might be the most probable next word, but another likely word could be gut or listen to your body or listen to your parents and listen to your grandma. And so this is essentially the core task that these large language models are trained to do. And the way the large language model will learn these probabilities is that it'll see so many examples in this massive corpus of text that it's trained on and it has a massive number of internal parameters so it can efficiently represent all the different statistical associations with different words. And an important point here is that context matters. If we simply added the word don't to the front of this string here and it changed it to don't listen to your, then this probability distribution could look entirely different because just by adding one word before this sentence, we completely change the meaning of the sentence. And so to put this a bit more mathematically, and I promise this is the most technical thing in this video, this is an example of a auto regression task. So auto meaning self, regression meaning you're trying to predict something. So what this notation means is what is the probability of the nth text or more technically the nth token given the preceding m token. So n minus one, n minus two, n minus three, so on and so forth. And so if you really want to boil everything down, this is the core task most large language models are doing. And somehow through this very simple task of predict the next word, we get this incredible performance from tools like chat GPT and other large language models. So now with that foundation set, hopefully you have a decent understanding of what large language models are and how they work and a broader context for them. Now let's talk about how we can use these in practice. Here I will talk about three levels in which we can use large language models. These three levels are ordered by the technical expertise and computational resources required. The most accessible way to use large language models is prompt engineering. Next we have model fine tuning. And then finally we have build your own large language model. So starting from level one, prompt engineering here, I have a pretty broad definition of prompt engineering. Here I define it as just using an LLM out of the box. So more specifically, not touching any of the model parameters. So of these tens of billions or hundreds of billions of parameters that define the model, we're not going to touch any of them. We're just going to leave them as is. Here I'll talk about two ways we can do this. One is the easy way and I'm sure is the way that most people in the world have interacted with large language models, which is using things like chat GPT. These are like intuitive user interfaces. They don't require any code and they're completely free. Someone can just go to the chat GPT website, type in a prompt and it'll spit out a response. So while this is definitely the easiest way to do it, it is a bit restrictive in that you have to go to their website. This doesn't really scale well if you're trying to build a product or service around it. But for a lot of use cases, this is actually super helpful. So for applications where the easy way doesn't cut it, there is the less easy way, which is using things like the open AI API or the hugging face transformers library. And these tools provide ways to interact with large language models programmatically. So essentially using Python. In the case of the open AI API, instead of typing your request in the chat GPT user interface, you can send it over to open AI using Python and their API and then you will get a response back. Of course, their API is not free, so you have to pay per API call. Another way we can do this is via open source solutions, one of which is the hugging face transformers library, which gives you easy access to open source large language models. So it's free and you can run these models locally. So no need to send your potentially proprietary or confidential information to a third party in open AI. So future videos of the series, we'll dive into all these different aspects. I'll talk about the open AI API, what it is, how it works, share example code. I'll dive into the hugging face transformers library, same situation. What the heck is it? How does it work? And then sharing some Python example code there. I'll also do a video talking about prompt engineering more generally. How can we create prompts to get good responses from large language models? And so while prompt engineering is the most accessible way to work with large language models, just working with a model out of the box may give you suboptimal performance on a specific task or use case. Or the model has really good performance, but it's massive. It has like a hundred billion parameters. So a question might be, is there a way we can use a smaller model, but kind of tweak it in a way to have good performance on our very narrow and specific use case. And so this brings us to level two, which is model fine tuning, which here I define as adjusting at least one internal model parameter for a particular task. And so here there are just generally two steps. One, you get a pre-trained large language model, maybe from open AI, or maybe an open source model from the hugging face transformers library. And then you update the model parameters given task specific examples. Kind of going back to the supervised learning versus self-supervised learning, the pre-trained model is going to be a self-supervised model. So it'll be trained on this simple word prediction task. But in step two, here's where we're going to do supervised learning or even reinforcement learning to tweak the model parameters for a specific use case. And so this turns out to work very well. Examples like ChatGPT, you're not working with the raw pre-trained model. The model that you are interacting with in ChatGPT is actually a fine-tuned model developed using reinforcement learning. And so a reason why this might work is that in doing this self-supervised task and doing the word prediction, the base model, this pre-trained large language model is learning useful representations for a wide variety of tasks. So in a future video, I will dive in more deeply into fine-tuning techniques. Another one is low rank adaptation or LORA for short. And then another popular one is reinforcement learning with human feedback or RLHF. And of course, there is a third step here. You'll deploy your fine-tuned large language model to do some kind of service or, you know, use it in your day-to-day life and you'll profit somehow. And so my sense is between prompt engineering and model fine-tuning, you can probably handle 99% of large language model use cases and applications. However, if you're a large organization, large enterprise, and security is a big concern. So you don't want to use open source models or you don't want to send data to a third party via an API. And maybe you want your large language model to be very good at a relatively specific set of tasks. You want to customize the training data in a very specific way and you want to own all the rights, have it for commercial use, all this kind of stuff. Then it can make sense to go one step further beyond model fine-tuning and build your own large language model. And so here I define it as just coming up with all the model parameters. So I'll just talk about how to do this at a very high level here, and I'll leave technical details for a future video in the series. First, we need to get our data. And so what this might look like is you'll get a book corpus, a Wikipedia corpus, and a Python corpus. And so this is billions of tokens of text. And then you will take that and pre-process it, refine it into your training data set. And then you can take the training data set and do the model training through self-supervised learning. And then out of that comes the pre-trained large language model. So you can take this as your starting point for level two and go from there. And so if you enjoyed this video and you want to read more, be sure to check out the blog in Towards Data Science. There I share some more details that I may have missed in this video. This series is both a video and blog series. So each video will have an associated blog, and there will also be tons of example code on the GitHub repository. The goal of the series is to really just make information about large language models much more accessible. I really do think this is the technological innovation of our time, and there are so many opportunities for potential use cases, applications, products, services that can come out of large language models. And that's something that I want to support. I think we'll be better off if more people understand this technology and are applying it to solving problems. So with that, be sure to hit the subscribe button to keep up with future videos in this series. If you have any questions or suggestions for other topics I should cover in this series, please drop those in the comment section below. And as always, thank you so much for your time and thanks for watching.\")]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "\n",
    "# Create LLM\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\")\n",
    "\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5, \"tenant\": \"knowledge_base_llm\"})\n",
    "retriever.invoke(\"Tell me everything I need to know about LLMs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step III - Understand what we are going to do next ðŸ¤”\n",
    "\n",
    "Alright, let's pause on the code for a little bit as you need to think about next steps when building your LLM app. If we want to have a bot that has memory + uses RAG, we need to be smart but also choose between several solutions as each have some trade-offs.\n",
    "\n",
    "The main problem with LLMs is the **context window**. If you feed too many tokens, you will overflow the model and therefore your code won't work. This is very likely to happen (at least as for today's models) especially if you have a chat history + a context. \n",
    "\n",
    "So what do we do? You have several solutions: \n",
    "\n",
    "* (the we will choose) - Build an LLM app in two steps:\n",
    "    1. We will build a *history based retriever* that will take into account user question **and** the chat history **and then reformulate the question based on these two parameters** before applying it to the retriever. This way we will have a question that will basically be a mix of the user and model interaction + the latest question \n",
    "\n",
    "    2. Based on the retrieved context, we'll ask the LLM to provide the final answer \n",
    "\n",
    "\n",
    "* Build an LLM app with no summary \n",
    "    1. This is an easier way to code the app but it will come with the risk of overflowing the context window \n",
    "    2. However your LLM will have access to the full history which can be useful if your user asks unrelated questions \n",
    "\n",
    "\n",
    "Again for this exercise, we will choose the first solution, but feel free to experiment the other one and work around the context window problem ðŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step IV - Build the History aware retriever prompt \n",
    "\n",
    "Alright let's tackle the first step of our solution which is: building the prompt. Here we want:\n",
    "\n",
    "* A system prompt that takes a `chat_history` and a user `input` (the question) as parameter \n",
    "* You should be able to build the chat template using `ChatTemplateMessage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = \"\"\"\n",
    "    Given a chat history and the latest user question, \n",
    "    formulate a standalone question which can be understood \n",
    "    without the chat history. Do NOT answer the question, \n",
    "    just reformulate it if needed and otherwise return it as is.\n",
    "\n",
    "    Here is the chat history:\n",
    "    {chat_history}\n",
    "\n",
    "    Here is the user question:\n",
    "    {input}\n",
    "\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step V - Build the history retriever chain \n",
    "\n",
    "Now the hard part: **Build the chain**. To do so you can: \n",
    "\n",
    "* First think about a simple chain that simply passes the prompt to the retriever \n",
    "* Once this is done, you will need to think about a specific case: **What happens when there is no chat_history** \n",
    "    * Even if I didn't look at your code, it is very likely that it will throw an error \n",
    "    * To fix that, you will need to have some kind of a conditional chain where you will simply call the retriever without taking the chat history into account \n",
    "    * To do so, I definitely advise you to look at [`RunnableBranch`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.branch.RunnableBranch.html#langchain_core.runnables.branch.RunnableBranch) which is great for that ðŸ˜‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### DOC 0\n",
      "\n",
      "So, let's get started, so I'll be talking about building LLMs today, so I think a lot of you have heard of LLMs before, but just as a quick recap, LLMs, standing for Large Language Models, are basically all the chatbots that you've been hearing about recently, so ChatGPT from OpenAI, Claude from Entropiq, Gemini, and Lama, and other type of models like this, and today we'll be talking about how do they actually work, so it's going to be an overview because it's only one lecture, and it's hard to compress everything, but hopefully I'll touch a little bit about all the components that are needed to train some of these LLMs. Also, if you have questions, please interrupt me and ask. If you have a question, most likely other people in the room or on Zoom have the same question, so please ask. Great, so what matters when training LLMs? So, there are a few key components that matter. One is the architecture, so as you probably all know, LLMs are neural networks, and when you think about neural networks, you have to think about what architecture you're using. Another component which is really important is the training loss and the training algorithm, so how you actually train these models. Then it's data, so what do you train these models on? The evaluation, which is how do you know whether you're actually making progress towards the goal of LLMs, and then the system component, so that is like how do you actually make these models run on modern hardware, which is really important because these models are really large, so now more than ever, systems are actually really an important topic for LLMs. So, those five components, you probably all know that LLMs, and if you don't know, LLMs are all based on transformers, or at least some version of transformers. I'm actually not going to talk about the architecture today, one, because I gave a lecture on transformers a few weeks ago, and two, because you can find so much information online on transformers, but I think you can- there's much less information about the other four topics, so I really want to talk about those. Another thing to say is that most of academia actually focuses on architecture and training algorithm and losses. As academics, and I've done that for a lot- a big part of my career, is simply we like thinking that this is like we make new architectures, new models, and it seems like it's very important, but in reality, honestly, what matters in practice is mostly the three other topics, so data, evaluation, and systems, which is what most of industry actually focuses on. So that's also one of the reasons why I don't want to talk too much about the architecture, because really the rest is super important. Great, so overview of the lecture, I'll be talking about pre-training. So pre-training, you probably heard that word, this is the general word, this is kind of the classical language modeling paradigm, where you basically train your language model to essentially model all of internet. And then there's a post-training, which is a more recent paradigm, which is taking these large language models and making them essentially AI assistants. So this is more of a recent trend since ChatGPT. So if you ever heard of GPT-3 or GPT-2, that's really pre-training land. If you heard of ChatGPT, which you probably have, this is really post-training land. So I'll be talking about both, but I'll start with pre-training. And specifically, I'll talk about what is the task of pre-training LLMs and what is the laws that people actually use. So language modeling, this is a quick recap. Language models at a high level are simply models of probability distribution over sequences of tokens or of words. So it's basically some model of p of x1 to xl, where x1 is basically word 1 and xl is the last word in the sequence or in the sentence. So very concretely, if you have a sentence like the mouse ate the cheese, what the language model gives you is simply a probability of this sentence being uttered by a human or being found online. So if you have another sentence like the mouse ate cheese, here there's grammatical mistakes. So the model should know that this should have some syntactic knowledge. So it should know that this has less likelihood of appearing online. If you have another sentence like the cheese ate the mouse, then the model should hopefully know about the fact that usually cheese don't eat mouse. So there's some semantic knowledge and this is less likely than the first sentence. So this is basically at a high level what language models are. One word that you probably have been hearing a lot in the news are generative models. So this is just something that can generate models that can generate sentences or can generate some data. The reason why we say language models are generative models is that once you have a model of a distribution, you can simply sample from this model and then we can generate data. So you can generate sentences using a language model. So the type of models that people are all currently using are what we call autoregressive language models. And the key idea of autoregressive language models is that you take this distribution over words and you basically decompose it into the distribution of the first word, multiply it by the distribution of the likelihood of the distribution of the second word given the first word, and multiply it by p of the third word given the first two words. So there's no approximation here. This is just the chain rule of probability, which hopefully you all know about, really no approximation. This is just one way of modeling a distribution. So slightly more concisely, you can write it as a product of p's of the next word given everything which happened in the past, so of the context. So this is what we call autoregressive language models. Again, this is really not the only way of modeling distribution, this is just one way. It has some benefits and some downsides. One downside of autoregressive language models is that when you actually sample from this autoregressive language model, you basically have a for loop which generates the next word, then conditions on that next word, and then regenerate the other word. So basically, if you have a longer sentence that you want to generate, it takes more time to generate it. So there are some downsides of this current paradigm, but that's what we currently have, so I'm going to talk about this one. Great. So autoregressive language models. At a high level, what the task of autoregressive language model is, is simply predicting the next word, as I just said. So if you have a sentence like she likely prefers, one potential next word might be dogs. And the way we do it is that we first tokenize. So you take these words or sub words, you tokenize them, and then you give an ID for each token. So here I have one, two, three. Then you pass it through this black box. As I already said, we're not going to talk about the architecture. You just pass it through a model, and you then get a distribution, a probability distribution over the next word, over the next token. And then you sample from this distribution, you get a new token, and then you de-tokenize. So you get a new ID, you de-tokenize, and that's how you basically sample from a language model. One thing which is important to note is that the last two steps are actually only needed during inference. When you do training, you just need to predict the most likely token, and you can just compare to the real token, which happened in practice, and then you basically change the weights of your model to increase the probability of generating that token. Great. So autoregressive neural language models. So to be slightly more specific, still without talking about the architecture, the first thing we do is that we have all of these- oh, sorry, yes? On the previous slide, when you're predicting the probability of the next token, does this mean that your final output vector has to be the same dimensionality as the number of tokens that you have? Yes. How do you deal with if you're adding more tokens to your co-presenters? Yeah. So we're going to talk about tokenization actually later, so you will get some sense of this. You basically can't deal with adding new tokens. I'm kind of exaggerating. There are methods for doing it, but essentially people don't do it. So it's really important to think about how you tokenize your text, and that's why we'll talk about that later. But it's a very good point to note is that basically the vocabulary size, so the number of tokens that you have, is essentially the output of your language model. So it's actually pretty large. Okay, so autoregressive neural language models. First thing you do is that you take every word or every token. You embed them, so you get some vector representation for each of these tokens. You pass them through some neural network, as we said, it's a transformer. Then you get a representation for all the words in the context. So it's basically a representation of the entire sentence. You pass it through a linear layer, as you just said, to basically map it to the number so that the output, the number of outputs is the number of tokens. You then pass it through some softmax, and you basically get a probability distribution over the next words given every word in the context. And the laws that you use is basically, it's essentially a task of classifying the next token. So it's a very simple kind of machine learning task. So you use the cross-entropy laws, where you basically look at the actual target that happened, which is a target distribution, which is a one-hot encoding, which here in this case says, I saw the real word that happened is cat. So that's a one-hot distribution over cat. And here, this is the distribution that you generated. And basically, you do cross-entropy, which really just increases the probability of generating cat and decreases the probability of generating all the other tokens. One thing to notice is that, as you all know, again, this is just equivalent to maximizing the text log likelihood, because you can just rewrite the max over the probability of this autoregressive language modeling task as just being this minimum of, I just added the log here and minus, which is just the minimum of the loss, which is the cross-entropy loss. So basically, minimizing the loss is the same thing as maximizing the likelihood of your text. Any questions? OK, tokenizer. So this is one thing that people usually don't talk that much about. Tokenizers are extremely important. So it's really important that you understand, at least, what they do at a high level. So why do we need tokenizers in the first place? First, it's more general than words. So one simple thing that you might think is, oh, we're just going to take every word that we will have. And you just say, every word is a token in its own. But then what happens is, if there's a typo in your word, then you might not have any token associated with this word with a typo. And then you don't know how to actually pass this word with a typo into a large language model. So what do you do next? And also, even if you think about words, words are fine with Latin-based languages. But if you think about a language like Thai, you won't have a simple way of tokenizing by spaces, because there are no spaces between words. So really, tokens are much more general than words. First thing. Second thing that you might think is that you might tokenize every sentence, character by character. You might say, A is one token, B is another token. That would actually work, and probably very well. The issue is that then your sequence becomes super long. And as you probably remember from the lecture on transformers, the complexity grows quadratically with the length of sequences. So you really don't want to have a super long sequence. So tokenizers basically try to deal with those two problems and give common sub-sequences a certain token. And usually, how you should be thinking about it is around an average of every token is around three, four letters. And there are many algorithms for tokenization. I'll just talk about one of them to give you a high level, which is what we call byte-pair encoding, which is actually pretty common, one of the two most common tokenizers. And the way that you train a tokenizer is that first, you start with a very large corpus of text. And here, I'm really not talking about training a large language model yet. This is purely for the tokenization step. So this is my large corpus of text with these five words. Then you associate every character in this corpus of text a different token. So here, I just split up every character with a different token, and I just color coded all of those tokens. And then what you do is that you go through your text, and every time you see pairs of tokens that are very common, the most common pair of token, you just merge them. So here, you see three times the tokens T and O next to each other, so you're just going to say this is a new token. And then you continue. You repeat that. So now you have T-O-K, TOK, which happens three times, TOK with an E, that happens two times, and TOKEN, which happens twice, and then EX, which also happens twice. So this is that if you were to train a tokenizer on this corpus of text, which is very small, that's how you would finish with a trained tokenizer. In reality, you do it on much larger corpuses of text. And this is the real tokenizer of, actually, I think this is GPT-3 or ChatGPT. And here, you see how it would actually separate these words. So basically, you see the same thing as what we gave in the previous example, TOKEN becomes its own token. So tokenizer is actually split up into two tokens, TOKEN and EIZER. So yeah, that's all about tokenizers. Any question on that? Yeah? How do you deal with spaces, and how do you deal with introduction? Yeah. So actually, there's a step before tokenizers, which is what we call pre-tokenizers, which is exactly what you just said. So this is mostly, in theory, there's no reason to deal with spaces and punctuation separately. You could just say every space gets its own token, every punctuation gets its own token, and you could just do all the merging. The problem is that, so there's an efficiency question. Actually, training these tokenizers takes a long time. So you're better off, because you have to consider every pair of token. So what you end up doing is saying, if there's a space, this is very, like pre-tokenizers are very English-specific. So you say, if there's a space, we're not going to start looking at the token that came before and the token that came afterwards. So you're not merging in between spaces. But this is just like a computation optimization. You could theoretically just deal with it the same way as you deal with any other character. Yeah? When you merge tokens, do you delete the tokens that you merged away, or do you keep the smaller tokens that you merged? You actually keep the smaller tokens. I mean, in reality, it doesn't matter much, because usually on large corpus of text, you will have actually everything. But you usually keep the small ones. And the reason why you want to do that is because if, in case there's a, as we said before, you have some grammatical mistakes or some typos, you still want to be able to represent these words by character. So yeah. Yes? Are the tokens unique? So, I mean, say, in this case, T-O-K-E-N, is there only one occurrence, or do you need to leave multiple occurrence so they can take on different meanings or something? Oh, oh, I see what you say. No, no, no. It's every token has its own unique ID. So a usual, this is a great question. For example, if you think about a bank, which could be bank for money or bank like water, they will have the same token, but the model will learn, the transformer will learn that based on the words that are around it, it should associate that, I'm saying, I'm being very hand-wavy here, but associate that with a representation that is either more like the bank money side or the bank water side. But that's the transformer that does that. It's not a tokenizer. Yes? Yeah, so you mentioned during tokenization, you keep the smaller tokens to start with, right? So if you start with a T, you keep the T, and then you tell your tokenizer to expand that amount of token. So let's say maybe you didn't train on token, but in your data, you are trying to encode token. So how does the tokenizer know to encode it with token or to do it with T? Yeah, it's a great question. You basically, when you, so when you tokenize, so that's after training of the tokenizer, when you actually apply the tokenizer, you basically always choose the largest token that you can apply. So if you can do token, you will never do T. You will always do token. But it's actually, so people don't usually talk that much about tokenizers, but there's a lot of computational benefits or computational tricks that you can do for making these things faster. So I really don't think we, and honestly, I think a lot of people think that we should just get away from tokenizers and just kind of tokenize\n",
      "### DOC 1\n",
      "\n",
      "This video is going to give you everything you need to go from knowing absolutely nothing about artificial intelligence and large language models to having a solid foundation of how these revolutionary technologies work. Over the past year, artificial intelligence has completely changed the world, with products like ChatGPT potentially appending every single industry and how people interact with technology in general. And in this video, I will be focusing on LLMs, how they work, ethical considerations, applications, and so much more. And this video was created in collaboration with an incredible program called AI Camp, in which high school students learn all about artificial intelligence. And I'll talk more about that later in the video. Let's go. So first, what is an LLM? Is it different from AI? And how is ChatGPT related to all of this? LLMs stand for large language models, which is a type of neural network that's trained on massive amounts of text data. It's generally trained on data that can be found online. Everything from web scraping to books to transcripts, anything that is text-based can be trained into a large language model. And taking a step back, what is a neural network? A neural network is essentially a series of algorithms that try to recognize patterns in data. And really what they're trying to do is simulate how the human brain works. And LLMs are a specific type of neural network that focus on understanding natural language. And as mentioned, LLMs learn by reading tons of books, articles, internet text, and there's really no limitation there. And so how do LLMs differ from traditional programming? Well, with traditional programming, it's instruction-based, which means if X, then Y. You're explicitly telling the computer what to do. You're giving it a set of instructions to execute. But with LLMs, it's a completely different story. You're teaching the computer not how to do things, but how to learn how to do things. And this is a much more flexible approach and is really good for a lot of different applications where previously traditional coding could not accomplish them. So one example application is image recognition. With image recognition, traditional programming would require you to hard code every single rule for how to, let's say, identify different letters. So A, B, C, D. But if you're handwriting these letters, everybody's handwritten letters look different. So how do you use traditional programming to identify every single possible variation? Well, that's where this AI approach comes in. Instead of giving a computer explicit instructions for how to identify a handwritten letter, you instead give it a bunch of examples of what handwritten letters look like, and then it can infer what a new handwritten letter looks like based on all of the examples that it has. What also sets machine learning and large language models apart in this new approach to programming is that they are much more flexible, much more adaptable, meaning they can learn from their mistakes and inaccuracies, and are thus so much more scalable than traditional programming. LLMs are incredibly powerful at a wide range of tasks, including summarization, text generation, creative writing, question and answer, programming, and if you've watched any of my videos, you know how powerful these large language models can be. And they're only getting better. Know that right now, large language models and AI in general are the worst they'll ever be. And as we're generating more data on the internet, and as we use synthetic data, which means data created by other large language models, these models are going to get better rapidly. And it's super exciting to think about what the future holds. Now let's talk a little bit about the history and evolution of large language models. We're going to cover just a few of the large language models today in this section. The history of LLMs traces all the way back to the ELISA model, which was from 1966, which was really the first language model. It had pre-programmed answers based on keywords. It had a very limited understanding of the English language. And like many early language models, you started to see holes in its logic after a few back and forths in a conversation. And then after that, language models really didn't evolve for a very long time. Although technically the first recurrent neural network was created in 1924 or RNN, they weren't really able to learn until 1972. And these new learning language models are a series of neural networks with layers and weights and a whole bunch of stuff that I'm not going to get into in this video. And RNNs were really the first technology that was able to predict the next word in a sentence rather than having everything pre-programmed for it. And that was really the basis for how current large language models work. And even after this and the advent of deep learning in the early 2000s, the field of AI evolved very slowly, with language models far behind what we see today. This all changed in 2017, where the Google DeepMind team released a research paper about a new technology called Transformers. And this paper was called Attention is All You Need. And a quick side note, I don't think Google even knew quite what they had published at that time. But that same paper is what led OpenAI to develop ChatGPT. So obviously other computer scientists saw the potential for the Transformers architecture. With this new Transformers architecture, it was far more advanced. It required decreased training time, and it had many other features like self-attention, which I'll cover later in this video. Transformers allowed for pre-trained large language models like GPT-1, which was developed by OpenAI in 2018. It had 117 million parameters, and it was completely revolutionary, but soon to be outclassed by other LLMs. Then after that, BERT was released, B-E-R-T, in 2018. That had 340 million parameters, and had bidirectionality, which means it had the ability to process text in both directions, which helped it have a better understanding of context. And as comparison, a unidirectional model only has an understanding of the words that came before the target text. And after this, LLMs didn't develop a lot of new technology, but they did increase greatly in scale. GPT-2 was released in early 2019, and had 2.5 billion parameters. Then GPT-3 in June of 2020, with 175 billion parameters. And it was at this point that the public started noticing large language models. GPT had a much better understanding of natural language than any of its predecessors. And this is the type of model that powers ChatGPT, which is probably the model that you're most familiar with. And ChatGPT became so popular because it was so much more accurate than anything anyone had ever seen before. And it was really because of its size. And because it was now built into this chatbot format, anybody could jump in and really understand how to interact with this model. ChatGPT 3.5 came out in December of 2022, and started this current wave of AI that we see today. Then in March 2023, GPT-4 was released, and it was incredible, and still is incredible to this day. It had a whopping reported 1.76 trillion parameters, and uses likely a mixture of experts approach, which means it has multiple models that are all fine-tuned for specific use cases. And then when somebody asks a question to it, it chooses which of those models to use. And then they added multi-modality and a bunch of other features. And that brings us to where we are today. All right, now let's talk about how LLMs actually work in a little bit more detail. The process of how large language models work can be split into three steps. The first of these steps is called tokenization. And there are neural networks that are trained to split long text into individual tokens. And a token is essentially about three fourths of a word. So if it's a shorter word like hi, or that, or there, it's probably just one token. But if you have a longer word like summarization, it's going to be split into multiple pieces. And the way that tokenization happens is actually different for every model. Some of them separate prefixes and suffixes. Let's look at an example. What is the tallest building? So what is the tallest building? Are all separate tokens. And so that separates the suffix off of tallest, but not building because it is taking the context into account. And this step is done so models can understand each word individually, just like humans. We understand each word individually and as groupings of words. And then the second step of LLMs is something called embeddings. The large language models turns those tokens into embedding vectors, turning those tokens into essentially a bunch of numerical representations of those tokens numbers. And this makes it significantly easier for the computer to read and understand each word and how the different words relate to each other. And these numbers all correspond with the position in an embeddings vector database. And then the final step in the process is transformers, which we'll get to in a little bit. But first, let's talk about vector databases. And I'm going to use the terms word and token interchangeably. So just keep that in mind because they're almost the same thing, not quite, but almost. And so these word embeddings that I've been talking about are placed into something called a vector database. These databases are storage and retrieval mechanisms that are highly optimized for vectors. And again, those are just numbers, long series of numbers. Because they're converted into these vectors, they can easily see which words are related to other words based on how similar they are, how close they are based on their embeddings. And that is how the large language model is able to predict the next word based on the previous words. Vector databases capture the relationship between data as vectors in multi-dimensional space. I know that sounds complicated, but it's really just a lot of numbers. Vectors are objects with a magnitude and a direction, which both influence how similar one vector is to another. And that is how LLMs represent words based on those numbers. Each word gets turned into a vector, capturing semantic meaning and its relationship to other words. So here's an example. The words book and worm, which independently might not look like they're related to each other, but they are related concepts because they frequently appear together. A bookworm, somebody who likes to read a lot. And because of that, they will have embeddings that look close to each other. And so models build up an understanding of natural language using these embeddings and looking for similarity of different words, terms, groupings of words, and all of these nuanced relationships. And the vector format helps models understand natural language better than other formats. And you can kind of think of all this like a map. If you have a map with two landmarks that are close to each other, they're likely going to have very similar coordinates. So it's kind of like that. Okay, now let's talk about transformers. Matrix representations can be made out of those vectors that we were just talking about. This is done by extracting some information out of the numbers and placing all of the information into a matrix through an algorithm called multi-head attention. The output of the multi-head attention algorithm is a set of numbers which tells the model how much the words and its order are contributing to the sentence as a whole. We transform the input matrix into an output matrix which will then correspond with a word having the same values as that output matrix. So basically we're taking that input matrix, converting it into an output matrix, and then converting it into natural language. And the word is the final output of this whole process. This transformation is done by the algorithm that was created during the training process. So the model's understanding of how to do this transformation is based on all of its knowledge that it was trained with, all of that text data from the internet, from books, from articles, etc. And it learned which sequences of words go together and their corresponding next words based on the weights determined during training. Transformers use an attention mechanism to understand the context of words within a sentence. It involves calculations with the dot product, which is essentially a number representing how much the word contributed to the sentence. It will find the difference between the dot products of words and give it correspondingly large values for attention. And it will take that word into account more if it has higher attention. Now let's talk about how large language models actually get trained. The first step of training a large language model is collecting the data. You need a lot of data. When I say billions of parameters, that is just a measure of how much data is actually going into training these models. And you need to find a really good data set. If you have really bad data going into a model, then you're going to have a really bad model. Garbage in, garbage out. So if a data set is incomplete or biased, the large language model will be also. And data sets are huge. We're talking about massive, massive amounts of data. They take data in from web pages, from books, from conversations, from reddit posts, from x posts, from youtube transcriptions. Basically anywhere where we can get some text data, that data is becoming so valuable. Let me put into context how massive the data sets we're talking about really are. So here's a little bit of text which is 276 tokens. That's it. Now if we zoom out, that one pixel is that many tokens. And now here's a representation of 285 million tokens, which is 0.02% of the 1.3 trillion tokens that some large language models take to train. And there's an entire science behind data pre-processing, which prepares the data to be used to train a model. Everything from looking at the data quality, to labeling consistency, data cleaning, data transformation, and data reduction. But I'm not going to go too deep into that. And this pre-processing can take a long time. And it depends on the type of machine being used, how much processing power you have, the size of the data set, the number of pre-processing steps, and a whole bunch of other factors that make it really difficult to know exactly how long pre-processing is going to take. But one thing that we know takes a long time is the actual training. Companies like NVIDIA are building hardware specifically tailored for the math behind large language models. And this hardware is constantly getting better. The software used to process these models are getting better also. And so the total time to process models is decreasing, but the size of the models is increasing. And to train these models, it is extremely expensive because you need a lot of processing power, electricity, and these chips are not cheap. And that is why NVIDIA stock price has skyrocketed. Their revenue growth has been extraordinary. And so with the process of training, we take this pre-processed text data that we talked about earlier, and it's fed into the model. And then using transformers, or whatever technology a model is actually based on, but most likely transformers, it will try to predict the next word based on the context of that data. And it's going to adjust the weights of the model to get the best possible output. And this process repeats millions and millions of times over and over again until we reach some optimal quality. And then the final step is evaluation. A small amount of the data is set aside for evaluation. And the model is tested on this data set for performance. And then the model is adjusted if necessary. The metric used to determine the effectiveness of the model is called perplexity. It will compare two words based on their similarity. And it will give a good score if the words are related and a bad score if it's not. And then we also use RLHF, reinforcement learning through human feedback. And that's when users or testers actually test the model and provide positive or negative scores based on the output. And then once again, the model is adjusted as necessary. All right, let's talk about fine-tuning now, which I think a lot of you are going to be interested in because it's something that the average person can get into quite easily. So we have these popular large language models that are trained on massive sets of data to build general language capabilities. And these pre-trained models, like BERT, like GPT, give developers a head start versus training models from scratch. But then in comes fine-tuning, which allows us to take these raw models, these foundation models, and fine-tune them for our specific use cases. So let's think about an example. Let's say you want to fine-tune a model to be able to take pizza orders, to be able to have conversations, answer questions about pizza, and finally be able to allow customers to buy pizza. You can take a pre-existing set of conversations that exemplify the back and forth between a pizza shop and a customer, load that in, fine-tune a model, and then all of a sudden that model is going to be much better at having conversations about pizza ordering. The model updates the weights to be better at understanding certain pizza terminology, questions, responses, tone, everything. And fine-tuning is much faster than a full training, and it produces much higher accuracy. And fine-tuning allows pre-trained models to be fine-tuned for real-world use cases. And finally, you can take a single foundational model and fine-tune it any number of times for any number of use cases. And there are a lot of great services out there that allow you to do that. And again, it's all about the quality of your data. So if you have a really good data set that you're going to fine-tune a model on, the model is going to be really, really good. And conversely, if you have a poor quality data set, it's not going to perform as well. All right, let me pause for a second and talk about AI Camp. So as mentioned earlier, this video, all of its content, the animations, have been created in collaboration with students from AI Camp. AI Camp is a learning experience for students that are age 13 and above. You work in small, personalized groups with experienced mentors. You work together to create an AI product using NLP, computer vision, and data science. AI Camp has both a three-week and a one-week program during summer that requires zero programming experience. And they also have a new program which is 10 weeks long during the school\n",
      "### DOC 2\n",
      "\n",
      "My name is David Andrzej and in this video, I'll teach you how to fine-tune Lama3 so that it performs 10 times better for your specific use case. Let's start with what even is fine-tuning and I made this explanation in plain English so that anybody can understand. Fine-tuning is adapting a pre-trained LLM like GPT-4 or in this case Lama3 to a specific task or domain. It involves adjusting a small portion of the parameters on a more focused dataset. So, you know, when a new model releases, what everybody needs to know is how many parameters it has. We have Lama3 8B and always that number like 8B or 70B, that's the number of parameters. So we're adjusting just a small number of them to make it more focused on a specific thing. Fine-tuning customizes the outputs to be more relevant and accurate for your use case. Here's the power of fine-tuning. Cost-effectiveness. It leverages the power of pre-trained LLMs which cost tens of millions of dollars, if not hundreds of millions, to train and we can just, you know, run a GPU for a few hours and fine-tune something for, I don't know, like cents, a few cents or a few dollars at most, which is just amazing. It gives you improved performance because you can enhance the LLM on your dataset and improve accuracy for specific tasks. And it also is more data efficient. You can achieve excellent results even with smaller datasets. So, you know, maybe even like 300, 500 entries while, you know, Lama3 was trained on 15 trillion tokens. I don't know about you, but I don't have nearly as much data as Zack. So that's why fine-tuning is great for people like you and me. So how does LLM fine-tuning actually work? First, you need to prepare your dataset. And this, you know, depending on how hardcore you want to go, this can take anywhere from 20 minutes to a few hours to weeks, potentially. Depends how far you want to take it. So you create a smaller, high-quality dataset tailored to your specific use case and label it appropriately, which I'll teach you in a bit. The pre-trained LLMs weights are updated incrementally using the optimization algorithms like gradient descent based on the new dataset. So we can only fine-tune LLMs that we have access to the weights, meaning open-source, open-weights LLMs. You cannot fine-tune GPT-4 if you are not OpenAI. OpenAI can do it, obviously, but me and you, we probably don't have GPT-4 just laying on our computer. Then you monitor and refine. You evaluate the model's performance on a validation set, preventing overfitting and guide adjustments. Now, here are some real-world use cases for fine-tuning. Fine-tuning an LLM on customer service transcripts can create a chatbot, like this one, that can address issue in a way specific to your company. So let's say you have a specific product, very niche, that there is not much data about it on the internet. And if somebody messages your customer support email, you want your chatbot to respond in a specific way based on the information of your product. And that data is proprietary. It's private. Only you have it. And you can fine-tune an LLM to respond based on that data. So, like, technically, if you have enough scripts, you can fine-tune an LLM to respond like you. And, you know, if you try chat GPT, if you even give, like, chat GPT some writing and tell it, continue in this writing style, it's terrible. So this is where fine-tuning could be better. Tailored content generation. So you can fine-tune an LLM on your posts and descriptions to create engaging summaries or marketing copy, again, in your writing style, tailored to your audience. Domain-specific analysis. So fine-tuning LLM on legal or medical text can make it much better for those specific benchmarks. So you might have a model that, let's say, it reaches 50 on some arbitrary benchmark. With fine-tuning, it can reach 70 or 80. Now let's dive into how to actually implement this on Llama3. So I created this Google collab. Well, actually, most of it was created by Anslov team. A huge shout out to Anslov because they did all the heavy lifting. So I'm going to also link their GitHub below. Now, first off, I added a component that's only available in April to the community. So if you join during April, you will get a personalized AI strategy to future-proof yourself and your business. So if you want to be among people who are building the future, if you want access to all the different courses, modules, and everything else in the community, and to two weekly calls, then consider joining. And especially if you want me to give you a personalized AI strategy to future-proof yourself. So if that sounds interesting to you, make sure to join the community. It's the first link in the description. Now let's fine-tune LLama3, shall we? So first thing, we check the GPU version available in the environment and install specific dependencies that are comparable with the detected GPU to prevent conflicts. So this is this cell. By the way, if you don't know how Google Colab works, which is, you know, the software I'm using right now, it's super simple. It's basically splitting the code into cells. It's called the Jupyter Notebook, but it's like much more easier to see. You can add text, you can add graphics, and it's great for like tutorials and explaining, right? So if you never use this, it's great because it's free. And Google actually gives you a GPU so you can use this T4 GPU to train this model for free. And if you want faster, you can obviously upgrade it, right? So I'm going to link this Colab below the video as well. So we run this cell, which does what I just explained. The next cell, we need to prepare to load a range of quantized language models, including the new 15 trillion LLama3 model, so trained on 15 trillion tokens. And it's optimized for efficiency with 4-bit quantization. I mean, I'm not going to even pretend I know everything about fine-tuning because I don't. So if it seems like I have gaps in my knowledge, it's because it is. I do have those gaps in my knowledge. So I try to make it as simple as possible. But if this proves something, it proves that you don't have to be a machine learning expert to fine-tune models. So just follow along. So here, this is the max sequence length. Obviously, LLama3 is up to 8,000. So I mean, 2,000 is plenty for this demonstration, but you can do anything. You can do 4,000 or 8,000. Here, I use 4-bit quantization to reduce memory usage, but it can be false as well. So here are the models. We can see, like, we have Mistral7B, LLama2, which is the old one, Gemma from Google. But obviously, we're interested in LLama3 8B. And by the way, we can also use LLama3 70B if you want, which obviously will take longer because it's a much bigger model. So in that case, you might want to buy the premium version of Collab or just wait for a while. But yeah, I mean, everything is the same. Just here, you would change the model to LLama3 70B. And if you want to use, like, gated models from HuggingFace, which gated means that you have to usually agree to some, you know, license or whatever, then here, just remove the comment and then put your HuggingFace token here. Super simple. Now, by the way, you always have to run this. So what do you do when you go to Google Collab? You click on runtime and click run all. That way, all of the cells run. But you can also do it one by one by clicking this button right here next to each cell. And it needs to have this little tick, green tick. That way, it was executed. Here, it's not because I, you know, removed the... I changed this. So anytime you make any change, it disappears. But that doesn't matter. It was still executed. So it's stored in the runtime. Next up, we integrate LoRa. Again, you don't have to understand what this is, but it's basically a way of fine-tuning into our model, which allows us to efficiently update just a fraction of the parameters, enhancing training speed and reducing computation load. So again, we are not training the model from scratch. We're just fine-tuning a few parameters for our specific use case. And here, you can change the R to any number greater than 0, 8, 16, 32, 64, up to you. And your goals, what you want to do with it. By the way, on Sloth, the reason I'm using it is because it makes fine-tuning much faster and consuming less memory. So it's actually a great framework for this. Dataprep. We now use the Alpaca dataset from Yama, which is this one, which has 50,000 rows. And I have it loaded in VS code here. Just that way, you see how it looks like in JSON formatting. So, you know, it's a lot of lines because for everyone, it's basically times five. Yeah, so like 250,000 lines. And it's like every one of them has an instruction. I should probably zoom it in. So every one entry has an instruction. Give three tips for staying healthy. Input, this is not mandatory because instruction is already enough context. And then output, this is what the LLM should say. And you do this enough times and the LLM learns. It basically learns, right? So we can see it probably better here. And if you want to use your own dataset, you have to format it the same way. So, you know, just having output, input and instructions, these three parameters. But yeah, just look at this. Not all of them have the input, which is fine. I mean, probably like 20% or 15% have the input. And that's just extra context. So yeah, I'm also going to link this dataset below. But if you want your own dataset, which, you know, if you want your own use case, just make sure to format it the same way. So, you know, instruction, some text, input, some extra context or empty, and output, how the model should respond. And, you know, if you're getting creative, you can definitely use LLMs to generate these large datasets much faster. I mean, maybe you create really like 20 high quality examples by hand. And then you run a team of agents for creating that dataset that can just, you know, use those 20 examples to create 50,000. Like in this dataset. But yeah, that's a topic for a whole nother video. So if you want me to make a video on how to make datasets for fine tuning, then let me know. But let's go back to our Colab. So then we define a system prompt, which is, you know, custom instruction system prompt, which you already know, hopefully. That formats tasks into instruction inputs and responses. So this has to fit with our dataset. And we apply it to our dataset for the model. And we add the EOS token to signal completion. So this token right here, here we define it. And here we add it because without this, the token generation continues forever. So we don't want that, obviously. So let's look at the system prompt. It's very simple. It says below is an instruction that describes a task paired with an input that provides further context. Write a response that appropriately completes the request. And that's our system prompt. And then we feed it the instruction, the input and response. And obviously you can change the system prompt if you want. Now, train the model. We do a 60 step. We do only 60 steps here to speed things up. You can, like, this is obviously very small because it's not even one epoch, training epoch. So if you want to like actually use something for production or your business, you probably want to train it for longer than 60 steps. And I'm going to show you how in this bit. So if you do multiple epochs, you have to turn max steps none. So here, okay. Number of trained epochs is not included in here. So what you would do is you would copy this and you would go in here and look at the steps, right? So we have the steps here. You would add this. Maybe you would do four or whatever, however many you want. The more, the better. But at a certain point, it starts to not yield better result. So max steps, you have to change it to none, right? So it's like 60 right now. So you do none. And this is where you would do like proper fine tuning. But, you know, I just added that 60 for demonstration. That way it's faster. And it still took like eight minutes. So I'm not going to replicate it. I'm just going to show you everything. But yeah, basically, you know, this is what you do. You decide how many epochs you want. And then at this stage, we're configuring our model's training setup where we define things like batch size and learning rate to teach our model effectively with the data we've prepared. So obviously you can like mess with stuff here. Again, I'm not going to pretend I understand everything. But the main things are, you know, packing like this can make it five times faster for short sequences. Obviously, the steps and the epochs. But yeah, I mean, if you're confused something, just take a screenshot. Boom, like this. And ask ShedGPD. Now, this is the current memory stats, right? So we're using the Tesla T4 GPU provided from Google for free. And the max memory is 14 gigabytes. And this is where the training begins. This is the magical part, right? So here we do this line of code, trainer stats, trainer.train. And this will give us the statistics as the model trains. So again, this is only 60 steps, which is like zero epochs. But yeah, you can see the training loss going down. So like basically, smaller number is better here. So you can see like at the start, we have 1.8. Like 1.9, and then it quickly starts dropping to like 0.9, you know, around 1, 0.8. So it fluctuates a bit, but it consistently goes down 0.7. But you can see it's reaching like a asymptote, right? Obviously, it's only 60 steps. So it really doesn't mean anything. But yeah, like we ended up like 0.8 from like 2. So it shows you like if the model is actually improving. So it shows you like if the model is actually improving. And this took like eight minutes. So you can see the stats here, right? So 476 seconds, almost exactly eight minutes. Peak reserve memory was 8.9 gigabytes. And for training was 3.3 gigabytes. So not like this is the power of Unslof. It's like really optimized for this to use, to run faster and to use less memory. So that way we can fine tune GPUs for cheaper. I mean, you know, I'm using a free T4 GPU from Google. So it's free, but it's faster. Like if you didn't use Unslof, it would be a lot slower. So, okay, so 60% of, we used 60% of max memory. So that's good because we didn't like hit the limit. So we still have like 40% reserved. And for training, it was only 22%, which is even better. Inference, which means here we actually run our new model that we fine tuned. And okay, so this data set is for like instructions. And this is basically when you see a model that is like instruct at the end of it, this is what they mean. It's just trained on a large data set of instructions. Because usually the models are more for like chatting, for text generation. You know, you give it some input and it's like gives you some output. It's, you know, for more conversational. Here for instructions, for the instruct models is to follow instructions. You give it a task and it completes it. So like we can see it probably here in VS code, like rewrite the sentence to change its meaning and then output the thief escaped. Compared to data subs, so this is like all tasks. It's all in instructions. And then it shows how the model should do it. So let's look at it, right? So now we've trained the model. This took like eight minutes to do. So all of you can do this. The beauty of using a Google Cloud is that obviously it doesn't matter what machine you have. Even if you have a terrible computer, this will take the exact same time because you're using the GPU and cloud. So obviously here you can change your prompt. I mean, this is, you know, I changed the prompts here. So this is my prompt. But always make sure to leave the output blank. So here, the first one is the instruction. Then this is the input, like the extra added context and the output, leave it blank because the model will generate it, right? So list the prime numbers contained within\n",
      "### DOC 3\n",
      "\n",
      "Hey everyone, I'm Shaw and I'm back with a new data science series. In this new series, I'm going to be talking about large language models and how to use them in practice. In this video, I will give a beginner-friendly introduction to large language models and describe three levels of working with them in practice. Future videos in this series will discuss various practical aspects of large language models, things like using OpenAI's Python API, using open-source solutions like the Hugging Phase Transformers library, how to fine-tune large language models, and of course, how to build a large language model from scratch. If you enjoyed this content, please be sure to like, subscribe, and share with others. And if you have any suggestions for me to include in this series, please share those in the comments section below. And so with that, let's get into the video. So to kick off the video series, in this video, I'm going to be giving a practical introduction to large language models. And this is meant to be very beginner-friendly and high level, and I'll leave more technical details and example code for future videos and blogs in this series. So a natural place to start is, what is a large language model, or LLM for short? So I'm sure most people are familiar with ChatGPT, however, if you are enlightened enough to not keep up with new cycles and tech hype and all this kind of stuff, ChatGPT is essentially a very impressive and advanced chatbot. So if you go to the ChatGPT website, you can ask it questions like, what's a large language model? And it will generate a response very quickly, like the one that we are seeing here. And that is really impressive. Like if you were ever on AOL, Instant Messenger, also called AIM, you know, back in early 2000s or in the early days of the internet, there were chatbots then, there have been chatbots for a long time, but this one feels different. Like the text is very impressive and it almost feels human-like. A question you might have when you hear the term large language model is, what makes it large? What's the difference between a large language model and a not large language model? And this was exactly the question I had when I first heard the term. And so one way we can put it is that large language models are a special type of language model. But what makes them so special? And I'm sure there's a lot that can be said about large language models. But to keep things simple, I'm going to talk about two distinguishing properties. The first quantitative and the second qualitative. So first, quantitatively, large language models are large. They have many, many more model parameters than past language models. And so these days, this is anywhere from tens to hundreds of billions of parameters. The model parameters are numbers that define how the model will take an input and generate the output. So it's essentially the numbers that define the model itself. Okay, so that's a quantitative perspective of what distinguishes large language models from not large language models. But there's also this qualitative perspective and these so-called emergent properties that start to show up when language models become large. And so emergent properties is the language used in this paper cited below, a survey of large language models available in the archive. Really great beginner's guide, I recommend it. But essentially what this term means is there are properties in large language models that do not appear in smaller language models. And so one example of this is zero-shot learning. One definition of zero-shot learning is the capability of a machine learning model to complete a task it was not explicitly trained to do. So while this may not sound super impressive to us very smart and sophisticated humans, this is actually a major innovation in how these state-of-the-art machine learning models are developed. So to see this, we can compare the old state-of-the-art paradigm to this new state-of-the-art paradigm. The old way, and not too long ago, we can say like about 5, 10 years ago, the way the high-performing best machine learning models were developed was strictly through supervised learning. What this would typically look like is you would train a model on thousands, if not millions, of labeled examples. And so what this might have looked like is you have some input text like, hello, hola, how's it going, esta bien, so on and so forth. And you take all these examples and you manually assign a label to each example. Here we're labeling the language, so English, Spanish, so on. And so you can imagine that this would take a tremendous amount of human effort to get thousands, if not millions, of high-quality examples. So let's compare this to the more recent innovation with large language models who use a different paradigm. They use so-called self-supervised learning. What that looks like in the context of large language models is you train a very large model on a very large corpus of data. And so what this can look like is if you're trying to build a model that can do language classification, instead of painstakingly generating this labeled data set, you can just take a corpus of English text and a corpus of Spanish text and train a model in a self-supervised way. So in contrast to supervised learning, self-supervised learning does not require manual labeling of each example in your data set. The so-called labels or targets for the model are actually defined from the inherent structure of the data or this context of the text. So you might be thinking to yourself, how does this self-supervised learning actually work? And so one of the most popular ways that this is done is the next word prediction paradigm. So suppose we have this text, listen to your, and we want to predict what the next word would be. But clearly there's not just one word that can go after this string of words. There are actually many words you can put after this text and it would make sense. In this next word prediction paradigm, what the language model is trying to do is to predict the probability distribution of the next word given the previous words. What this might look like is listen to your heart might be the most probable next word, but another likely word could be gut or listen to your body or listen to your parents and listen to your grandma. And so this is essentially the core task that these large language models are trained to do. And the way the large language model will learn these probabilities is that it'll see so many examples in this massive corpus of text that it's trained on and it has a massive number of internal parameters so it can efficiently represent all the different statistical associations with different words. And an important point here is that context matters. If we simply added the word don't to the front of this string here and it changed it to don't listen to your, then this probability distribution could look entirely different because just by adding one word before this sentence, we completely change the meaning of the sentence. And so to put this a bit more mathematically, and I promise this is the most technical thing in this video, this is an example of a auto regression task. So auto meaning self, regression meaning you're trying to predict something. So what this notation means is what is the probability of the nth text or more technically the nth token given the preceding m token. So n minus one, n minus two, n minus three, so on and so forth. And so if you really want to boil everything down, this is the core task most large language models are doing. And somehow through this very simple task of predict the next word, we get this incredible performance from tools like chat GPT and other large language models. So now with that foundation set, hopefully you have a decent understanding of what large language models are and how they work and a broader context for them. Now let's talk about how we can use these in practice. Here I will talk about three levels in which we can use large language models. These three levels are ordered by the technical expertise and computational resources required. The most accessible way to use large language models is prompt engineering. Next we have model fine tuning. And then finally we have build your own large language model. So starting from level one, prompt engineering here, I have a pretty broad definition of prompt engineering. Here I define it as just using an LLM out of the box. So more specifically, not touching any of the model parameters. So of these tens of billions or hundreds of billions of parameters that define the model, we're not going to touch any of them. We're just going to leave them as is. Here I'll talk about two ways we can do this. One is the easy way and I'm sure is the way that most people in the world have interacted with large language models, which is using things like chat GPT. These are like intuitive user interfaces. They don't require any code and they're completely free. Someone can just go to the chat GPT website, type in a prompt and it'll spit out a response. So while this is definitely the easiest way to do it, it is a bit restrictive in that you have to go to their website. This doesn't really scale well if you're trying to build a product or service around it. But for a lot of use cases, this is actually super helpful. So for applications where the easy way doesn't cut it, there is the less easy way, which is using things like the open AI API or the hugging face transformers library. And these tools provide ways to interact with large language models programmatically. So essentially using Python. In the case of the open AI API, instead of typing your request in the chat GPT user interface, you can send it over to open AI using Python and their API and then you will get a response back. Of course, their API is not free, so you have to pay per API call. Another way we can do this is via open source solutions, one of which is the hugging face transformers library, which gives you easy access to open source large language models. So it's free and you can run these models locally. So no need to send your potentially proprietary or confidential information to a third party in open AI. So future videos of the series, we'll dive into all these different aspects. I'll talk about the open AI API, what it is, how it works, share example code. I'll dive into the hugging face transformers library, same situation. What the heck is it? How does it work? And then sharing some Python example code there. I'll also do a video talking about prompt engineering more generally. How can we create prompts to get good responses from large language models? And so while prompt engineering is the most accessible way to work with large language models, just working with a model out of the box may give you suboptimal performance on a specific task or use case. Or the model has really good performance, but it's massive. It has like a hundred billion parameters. So a question might be, is there a way we can use a smaller model, but kind of tweak it in a way to have good performance on our very narrow and specific use case. And so this brings us to level two, which is model fine tuning, which here I define as adjusting at least one internal model parameter for a particular task. And so here there are just generally two steps. One, you get a pre-trained large language model, maybe from open AI, or maybe an open source model from the hugging face transformers library. And then you update the model parameters given task specific examples. Kind of going back to the supervised learning versus self-supervised learning, the pre-trained model is going to be a self-supervised model. So it'll be trained on this simple word prediction task. But in step two, here's where we're going to do supervised learning or even reinforcement learning to tweak the model parameters for a specific use case. And so this turns out to work very well. Examples like ChatGPT, you're not working with the raw pre-trained model. The model that you are interacting with in ChatGPT is actually a fine-tuned model developed using reinforcement learning. And so a reason why this might work is that in doing this self-supervised task and doing the word prediction, the base model, this pre-trained large language model is learning useful representations for a wide variety of tasks. So in a future video, I will dive in more deeply into fine-tuning techniques. Another one is low rank adaptation or LORA for short. And then another popular one is reinforcement learning with human feedback or RLHF. And of course, there is a third step here. You'll deploy your fine-tuned large language model to do some kind of service or, you know, use it in your day-to-day life and you'll profit somehow. And so my sense is between prompt engineering and model fine-tuning, you can probably handle 99% of large language model use cases and applications. However, if you're a large organization, large enterprise, and security is a big concern. So you don't want to use open source models or you don't want to send data to a third party via an API. And maybe you want your large language model to be very good at a relatively specific set of tasks. You want to customize the training data in a very specific way and you want to own all the rights, have it for commercial use, all this kind of stuff. Then it can make sense to go one step further beyond model fine-tuning and build your own large language model. And so here I define it as just coming up with all the model parameters. So I'll just talk about how to do this at a very high level here, and I'll leave technical details for a future video in the series. First, we need to get our data. And so what this might look like is you'll get a book corpus, a Wikipedia corpus, and a Python corpus. And so this is billions of tokens of text. And then you will take that and pre-process it, refine it into your training data set. And then you can take the training data set and do the model training through self-supervised learning. And then out of that comes the pre-trained large language model. So you can take this as your starting point for level two and go from there. And so if you enjoyed this video and you want to read more, be sure to check out the blog in Towards Data Science. There I share some more details that I may have missed in this video. This series is both a video and blog series. So each video will have an associated blog, and there will also be tons of example code on the GitHub repository. The goal of the series is to really just make information about large language models much more accessible. I really do think this is the technological innovation of our time, and there are so many opportunities for potential use cases, applications, products, services that can come out of large language models. And that's something that I want to support. I think we'll be better off if more people understand this technology and are applying it to solving problems. So with that, be sure to hit the subscribe button to keep up with future videos in this series. If you have any questions or suggestions for other topics I should cover in this series, please drop those in the comment section below. And as always, thank you so much for your time and thanks for watching.\n",
      "### DOC 4\n",
      "\n",
      "It's probably the main two things it's used for. So if you want your model shared, then you would do that. But if you want it just saved on your computer, do saved pre-trained for a local save. By the way, this only saves the LoRa adapters, meaning basically the things that were changed. It doesn't save the entire model with the change parameters, just the changes, right? So it's less memory and just faster to save. But if you want to save the LoRa adapters with the saved model, you can change this. If you want to load the LoRa adapters, we saved for inference, you would change this false to true. So simply changing this. And yeah, this is the model name. So obviously, you can change this. This is your model used for training. Here is just LoRa model, but you can name it Lama3. I don't know, copywriting, or Lama3 medical diagnosis, whatever your use case is, obviously. And then here, the Alpaca prompt. So yeah, this is the variable we declared earlier. So this is the importance. You can't just go into the Colab and try to running this cell. You have to run the cells from above. Otherwise, this will not work. So whenever you're using a Jupyter Notebook, such as Google Colab, always run all cells in order. Otherwise, it will not work. So yeah. So this is the same format, right, from earlier instruction input-output. At this point, you should be familiar with this. And that's just for this particular data set and for this style of prompting. So if you have a different one, then follow the different one. So obviously, here, what is a famous tower in Paris? Obviously, it's Eiffel Tower, blah, blah, blah. It gives some extra info about it. So you can also use Hugging Face AutoModel for PerfCasualLM. But Unslof does not recommend this because it's a lot slower than Unslof. So yeah, if possible, use Unslof for speed. And as the name suggests of Unslof, it's unslowing everything. It's making everything two to five times faster. So why not do that with 80% less memory? So yeah. OK, and then we're preparing to save our trained model in a more compact format and then upload it into a cloud platform, which allows for less storage and compression power. So again, I'm not going to even pretend I understand everything because this is honestly stepping outside of my comfort zone. But just building this and doing this fine-tuning taught me a lot. So if you want more technical videos like this, let me know. Next, we're ready to compress our model using various quantization methods, which means just making it easier to run on a machine. So maybe you cannot, like if you have a bad computer, maybe you cannot run the full model, but you definitely can run a quantized version of it. It makes it leaner, and then we upload it to the cloud for easy sharing. And this is what this piece of code does. And so we use the model-unslof.gguf file or the quantized version. So the Q4 means quantized in LlamaCCP. Or if you want a UI-based system, which probably you do, which is easier to use, you can use GPT4 or this other one. It's escaping me. But yeah, these are basically these UI-based systems that you can use to LLM anything. Yeah, I don't know if this supports it. But yeah, basically, these frameworks have a UI that's easy to chat with. And you can use open source model there. So if you fine-tune this, you can upload this to GPT4 and chat with your own model very easily. And yeah, that's it. You know how to fine-tune Llama3 for your own specific use case. Again, I'm going to leave these resources below the video. And if you have any questions regarding to unslof, join their Discord. So yeah, that's it. If you find this useful, then please subscribe. And again, if you want, during April, which is, what, like eight days, nine days left, if you join the community, you will get a personalized AI strategy to future-proof yourself and your business. So if that sounds valuable to you, then make sure to join. It's the first link in the description. Thank you for watching.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "\n",
    "history_aware_retriever = RunnableBranch(\n",
    "    (\n",
    "        # Both empty string and empty list evaluate to False\n",
    "        lambda x: not x.get(\"chat_history\", False),\n",
    "        # If no chat history, then we just pass input to retriever\n",
    "        (lambda x: x[\"input\"]) | retriever,\n",
    "    ),\n",
    "    # If chat history, then we pass inputs to LLM chain, then to retriever\n",
    "    contextualize_q_prompt | llm | StrOutputParser() | retriever,\n",
    ").with_config(run_name=\"chat_retriever_chain\")\n",
    "\n",
    "resp = history_aware_retriever.invoke({\n",
    "    \"input\": \"What should I learn first If I want to build my own?\",\n",
    "    \"chat_history\": [HumanMessage(\"What are LLMs?\"), AIMessage(\"LLMs are specific models in Artificial Intelligence\")]\n",
    "})\n",
    "for i,doc in enumerate(resp):\n",
    "    print(f\"### DOC {i}\\n\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step VI - Create the question-answer prompt \n",
    "\n",
    "You finished the first step of the LLM app! ðŸ‘ Now let's tackle the next (and final) one! \n",
    "\n",
    "First, let's create a new system prompt that will simply tell our LLM to answer a question based on a given context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Answer question ###\n",
    "system_prompt = \"\"\"\n",
    "    You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer \n",
    "    the question. If you don't know the answer, say that you \n",
    "    don't know. Use three sentences maximum and keep the \n",
    "    answer concise.\n",
    "\n",
    "    Here is the user question:\n",
    "    {input}\n",
    "\n",
    "    Here is the context to help you answer:\n",
    "    {context}\n",
    "\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step VII - Create the question-answer chain \n",
    "\n",
    "Alright, that's the hardest part: **the chain**. Let's review what we need:\n",
    "\n",
    "* The chain should take a `context` and `input` \n",
    "* Both these variables should be passed through the whole chain as we will need it as output of our chain \n",
    "* The chain should output a dictionnary with:\n",
    "    * `context`\n",
    "    * `input`\n",
    "    * `answer`\n",
    "\n",
    "Now some hints ðŸ˜˜\n",
    "\n",
    "* If you want to pass variables through the chain you will need to use [`RunnablePassthrough`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html#langchain_core.runnables.passthrough.RunnablePassthrough)\n",
    "    * Use also the `.assign()` method to provide specific keys\n",
    "\n",
    "* Don't forget that you will also need the `format_docs` function and you will most likely need to wrap it up around the [`RunnableLambda`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html#langchain_core.runnables.base.RunnableLambda) to make it chainable\n",
    "\n",
    "* The question answer chain should contain the history retriever chain to work! (so part of your q&a chain should have the history aware retriever chain)\n",
    "\n",
    "This is the hardest part of this exercise but with the hint above you should be able to do it. Take some time to read the documentation carefully to really understand the concepts of chaining in Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY:input\n",
      "VALUE:What should I learn first If I want to build my own?\n",
      "KEY:chat_history\n",
      "VALUE:[HumanMessage(content='What are LLMs?', additional_kwargs={}, response_metadata={}), AIMessage(content='LLMs are specific models in Artificial Intelligence', additional_kwargs={}, response_metadata={})]\n",
      "KEY:context\n",
      "VALUE:So, let's get started, so I'll be talking about building LLMs today, so I think a lot of you have heard of LLMs before, but just as a quick recap, LLMs, standing for Large Language Models, are basically all the chatbots that you've been hearing about recently, so ChatGPT from OpenAI, Claude from Entropiq, Gemini, and Lama, and other type of models like this, and today we'll be talking about how do they actually work, so it's going to be an overview because it's only one lecture, and it's hard to compress everything, but hopefully I'll touch a little bit about all the components that are needed to train some of these LLMs. Also, if you have questions, please interrupt me and ask. If you have a question, most likely other people in the room or on Zoom have the same question, so please ask. Great, so what matters when training LLMs? So, there are a few key components that matter. One is the architecture, so as you probably all know, LLMs are neural networks, and when you think about neural networks, you have to think about what architecture you're using. Another component which is really important is the training loss and the training algorithm, so how you actually train these models. Then it's data, so what do you train these models on? The evaluation, which is how do you know whether you're actually making progress towards the goal of LLMs, and then the system component, so that is like how do you actually make these models run on modern hardware, which is really important because these models are really large, so now more than ever, systems are actually really an important topic for LLMs. So, those five components, you probably all know that LLMs, and if you don't know, LLMs are all based on transformers, or at least some version of transformers. I'm actually not going to talk about the architecture today, one, because I gave a lecture on transformers a few weeks ago, and two, because you can find so much information online on transformers, but I think you can- there's much less information about the other four topics, so I really want to talk about those. Another thing to say is that most of academia actually focuses on architecture and training algorithm and losses. As academics, and I've done that for a lot- a big part of my career, is simply we like thinking that this is like we make new architectures, new models, and it seems like it's very important, but in reality, honestly, what matters in practice is mostly the three other topics, so data, evaluation, and systems, which is what most of industry actually focuses on. So that's also one of the reasons why I don't want to talk too much about the architecture, because really the rest is super important. Great, so overview of the lecture, I'll be talking about pre-training. So pre-training, you probably heard that word, this is the general word, this is kind of the classical language modeling paradigm, where you basically train your language model to essentially model all of internet. And then there's a post-training, which is a more recent paradigm, which is taking these large language models and making them essentially AI assistants. So this is more of a recent trend since ChatGPT. So if you ever heard of GPT-3 or GPT-2, that's really pre-training land. If you heard of ChatGPT, which you probably have, this is really post-training land. So I'll be talking about both, but I'll start with pre-training. And specifically, I'll talk about what is the task of pre-training LLMs and what is the laws that people actually use. So language modeling, this is a quick recap. Language models at a high level are simply models of probability distribution over sequences of tokens or of words. So it's basically some model of p of x1 to xl, where x1 is basically word 1 and xl is the last word in the sequence or in the sentence. So very concretely, if you have a sentence like the mouse ate the cheese, what the language model gives you is simply a probability of this sentence being uttered by a human or being found online. So if you have another sentence like the mouse ate cheese, here there's grammatical mistakes. So the model should know that this should have some syntactic knowledge. So it should know that this has less likelihood of appearing online. If you have another sentence like the cheese ate the mouse, then the model should hopefully know about the fact that usually cheese don't eat mouse. So there's some semantic knowledge and this is less likely than the first sentence. So this is basically at a high level what language models are. One word that you probably have been hearing a lot in the news are generative models. So this is just something that can generate models that can generate sentences or can generate some data. The reason why we say language models are generative models is that once you have a model of a distribution, you can simply sample from this model and then we can generate data. So you can generate sentences using a language model. So the type of models that people are all currently using are what we call autoregressive language models. And the key idea of autoregressive language models is that you take this distribution over words and you basically decompose it into the distribution of the first word, multiply it by the distribution of the likelihood of the distribution of the second word given the first word, and multiply it by p of the third word given the first two words. So there's no approximation here. This is just the chain rule of probability, which hopefully you all know about, really no approximation. This is just one way of modeling a distribution. So slightly more concisely, you can write it as a product of p's of the next word given everything which happened in the past, so of the context. So this is what we call autoregressive language models. Again, this is really not the only way of modeling distribution, this is just one way. It has some benefits and some downsides. One downside of autoregressive language models is that when you actually sample from this autoregressive language model, you basically have a for loop which generates the next word, then conditions on that next word, and then regenerate the other word. So basically, if you have a longer sentence that you want to generate, it takes more time to generate it. So there are some downsides of this current paradigm, but that's what we currently have, so I'm going to talk about this one. Great. So autoregressive language models. At a high level, what the task of autoregressive language model is, is simply predicting the next word, as I just said. So if you have a sentence like she likely prefers, one potential next word might be dogs. And the way we do it is that we first tokenize. So you take these words or sub words, you tokenize them, and then you give an ID for each token. So here I have one, two, three. Then you pass it through this black box. As I already said, we're not going to talk about the architecture. You just pass it through a model, and you then get a distribution, a probability distribution over the next word, over the next token. And then you sample from this distribution, you get a new token, and then you de-tokenize. So you get a new ID, you de-tokenize, and that's how you basically sample from a language model. One thing which is important to note is that the last two steps are actually only needed during inference. When you do training, you just need to predict the most likely token, and you can just compare to the real token, which happened in practice, and then you basically change the weights of your model to increase the probability of generating that token. Great. So autoregressive neural language models. So to be slightly more specific, still without talking about the architecture, the first thing we do is that we have all of these- oh, sorry, yes? On the previous slide, when you're predicting the probability of the next token, does this mean that your final output vector has to be the same dimensionality as the number of tokens that you have? Yes. How do you deal with if you're adding more tokens to your co-presenters? Yeah. So we're going to talk about tokenization actually later, so you will get some sense of this. You basically can't deal with adding new tokens. I'm kind of exaggerating. There are methods for doing it, but essentially people don't do it. So it's really important to think about how you tokenize your text, and that's why we'll talk about that later. But it's a very good point to note is that basically the vocabulary size, so the number of tokens that you have, is essentially the output of your language model. So it's actually pretty large. Okay, so autoregressive neural language models. First thing you do is that you take every word or every token. You embed them, so you get some vector representation for each of these tokens. You pass them through some neural network, as we said, it's a transformer. Then you get a representation for all the words in the context. So it's basically a representation of the entire sentence. You pass it through a linear layer, as you just said, to basically map it to the number so that the output, the number of outputs is the number of tokens. You then pass it through some softmax, and you basically get a probability distribution over the next words given every word in the context. And the laws that you use is basically, it's essentially a task of classifying the next token. So it's a very simple kind of machine learning task. So you use the cross-entropy laws, where you basically look at the actual target that happened, which is a target distribution, which is a one-hot encoding, which here in this case says, I saw the real word that happened is cat. So that's a one-hot distribution over cat. And here, this is the distribution that you generated. And basically, you do cross-entropy, which really just increases the probability of generating cat and decreases the probability of generating all the other tokens. One thing to notice is that, as you all know, again, this is just equivalent to maximizing the text log likelihood, because you can just rewrite the max over the probability of this autoregressive language modeling task as just being this minimum of, I just added the log here and minus, which is just the minimum of the loss, which is the cross-entropy loss. So basically, minimizing the loss is the same thing as maximizing the likelihood of your text. Any questions? OK, tokenizer. So this is one thing that people usually don't talk that much about. Tokenizers are extremely important. So it's really important that you understand, at least, what they do at a high level. So why do we need tokenizers in the first place? First, it's more general than words. So one simple thing that you might think is, oh, we're just going to take every word that we will have. And you just say, every word is a token in its own. But then what happens is, if there's a typo in your word, then you might not have any token associated with this word with a typo. And then you don't know how to actually pass this word with a typo into a large language model. So what do you do next? And also, even if you think about words, words are fine with Latin-based languages. But if you think about a language like Thai, you won't have a simple way of tokenizing by spaces, because there are no spaces between words. So really, tokens are much more general than words. First thing. Second thing that you might think is that you might tokenize every sentence, character by character. You might say, A is one token, B is another token. That would actually work, and probably very well. The issue is that then your sequence becomes super long. And as you probably remember from the lecture on transformers, the complexity grows quadratically with the length of sequences. So you really don't want to have a super long sequence. So tokenizers basically try to deal with those two problems and give common sub-sequences a certain token. And usually, how you should be thinking about it is around an average of every token is around three, four letters. And there are many algorithms for tokenization. I'll just talk about one of them to give you a high level, which is what we call byte-pair encoding, which is actually pretty common, one of the two most common tokenizers. And the way that you train a tokenizer is that first, you start with a very large corpus of text. And here, I'm really not talking about training a large language model yet. This is purely for the tokenization step. So this is my large corpus of text with these five words. Then you associate every character in this corpus of text a different token. So here, I just split up every character with a different token, and I just color coded all of those tokens. And then what you do is that you go through your text, and every time you see pairs of tokens that are very common, the most common pair of token, you just merge them. So here, you see three times the tokens T and O next to each other, so you're just going to say this is a new token. And then you continue. You repeat that. So now you have T-O-K, TOK, which happens three times, TOK with an E, that happens two times, and TOKEN, which happens twice, and then EX, which also happens twice. So this is that if you were to train a tokenizer on this corpus of text, which is very small, that's how you would finish with a trained tokenizer. In reality, you do it on much larger corpuses of text. And this is the real tokenizer of, actually, I think this is GPT-3 or ChatGPT. And here, you see how it would actually separate these words. So basically, you see the same thing as what we gave in the previous example, TOKEN becomes its own token. So tokenizer is actually split up into two tokens, TOKEN and EIZER. So yeah, that's all about tokenizers. Any question on that? Yeah? How do you deal with spaces, and how do you deal with introduction? Yeah. So actually, there's a step before tokenizers, which is what we call pre-tokenizers, which is exactly what you just said. So this is mostly, in theory, there's no reason to deal with spaces and punctuation separately. You could just say every space gets its own token, every punctuation gets its own token, and you could just do all the merging. The problem is that, so there's an efficiency question. Actually, training these tokenizers takes a long time. So you're better off, because you have to consider every pair of token. So what you end up doing is saying, if there's a space, this is very, like pre-tokenizers are very English-specific. So you say, if there's a space, we're not going to start looking at the token that came before and the token that came afterwards. So you're not merging in between spaces. But this is just like a computation optimization. You could theoretically just deal with it the same way as you deal with any other character. Yeah? When you merge tokens, do you delete the tokens that you merged away, or do you keep the smaller tokens that you merged? You actually keep the smaller tokens. I mean, in reality, it doesn't matter much, because usually on large corpus of text, you will have actually everything. But you usually keep the small ones. And the reason why you want to do that is because if, in case there's a, as we said before, you have some grammatical mistakes or some typos, you still want to be able to represent these words by character. So yeah. Yes? Are the tokens unique? So, I mean, say, in this case, T-O-K-E-N, is there only one occurrence, or do you need to leave multiple occurrence so they can take on different meanings or something? Oh, oh, I see what you say. No, no, no. It's every token has its own unique ID. So a usual, this is a great question. For example, if you think about a bank, which could be bank for money or bank like water, they will have the same token, but the model will learn, the transformer will learn that based on the words that are around it, it should associate that, I'm saying, I'm being very hand-wavy here, but associate that with a representation that is either more like the bank money side or the bank water side. But that's the transformer that does that. It's not a tokenizer. Yes? Yeah, so you mentioned during tokenization, you keep the smaller tokens to start with, right? So if you start with a T, you keep the T, and then you tell your tokenizer to expand that amount of token. So let's say maybe you didn't train on token, but in your data, you are trying to encode token. So how does the tokenizer know to encode it with token or to do it with T? Yeah, it's a great question. You basically, when you, so when you tokenize, so that's after training of the tokenizer, when you actually apply the tokenizer, you basically always choose the largest token that you can apply. So if you can do token, you will never do T. You will always do token. But it's actually, so people don't usually talk that much about tokenizers, but there's a lot of computational benefits or computational tricks that you can do for making these things faster. So I really don't think we, and honestly, I think a lot of people think that we should just get away from tokenizers and just kind of tokenize\n",
      "\n",
      "This video is going to give you everything you need to go from knowing absolutely nothing about artificial intelligence and large language models to having a solid foundation of how these revolutionary technologies work. Over the past year, artificial intelligence has completely changed the world, with products like ChatGPT potentially appending every single industry and how people interact with technology in general. And in this video, I will be focusing on LLMs, how they work, ethical considerations, applications, and so much more. And this video was created in collaboration with an incredible program called AI Camp, in which high school students learn all about artificial intelligence. And I'll talk more about that later in the video. Let's go. So first, what is an LLM? Is it different from AI? And how is ChatGPT related to all of this? LLMs stand for large language models, which is a type of neural network that's trained on massive amounts of text data. It's generally trained on data that can be found online. Everything from web scraping to books to transcripts, anything that is text-based can be trained into a large language model. And taking a step back, what is a neural network? A neural network is essentially a series of algorithms that try to recognize patterns in data. And really what they're trying to do is simulate how the human brain works. And LLMs are a specific type of neural network that focus on understanding natural language. And as mentioned, LLMs learn by reading tons of books, articles, internet text, and there's really no limitation there. And so how do LLMs differ from traditional programming? Well, with traditional programming, it's instruction-based, which means if X, then Y. You're explicitly telling the computer what to do. You're giving it a set of instructions to execute. But with LLMs, it's a completely different story. You're teaching the computer not how to do things, but how to learn how to do things. And this is a much more flexible approach and is really good for a lot of different applications where previously traditional coding could not accomplish them. So one example application is image recognition. With image recognition, traditional programming would require you to hard code every single rule for how to, let's say, identify different letters. So A, B, C, D. But if you're handwriting these letters, everybody's handwritten letters look different. So how do you use traditional programming to identify every single possible variation? Well, that's where this AI approach comes in. Instead of giving a computer explicit instructions for how to identify a handwritten letter, you instead give it a bunch of examples of what handwritten letters look like, and then it can infer what a new handwritten letter looks like based on all of the examples that it has. What also sets machine learning and large language models apart in this new approach to programming is that they are much more flexible, much more adaptable, meaning they can learn from their mistakes and inaccuracies, and are thus so much more scalable than traditional programming. LLMs are incredibly powerful at a wide range of tasks, including summarization, text generation, creative writing, question and answer, programming, and if you've watched any of my videos, you know how powerful these large language models can be. And they're only getting better. Know that right now, large language models and AI in general are the worst they'll ever be. And as we're generating more data on the internet, and as we use synthetic data, which means data created by other large language models, these models are going to get better rapidly. And it's super exciting to think about what the future holds. Now let's talk a little bit about the history and evolution of large language models. We're going to cover just a few of the large language models today in this section. The history of LLMs traces all the way back to the ELISA model, which was from 1966, which was really the first language model. It had pre-programmed answers based on keywords. It had a very limited understanding of the English language. And like many early language models, you started to see holes in its logic after a few back and forths in a conversation. And then after that, language models really didn't evolve for a very long time. Although technically the first recurrent neural network was created in 1924 or RNN, they weren't really able to learn until 1972. And these new learning language models are a series of neural networks with layers and weights and a whole bunch of stuff that I'm not going to get into in this video. And RNNs were really the first technology that was able to predict the next word in a sentence rather than having everything pre-programmed for it. And that was really the basis for how current large language models work. And even after this and the advent of deep learning in the early 2000s, the field of AI evolved very slowly, with language models far behind what we see today. This all changed in 2017, where the Google DeepMind team released a research paper about a new technology called Transformers. And this paper was called Attention is All You Need. And a quick side note, I don't think Google even knew quite what they had published at that time. But that same paper is what led OpenAI to develop ChatGPT. So obviously other computer scientists saw the potential for the Transformers architecture. With this new Transformers architecture, it was far more advanced. It required decreased training time, and it had many other features like self-attention, which I'll cover later in this video. Transformers allowed for pre-trained large language models like GPT-1, which was developed by OpenAI in 2018. It had 117 million parameters, and it was completely revolutionary, but soon to be outclassed by other LLMs. Then after that, BERT was released, B-E-R-T, in 2018. That had 340 million parameters, and had bidirectionality, which means it had the ability to process text in both directions, which helped it have a better understanding of context. And as comparison, a unidirectional model only has an understanding of the words that came before the target text. And after this, LLMs didn't develop a lot of new technology, but they did increase greatly in scale. GPT-2 was released in early 2019, and had 2.5 billion parameters. Then GPT-3 in June of 2020, with 175 billion parameters. And it was at this point that the public started noticing large language models. GPT had a much better understanding of natural language than any of its predecessors. And this is the type of model that powers ChatGPT, which is probably the model that you're most familiar with. And ChatGPT became so popular because it was so much more accurate than anything anyone had ever seen before. And it was really because of its size. And because it was now built into this chatbot format, anybody could jump in and really understand how to interact with this model. ChatGPT 3.5 came out in December of 2022, and started this current wave of AI that we see today. Then in March 2023, GPT-4 was released, and it was incredible, and still is incredible to this day. It had a whopping reported 1.76 trillion parameters, and uses likely a mixture of experts approach, which means it has multiple models that are all fine-tuned for specific use cases. And then when somebody asks a question to it, it chooses which of those models to use. And then they added multi-modality and a bunch of other features. And that brings us to where we are today. All right, now let's talk about how LLMs actually work in a little bit more detail. The process of how large language models work can be split into three steps. The first of these steps is called tokenization. And there are neural networks that are trained to split long text into individual tokens. And a token is essentially about three fourths of a word. So if it's a shorter word like hi, or that, or there, it's probably just one token. But if you have a longer word like summarization, it's going to be split into multiple pieces. And the way that tokenization happens is actually different for every model. Some of them separate prefixes and suffixes. Let's look at an example. What is the tallest building? So what is the tallest building? Are all separate tokens. And so that separates the suffix off of tallest, but not building because it is taking the context into account. And this step is done so models can understand each word individually, just like humans. We understand each word individually and as groupings of words. And then the second step of LLMs is something called embeddings. The large language models turns those tokens into embedding vectors, turning those tokens into essentially a bunch of numerical representations of those tokens numbers. And this makes it significantly easier for the computer to read and understand each word and how the different words relate to each other. And these numbers all correspond with the position in an embeddings vector database. And then the final step in the process is transformers, which we'll get to in a little bit. But first, let's talk about vector databases. And I'm going to use the terms word and token interchangeably. So just keep that in mind because they're almost the same thing, not quite, but almost. And so these word embeddings that I've been talking about are placed into something called a vector database. These databases are storage and retrieval mechanisms that are highly optimized for vectors. And again, those are just numbers, long series of numbers. Because they're converted into these vectors, they can easily see which words are related to other words based on how similar they are, how close they are based on their embeddings. And that is how the large language model is able to predict the next word based on the previous words. Vector databases capture the relationship between data as vectors in multi-dimensional space. I know that sounds complicated, but it's really just a lot of numbers. Vectors are objects with a magnitude and a direction, which both influence how similar one vector is to another. And that is how LLMs represent words based on those numbers. Each word gets turned into a vector, capturing semantic meaning and its relationship to other words. So here's an example. The words book and worm, which independently might not look like they're related to each other, but they are related concepts because they frequently appear together. A bookworm, somebody who likes to read a lot. And because of that, they will have embeddings that look close to each other. And so models build up an understanding of natural language using these embeddings and looking for similarity of different words, terms, groupings of words, and all of these nuanced relationships. And the vector format helps models understand natural language better than other formats. And you can kind of think of all this like a map. If you have a map with two landmarks that are close to each other, they're likely going to have very similar coordinates. So it's kind of like that. Okay, now let's talk about transformers. Matrix representations can be made out of those vectors that we were just talking about. This is done by extracting some information out of the numbers and placing all of the information into a matrix through an algorithm called multi-head attention. The output of the multi-head attention algorithm is a set of numbers which tells the model how much the words and its order are contributing to the sentence as a whole. We transform the input matrix into an output matrix which will then correspond with a word having the same values as that output matrix. So basically we're taking that input matrix, converting it into an output matrix, and then converting it into natural language. And the word is the final output of this whole process. This transformation is done by the algorithm that was created during the training process. So the model's understanding of how to do this transformation is based on all of its knowledge that it was trained with, all of that text data from the internet, from books, from articles, etc. And it learned which sequences of words go together and their corresponding next words based on the weights determined during training. Transformers use an attention mechanism to understand the context of words within a sentence. It involves calculations with the dot product, which is essentially a number representing how much the word contributed to the sentence. It will find the difference between the dot products of words and give it correspondingly large values for attention. And it will take that word into account more if it has higher attention. Now let's talk about how large language models actually get trained. The first step of training a large language model is collecting the data. You need a lot of data. When I say billions of parameters, that is just a measure of how much data is actually going into training these models. And you need to find a really good data set. If you have really bad data going into a model, then you're going to have a really bad model. Garbage in, garbage out. So if a data set is incomplete or biased, the large language model will be also. And data sets are huge. We're talking about massive, massive amounts of data. They take data in from web pages, from books, from conversations, from reddit posts, from x posts, from youtube transcriptions. Basically anywhere where we can get some text data, that data is becoming so valuable. Let me put into context how massive the data sets we're talking about really are. So here's a little bit of text which is 276 tokens. That's it. Now if we zoom out, that one pixel is that many tokens. And now here's a representation of 285 million tokens, which is 0.02% of the 1.3 trillion tokens that some large language models take to train. And there's an entire science behind data pre-processing, which prepares the data to be used to train a model. Everything from looking at the data quality, to labeling consistency, data cleaning, data transformation, and data reduction. But I'm not going to go too deep into that. And this pre-processing can take a long time. And it depends on the type of machine being used, how much processing power you have, the size of the data set, the number of pre-processing steps, and a whole bunch of other factors that make it really difficult to know exactly how long pre-processing is going to take. But one thing that we know takes a long time is the actual training. Companies like NVIDIA are building hardware specifically tailored for the math behind large language models. And this hardware is constantly getting better. The software used to process these models are getting better also. And so the total time to process models is decreasing, but the size of the models is increasing. And to train these models, it is extremely expensive because you need a lot of processing power, electricity, and these chips are not cheap. And that is why NVIDIA stock price has skyrocketed. Their revenue growth has been extraordinary. And so with the process of training, we take this pre-processed text data that we talked about earlier, and it's fed into the model. And then using transformers, or whatever technology a model is actually based on, but most likely transformers, it will try to predict the next word based on the context of that data. And it's going to adjust the weights of the model to get the best possible output. And this process repeats millions and millions of times over and over again until we reach some optimal quality. And then the final step is evaluation. A small amount of the data is set aside for evaluation. And the model is tested on this data set for performance. And then the model is adjusted if necessary. The metric used to determine the effectiveness of the model is called perplexity. It will compare two words based on their similarity. And it will give a good score if the words are related and a bad score if it's not. And then we also use RLHF, reinforcement learning through human feedback. And that's when users or testers actually test the model and provide positive or negative scores based on the output. And then once again, the model is adjusted as necessary. All right, let's talk about fine-tuning now, which I think a lot of you are going to be interested in because it's something that the average person can get into quite easily. So we have these popular large language models that are trained on massive sets of data to build general language capabilities. And these pre-trained models, like BERT, like GPT, give developers a head start versus training models from scratch. But then in comes fine-tuning, which allows us to take these raw models, these foundation models, and fine-tune them for our specific use cases. So let's think about an example. Let's say you want to fine-tune a model to be able to take pizza orders, to be able to have conversations, answer questions about pizza, and finally be able to allow customers to buy pizza. You can take a pre-existing set of conversations that exemplify the back and forth between a pizza shop and a customer, load that in, fine-tune a model, and then all of a sudden that model is going to be much better at having conversations about pizza ordering. The model updates the weights to be better at understanding certain pizza terminology, questions, responses, tone, everything. And fine-tuning is much faster than a full training, and it produces much higher accuracy. And fine-tuning allows pre-trained models to be fine-tuned for real-world use cases. And finally, you can take a single foundational model and fine-tune it any number of times for any number of use cases. And there are a lot of great services out there that allow you to do that. And again, it's all about the quality of your data. So if you have a really good data set that you're going to fine-tune a model on, the model is going to be really, really good. And conversely, if you have a poor quality data set, it's not going to perform as well. All right, let me pause for a second and talk about AI Camp. So as mentioned earlier, this video, all of its content, the animations, have been created in collaboration with students from AI Camp. AI Camp is a learning experience for students that are age 13 and above. You work in small, personalized groups with experienced mentors. You work together to create an AI product using NLP, computer vision, and data science. AI Camp has both a three-week and a one-week program during summer that requires zero programming experience. And they also have a new program which is 10 weeks long during the school\n",
      "\n",
      "My name is David Andrzej and in this video, I'll teach you how to fine-tune Lama3 so that it performs 10 times better for your specific use case. Let's start with what even is fine-tuning and I made this explanation in plain English so that anybody can understand. Fine-tuning is adapting a pre-trained LLM like GPT-4 or in this case Lama3 to a specific task or domain. It involves adjusting a small portion of the parameters on a more focused dataset. So, you know, when a new model releases, what everybody needs to know is how many parameters it has. We have Lama3 8B and always that number like 8B or 70B, that's the number of parameters. So we're adjusting just a small number of them to make it more focused on a specific thing. Fine-tuning customizes the outputs to be more relevant and accurate for your use case. Here's the power of fine-tuning. Cost-effectiveness. It leverages the power of pre-trained LLMs which cost tens of millions of dollars, if not hundreds of millions, to train and we can just, you know, run a GPU for a few hours and fine-tune something for, I don't know, like cents, a few cents or a few dollars at most, which is just amazing. It gives you improved performance because you can enhance the LLM on your dataset and improve accuracy for specific tasks. And it also is more data efficient. You can achieve excellent results even with smaller datasets. So, you know, maybe even like 300, 500 entries while, you know, Lama3 was trained on 15 trillion tokens. I don't know about you, but I don't have nearly as much data as Zack. So that's why fine-tuning is great for people like you and me. So how does LLM fine-tuning actually work? First, you need to prepare your dataset. And this, you know, depending on how hardcore you want to go, this can take anywhere from 20 minutes to a few hours to weeks, potentially. Depends how far you want to take it. So you create a smaller, high-quality dataset tailored to your specific use case and label it appropriately, which I'll teach you in a bit. The pre-trained LLMs weights are updated incrementally using the optimization algorithms like gradient descent based on the new dataset. So we can only fine-tune LLMs that we have access to the weights, meaning open-source, open-weights LLMs. You cannot fine-tune GPT-4 if you are not OpenAI. OpenAI can do it, obviously, but me and you, we probably don't have GPT-4 just laying on our computer. Then you monitor and refine. You evaluate the model's performance on a validation set, preventing overfitting and guide adjustments. Now, here are some real-world use cases for fine-tuning. Fine-tuning an LLM on customer service transcripts can create a chatbot, like this one, that can address issue in a way specific to your company. So let's say you have a specific product, very niche, that there is not much data about it on the internet. And if somebody messages your customer support email, you want your chatbot to respond in a specific way based on the information of your product. And that data is proprietary. It's private. Only you have it. And you can fine-tune an LLM to respond based on that data. So, like, technically, if you have enough scripts, you can fine-tune an LLM to respond like you. And, you know, if you try chat GPT, if you even give, like, chat GPT some writing and tell it, continue in this writing style, it's terrible. So this is where fine-tuning could be better. Tailored content generation. So you can fine-tune an LLM on your posts and descriptions to create engaging summaries or marketing copy, again, in your writing style, tailored to your audience. Domain-specific analysis. So fine-tuning LLM on legal or medical text can make it much better for those specific benchmarks. So you might have a model that, let's say, it reaches 50 on some arbitrary benchmark. With fine-tuning, it can reach 70 or 80. Now let's dive into how to actually implement this on Llama3. So I created this Google collab. Well, actually, most of it was created by Anslov team. A huge shout out to Anslov because they did all the heavy lifting. So I'm going to also link their GitHub below. Now, first off, I added a component that's only available in April to the community. So if you join during April, you will get a personalized AI strategy to future-proof yourself and your business. So if you want to be among people who are building the future, if you want access to all the different courses, modules, and everything else in the community, and to two weekly calls, then consider joining. And especially if you want me to give you a personalized AI strategy to future-proof yourself. So if that sounds interesting to you, make sure to join the community. It's the first link in the description. Now let's fine-tune LLama3, shall we? So first thing, we check the GPU version available in the environment and install specific dependencies that are comparable with the detected GPU to prevent conflicts. So this is this cell. By the way, if you don't know how Google Colab works, which is, you know, the software I'm using right now, it's super simple. It's basically splitting the code into cells. It's called the Jupyter Notebook, but it's like much more easier to see. You can add text, you can add graphics, and it's great for like tutorials and explaining, right? So if you never use this, it's great because it's free. And Google actually gives you a GPU so you can use this T4 GPU to train this model for free. And if you want faster, you can obviously upgrade it, right? So I'm going to link this Colab below the video as well. So we run this cell, which does what I just explained. The next cell, we need to prepare to load a range of quantized language models, including the new 15 trillion LLama3 model, so trained on 15 trillion tokens. And it's optimized for efficiency with 4-bit quantization. I mean, I'm not going to even pretend I know everything about fine-tuning because I don't. So if it seems like I have gaps in my knowledge, it's because it is. I do have those gaps in my knowledge. So I try to make it as simple as possible. But if this proves something, it proves that you don't have to be a machine learning expert to fine-tune models. So just follow along. So here, this is the max sequence length. Obviously, LLama3 is up to 8,000. So I mean, 2,000 is plenty for this demonstration, but you can do anything. You can do 4,000 or 8,000. Here, I use 4-bit quantization to reduce memory usage, but it can be false as well. So here are the models. We can see, like, we have Mistral7B, LLama2, which is the old one, Gemma from Google. But obviously, we're interested in LLama3 8B. And by the way, we can also use LLama3 70B if you want, which obviously will take longer because it's a much bigger model. So in that case, you might want to buy the premium version of Collab or just wait for a while. But yeah, I mean, everything is the same. Just here, you would change the model to LLama3 70B. And if you want to use, like, gated models from HuggingFace, which gated means that you have to usually agree to some, you know, license or whatever, then here, just remove the comment and then put your HuggingFace token here. Super simple. Now, by the way, you always have to run this. So what do you do when you go to Google Collab? You click on runtime and click run all. That way, all of the cells run. But you can also do it one by one by clicking this button right here next to each cell. And it needs to have this little tick, green tick. That way, it was executed. Here, it's not because I, you know, removed the... I changed this. So anytime you make any change, it disappears. But that doesn't matter. It was still executed. So it's stored in the runtime. Next up, we integrate LoRa. Again, you don't have to understand what this is, but it's basically a way of fine-tuning into our model, which allows us to efficiently update just a fraction of the parameters, enhancing training speed and reducing computation load. So again, we are not training the model from scratch. We're just fine-tuning a few parameters for our specific use case. And here, you can change the R to any number greater than 0, 8, 16, 32, 64, up to you. And your goals, what you want to do with it. By the way, on Sloth, the reason I'm using it is because it makes fine-tuning much faster and consuming less memory. So it's actually a great framework for this. Dataprep. We now use the Alpaca dataset from Yama, which is this one, which has 50,000 rows. And I have it loaded in VS code here. Just that way, you see how it looks like in JSON formatting. So, you know, it's a lot of lines because for everyone, it's basically times five. Yeah, so like 250,000 lines. And it's like every one of them has an instruction. I should probably zoom it in. So every one entry has an instruction. Give three tips for staying healthy. Input, this is not mandatory because instruction is already enough context. And then output, this is what the LLM should say. And you do this enough times and the LLM learns. It basically learns, right? So we can see it probably better here. And if you want to use your own dataset, you have to format it the same way. So, you know, just having output, input and instructions, these three parameters. But yeah, just look at this. Not all of them have the input, which is fine. I mean, probably like 20% or 15% have the input. And that's just extra context. So yeah, I'm also going to link this dataset below. But if you want your own dataset, which, you know, if you want your own use case, just make sure to format it the same way. So, you know, instruction, some text, input, some extra context or empty, and output, how the model should respond. And, you know, if you're getting creative, you can definitely use LLMs to generate these large datasets much faster. I mean, maybe you create really like 20 high quality examples by hand. And then you run a team of agents for creating that dataset that can just, you know, use those 20 examples to create 50,000. Like in this dataset. But yeah, that's a topic for a whole nother video. So if you want me to make a video on how to make datasets for fine tuning, then let me know. But let's go back to our Colab. So then we define a system prompt, which is, you know, custom instruction system prompt, which you already know, hopefully. That formats tasks into instruction inputs and responses. So this has to fit with our dataset. And we apply it to our dataset for the model. And we add the EOS token to signal completion. So this token right here, here we define it. And here we add it because without this, the token generation continues forever. So we don't want that, obviously. So let's look at the system prompt. It's very simple. It says below is an instruction that describes a task paired with an input that provides further context. Write a response that appropriately completes the request. And that's our system prompt. And then we feed it the instruction, the input and response. And obviously you can change the system prompt if you want. Now, train the model. We do a 60 step. We do only 60 steps here to speed things up. You can, like, this is obviously very small because it's not even one epoch, training epoch. So if you want to like actually use something for production or your business, you probably want to train it for longer than 60 steps. And I'm going to show you how in this bit. So if you do multiple epochs, you have to turn max steps none. So here, okay. Number of trained epochs is not included in here. So what you would do is you would copy this and you would go in here and look at the steps, right? So we have the steps here. You would add this. Maybe you would do four or whatever, however many you want. The more, the better. But at a certain point, it starts to not yield better result. So max steps, you have to change it to none, right? So it's like 60 right now. So you do none. And this is where you would do like proper fine tuning. But, you know, I just added that 60 for demonstration. That way it's faster. And it still took like eight minutes. So I'm not going to replicate it. I'm just going to show you everything. But yeah, basically, you know, this is what you do. You decide how many epochs you want. And then at this stage, we're configuring our model's training setup where we define things like batch size and learning rate to teach our model effectively with the data we've prepared. So obviously you can like mess with stuff here. Again, I'm not going to pretend I understand everything. But the main things are, you know, packing like this can make it five times faster for short sequences. Obviously, the steps and the epochs. But yeah, I mean, if you're confused something, just take a screenshot. Boom, like this. And ask ShedGPD. Now, this is the current memory stats, right? So we're using the Tesla T4 GPU provided from Google for free. And the max memory is 14 gigabytes. And this is where the training begins. This is the magical part, right? So here we do this line of code, trainer stats, trainer.train. And this will give us the statistics as the model trains. So again, this is only 60 steps, which is like zero epochs. But yeah, you can see the training loss going down. So like basically, smaller number is better here. So you can see like at the start, we have 1.8. Like 1.9, and then it quickly starts dropping to like 0.9, you know, around 1, 0.8. So it fluctuates a bit, but it consistently goes down 0.7. But you can see it's reaching like a asymptote, right? Obviously, it's only 60 steps. So it really doesn't mean anything. But yeah, like we ended up like 0.8 from like 2. So it shows you like if the model is actually improving. So it shows you like if the model is actually improving. And this took like eight minutes. So you can see the stats here, right? So 476 seconds, almost exactly eight minutes. Peak reserve memory was 8.9 gigabytes. And for training was 3.3 gigabytes. So not like this is the power of Unslof. It's like really optimized for this to use, to run faster and to use less memory. So that way we can fine tune GPUs for cheaper. I mean, you know, I'm using a free T4 GPU from Google. So it's free, but it's faster. Like if you didn't use Unslof, it would be a lot slower. So, okay, so 60% of, we used 60% of max memory. So that's good because we didn't like hit the limit. So we still have like 40% reserved. And for training, it was only 22%, which is even better. Inference, which means here we actually run our new model that we fine tuned. And okay, so this data set is for like instructions. And this is basically when you see a model that is like instruct at the end of it, this is what they mean. It's just trained on a large data set of instructions. Because usually the models are more for like chatting, for text generation. You know, you give it some input and it's like gives you some output. It's, you know, for more conversational. Here for instructions, for the instruct models is to follow instructions. You give it a task and it completes it. So like we can see it probably here in VS code, like rewrite the sentence to change its meaning and then output the thief escaped. Compared to data subs, so this is like all tasks. It's all in instructions. And then it shows how the model should do it. So let's look at it, right? So now we've trained the model. This took like eight minutes to do. So all of you can do this. The beauty of using a Google Cloud is that obviously it doesn't matter what machine you have. Even if you have a terrible computer, this will take the exact same time because you're using the GPU and cloud. So obviously here you can change your prompt. I mean, this is, you know, I changed the prompts here. So this is my prompt. But always make sure to leave the output blank. So here, the first one is the instruction. Then this is the input, like the extra added context and the output, leave it blank because the model will generate it, right? So list the prime numbers contained within\n",
      "\n",
      "Hey everyone, I'm Shaw and I'm back with a new data science series. In this new series, I'm going to be talking about large language models and how to use them in practice. In this video, I will give a beginner-friendly introduction to large language models and describe three levels of working with them in practice. Future videos in this series will discuss various practical aspects of large language models, things like using OpenAI's Python API, using open-source solutions like the Hugging Phase Transformers library, how to fine-tune large language models, and of course, how to build a large language model from scratch. If you enjoyed this content, please be sure to like, subscribe, and share with others. And if you have any suggestions for me to include in this series, please share those in the comments section below. And so with that, let's get into the video. So to kick off the video series, in this video, I'm going to be giving a practical introduction to large language models. And this is meant to be very beginner-friendly and high level, and I'll leave more technical details and example code for future videos and blogs in this series. So a natural place to start is, what is a large language model, or LLM for short? So I'm sure most people are familiar with ChatGPT, however, if you are enlightened enough to not keep up with new cycles and tech hype and all this kind of stuff, ChatGPT is essentially a very impressive and advanced chatbot. So if you go to the ChatGPT website, you can ask it questions like, what's a large language model? And it will generate a response very quickly, like the one that we are seeing here. And that is really impressive. Like if you were ever on AOL, Instant Messenger, also called AIM, you know, back in early 2000s or in the early days of the internet, there were chatbots then, there have been chatbots for a long time, but this one feels different. Like the text is very impressive and it almost feels human-like. A question you might have when you hear the term large language model is, what makes it large? What's the difference between a large language model and a not large language model? And this was exactly the question I had when I first heard the term. And so one way we can put it is that large language models are a special type of language model. But what makes them so special? And I'm sure there's a lot that can be said about large language models. But to keep things simple, I'm going to talk about two distinguishing properties. The first quantitative and the second qualitative. So first, quantitatively, large language models are large. They have many, many more model parameters than past language models. And so these days, this is anywhere from tens to hundreds of billions of parameters. The model parameters are numbers that define how the model will take an input and generate the output. So it's essentially the numbers that define the model itself. Okay, so that's a quantitative perspective of what distinguishes large language models from not large language models. But there's also this qualitative perspective and these so-called emergent properties that start to show up when language models become large. And so emergent properties is the language used in this paper cited below, a survey of large language models available in the archive. Really great beginner's guide, I recommend it. But essentially what this term means is there are properties in large language models that do not appear in smaller language models. And so one example of this is zero-shot learning. One definition of zero-shot learning is the capability of a machine learning model to complete a task it was not explicitly trained to do. So while this may not sound super impressive to us very smart and sophisticated humans, this is actually a major innovation in how these state-of-the-art machine learning models are developed. So to see this, we can compare the old state-of-the-art paradigm to this new state-of-the-art paradigm. The old way, and not too long ago, we can say like about 5, 10 years ago, the way the high-performing best machine learning models were developed was strictly through supervised learning. What this would typically look like is you would train a model on thousands, if not millions, of labeled examples. And so what this might have looked like is you have some input text like, hello, hola, how's it going, esta bien, so on and so forth. And you take all these examples and you manually assign a label to each example. Here we're labeling the language, so English, Spanish, so on. And so you can imagine that this would take a tremendous amount of human effort to get thousands, if not millions, of high-quality examples. So let's compare this to the more recent innovation with large language models who use a different paradigm. They use so-called self-supervised learning. What that looks like in the context of large language models is you train a very large model on a very large corpus of data. And so what this can look like is if you're trying to build a model that can do language classification, instead of painstakingly generating this labeled data set, you can just take a corpus of English text and a corpus of Spanish text and train a model in a self-supervised way. So in contrast to supervised learning, self-supervised learning does not require manual labeling of each example in your data set. The so-called labels or targets for the model are actually defined from the inherent structure of the data or this context of the text. So you might be thinking to yourself, how does this self-supervised learning actually work? And so one of the most popular ways that this is done is the next word prediction paradigm. So suppose we have this text, listen to your, and we want to predict what the next word would be. But clearly there's not just one word that can go after this string of words. There are actually many words you can put after this text and it would make sense. In this next word prediction paradigm, what the language model is trying to do is to predict the probability distribution of the next word given the previous words. What this might look like is listen to your heart might be the most probable next word, but another likely word could be gut or listen to your body or listen to your parents and listen to your grandma. And so this is essentially the core task that these large language models are trained to do. And the way the large language model will learn these probabilities is that it'll see so many examples in this massive corpus of text that it's trained on and it has a massive number of internal parameters so it can efficiently represent all the different statistical associations with different words. And an important point here is that context matters. If we simply added the word don't to the front of this string here and it changed it to don't listen to your, then this probability distribution could look entirely different because just by adding one word before this sentence, we completely change the meaning of the sentence. And so to put this a bit more mathematically, and I promise this is the most technical thing in this video, this is an example of a auto regression task. So auto meaning self, regression meaning you're trying to predict something. So what this notation means is what is the probability of the nth text or more technically the nth token given the preceding m token. So n minus one, n minus two, n minus three, so on and so forth. And so if you really want to boil everything down, this is the core task most large language models are doing. And somehow through this very simple task of predict the next word, we get this incredible performance from tools like chat GPT and other large language models. So now with that foundation set, hopefully you have a decent understanding of what large language models are and how they work and a broader context for them. Now let's talk about how we can use these in practice. Here I will talk about three levels in which we can use large language models. These three levels are ordered by the technical expertise and computational resources required. The most accessible way to use large language models is prompt engineering. Next we have model fine tuning. And then finally we have build your own large language model. So starting from level one, prompt engineering here, I have a pretty broad definition of prompt engineering. Here I define it as just using an LLM out of the box. So more specifically, not touching any of the model parameters. So of these tens of billions or hundreds of billions of parameters that define the model, we're not going to touch any of them. We're just going to leave them as is. Here I'll talk about two ways we can do this. One is the easy way and I'm sure is the way that most people in the world have interacted with large language models, which is using things like chat GPT. These are like intuitive user interfaces. They don't require any code and they're completely free. Someone can just go to the chat GPT website, type in a prompt and it'll spit out a response. So while this is definitely the easiest way to do it, it is a bit restrictive in that you have to go to their website. This doesn't really scale well if you're trying to build a product or service around it. But for a lot of use cases, this is actually super helpful. So for applications where the easy way doesn't cut it, there is the less easy way, which is using things like the open AI API or the hugging face transformers library. And these tools provide ways to interact with large language models programmatically. So essentially using Python. In the case of the open AI API, instead of typing your request in the chat GPT user interface, you can send it over to open AI using Python and their API and then you will get a response back. Of course, their API is not free, so you have to pay per API call. Another way we can do this is via open source solutions, one of which is the hugging face transformers library, which gives you easy access to open source large language models. So it's free and you can run these models locally. So no need to send your potentially proprietary or confidential information to a third party in open AI. So future videos of the series, we'll dive into all these different aspects. I'll talk about the open AI API, what it is, how it works, share example code. I'll dive into the hugging face transformers library, same situation. What the heck is it? How does it work? And then sharing some Python example code there. I'll also do a video talking about prompt engineering more generally. How can we create prompts to get good responses from large language models? And so while prompt engineering is the most accessible way to work with large language models, just working with a model out of the box may give you suboptimal performance on a specific task or use case. Or the model has really good performance, but it's massive. It has like a hundred billion parameters. So a question might be, is there a way we can use a smaller model, but kind of tweak it in a way to have good performance on our very narrow and specific use case. And so this brings us to level two, which is model fine tuning, which here I define as adjusting at least one internal model parameter for a particular task. And so here there are just generally two steps. One, you get a pre-trained large language model, maybe from open AI, or maybe an open source model from the hugging face transformers library. And then you update the model parameters given task specific examples. Kind of going back to the supervised learning versus self-supervised learning, the pre-trained model is going to be a self-supervised model. So it'll be trained on this simple word prediction task. But in step two, here's where we're going to do supervised learning or even reinforcement learning to tweak the model parameters for a specific use case. And so this turns out to work very well. Examples like ChatGPT, you're not working with the raw pre-trained model. The model that you are interacting with in ChatGPT is actually a fine-tuned model developed using reinforcement learning. And so a reason why this might work is that in doing this self-supervised task and doing the word prediction, the base model, this pre-trained large language model is learning useful representations for a wide variety of tasks. So in a future video, I will dive in more deeply into fine-tuning techniques. Another one is low rank adaptation or LORA for short. And then another popular one is reinforcement learning with human feedback or RLHF. And of course, there is a third step here. You'll deploy your fine-tuned large language model to do some kind of service or, you know, use it in your day-to-day life and you'll profit somehow. And so my sense is between prompt engineering and model fine-tuning, you can probably handle 99% of large language model use cases and applications. However, if you're a large organization, large enterprise, and security is a big concern. So you don't want to use open source models or you don't want to send data to a third party via an API. And maybe you want your large language model to be very good at a relatively specific set of tasks. You want to customize the training data in a very specific way and you want to own all the rights, have it for commercial use, all this kind of stuff. Then it can make sense to go one step further beyond model fine-tuning and build your own large language model. And so here I define it as just coming up with all the model parameters. So I'll just talk about how to do this at a very high level here, and I'll leave technical details for a future video in the series. First, we need to get our data. And so what this might look like is you'll get a book corpus, a Wikipedia corpus, and a Python corpus. And so this is billions of tokens of text. And then you will take that and pre-process it, refine it into your training data set. And then you can take the training data set and do the model training through self-supervised learning. And then out of that comes the pre-trained large language model. So you can take this as your starting point for level two and go from there. And so if you enjoyed this video and you want to read more, be sure to check out the blog in Towards Data Science. There I share some more details that I may have missed in this video. This series is both a video and blog series. So each video will have an associated blog, and there will also be tons of example code on the GitHub repository. The goal of the series is to really just make information about large language models much more accessible. I really do think this is the technological innovation of our time, and there are so many opportunities for potential use cases, applications, products, services that can come out of large language models. And that's something that I want to support. I think we'll be better off if more people understand this technology and are applying it to solving problems. So with that, be sure to hit the subscribe button to keep up with future videos in this series. If you have any questions or suggestions for other topics I should cover in this series, please drop those in the comment section below. And as always, thank you so much for your time and thanks for watching.\n",
      "\n",
      "It's probably the main two things it's used for. So if you want your model shared, then you would do that. But if you want it just saved on your computer, do saved pre-trained for a local save. By the way, this only saves the LoRa adapters, meaning basically the things that were changed. It doesn't save the entire model with the change parameters, just the changes, right? So it's less memory and just faster to save. But if you want to save the LoRa adapters with the saved model, you can change this. If you want to load the LoRa adapters, we saved for inference, you would change this false to true. So simply changing this. And yeah, this is the model name. So obviously, you can change this. This is your model used for training. Here is just LoRa model, but you can name it Lama3. I don't know, copywriting, or Lama3 medical diagnosis, whatever your use case is, obviously. And then here, the Alpaca prompt. So yeah, this is the variable we declared earlier. So this is the importance. You can't just go into the Colab and try to running this cell. You have to run the cells from above. Otherwise, this will not work. So whenever you're using a Jupyter Notebook, such as Google Colab, always run all cells in order. Otherwise, it will not work. So yeah. So this is the same format, right, from earlier instruction input-output. At this point, you should be familiar with this. And that's just for this particular data set and for this style of prompting. So if you have a different one, then follow the different one. So obviously, here, what is a famous tower in Paris? Obviously, it's Eiffel Tower, blah, blah, blah. It gives some extra info about it. So you can also use Hugging Face AutoModel for PerfCasualLM. But Unslof does not recommend this because it's a lot slower than Unslof. So yeah, if possible, use Unslof for speed. And as the name suggests of Unslof, it's unslowing everything. It's making everything two to five times faster. So why not do that with 80% less memory? So yeah. OK, and then we're preparing to save our trained model in a more compact format and then upload it into a cloud platform, which allows for less storage and compression power. So again, I'm not going to even pretend I understand everything because this is honestly stepping outside of my comfort zone. But just building this and doing this fine-tuning taught me a lot. So if you want more technical videos like this, let me know. Next, we're ready to compress our model using various quantization methods, which means just making it easier to run on a machine. So maybe you cannot, like if you have a bad computer, maybe you cannot run the full model, but you definitely can run a quantized version of it. It makes it leaner, and then we upload it to the cloud for easy sharing. And this is what this piece of code does. And so we use the model-unslof.gguf file or the quantized version. So the Q4 means quantized in LlamaCCP. Or if you want a UI-based system, which probably you do, which is easier to use, you can use GPT4 or this other one. It's escaping me. But yeah, these are basically these UI-based systems that you can use to LLM anything. Yeah, I don't know if this supports it. But yeah, basically, these frameworks have a UI that's easy to chat with. And you can use open source model there. So if you fine-tune this, you can upload this to GPT4 and chat with your own model very easily. And yeah, that's it. You know how to fine-tune Llama3 for your own specific use case. Again, I'm going to leave these resources below the video. And if you have any questions regarding to unslof, join their Discord. So yeah, that's it. If you find this useful, then please subscribe. And again, if you want, during April, which is, what, like eight days, nine days left, if you join the community, you will get a personalized AI strategy to future-proof yourself and your business. So if that sounds valuable to you, then make sure to join. It's the first link in the description. Thank you for watching.\n",
      "KEY:answer\n",
      "VALUE:To build your own Large Language Model (LLM), you should start by understanding the fundamental components and processes involved. Here are the key steps:\n",
      "\n",
      "1. **Understand the Basics**: Learn about neural networks, specifically transformers, as LLMs are based on this architecture. Familiarize yourself with concepts like tokenization, embeddings, and the training process.\n",
      "\n",
      "2. **Data Preparation**: Collect and preprocess a large dataset of text. This dataset will be used to train your model. Ensure the data is diverse and relevant to the tasks you want your LLM to perform.\n",
      "\n",
      "3. **Training**: Use the prepared data to train your model. This involves defining the model architecture, setting up the training algorithm, and iteratively updating the model's parameters to minimize the training loss.\n",
      "\n",
      "For more detailed guidance, consider following tutorials or courses that walk you through these steps in depth.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "### Answer question ###\n",
    "system_prompt = \"\"\"\n",
    "    You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer \n",
    "    the question. If you don't know the answer, say that you \n",
    "    don't know. Use three sentences maximum and keep the \n",
    "    answer concise.\n",
    "\n",
    "    Here is the user question:\n",
    "    {input}\n",
    "\n",
    "    Here is the context to help you answer:\n",
    "    {context}\n",
    "\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# This is the most complicated part as you most likely never seen it \n",
    "# Basically we wrap the whole chain using RunnablePassthrough so that \n",
    "# the data from the beginning of the chain is passed through \n",
    "# the end of the chain \n",
    "# Each of the parameters in the .assign() methods are actually the keys of the future dictionnary \n",
    "# that will be output at the end of the chain\n",
    "question_answer_chain = RunnablePassthrough.assign(\n",
    "    context=history_aware_retriever | RunnableLambda(format_docs),\n",
    "    input=lambda x: x[\"input\"]\n",
    ").assign(answer=qa_prompt | llm | StrOutputParser())\n",
    "\n",
    "\n",
    "resp = question_answer_chain.invoke({\n",
    "    \"input\": \"What should I learn first If I want to build my own?\",\n",
    "    \"chat_history\": [HumanMessage(\"What are LLMs?\"), AIMessage(\"LLMs are specific models in Artificial Intelligence\")]\n",
    "})\n",
    "\n",
    "for key, value in resp.items():\n",
    "    print(f\"KEY:{key}\")\n",
    "    print(f\"VALUE:{value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step VIII - Create the graph\n",
    "\n",
    "Alright you've done the hardest part! Now all we have to do is to incorporate the final chain in a LangGraph! The only thing that is new is that we want to have custom values to be monitored (not just the messages history). Therefore you will need to build **a custom state**. To do so:\n",
    "\n",
    "* You will need to create a new `State` class that will inherit the `TypeDict` class \n",
    "    * This will allow you to output all parameters as dictionnary keys \n",
    "\n",
    "* This `State` should have the following attributes:\n",
    "    * input (as string)\n",
    "    * `chat_history` which should be a `Sequence` of `BaseMessage`\n",
    "        * Also use the prebuilt `add_message` method from `langgraph.graph.message` \n",
    "    * `context` (as string)\n",
    "    * `answer` (as string)\n",
    "\n",
    "The rest of the code should be the same as what you've learned so far about LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "# LangGraph uses the concepts of graphs which corresponds to a workflow \n",
    "# The first thing you need to do is to instanciate that graph using StateGraph.\n",
    "# StateGraph needs to be provided a schema meaning the data it is expected to handle that is called states\n",
    "# A State corresponds to the data stored at a given moment in your graph as well as functions (called \"reducers\") \n",
    "# which purpose is to update the State.\n",
    "# In our case, we use MessagesState which is pre-configured State meant for messages\n",
    "\n",
    "### Statefully manage chat history ###\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    chat_history: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    context: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    print(state)\n",
    "    response = question_answer_chain.invoke(state)\n",
    "    return {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(state[\"input\"]),\n",
    "            AIMessage(response[\"answer\"]),\n",
    "        ],\n",
    "        \"context\": response[\"context\"],\n",
    "        \"answer\": response[\"answer\"],\n",
    "    }\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step IX - Test your app \n",
    "\n",
    "Now it's time to test your application! Try it ðŸ¤—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Hi my name is Obi-Wan, what is the purpose of an LLM?', 'chat_history': []}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Hi my name is Obi-Wan, what is the purpose of an LLM?',\n",
       " 'chat_history': [HumanMessage(content='Hi my name is Obi-Wan, what is the purpose of an LLM?', additional_kwargs={}, response_metadata={}, id='df4b4ce7-3a69-4865-b825-902c8c2a9c06'),\n",
       "  AIMessage(content='The purpose of a Large Language Model (LLM) is to understand and generate human language based on patterns it has learned from vast amounts of text data. LLMs are a type of neural network designed to handle a wide range of tasks, including summarization, text generation, creative writing, question-answering, programming, and more. They are trained on massive datasets from various sources like web pages, books, and transcripts, allowing them to learn and adapt to different contexts and tasks.', additional_kwargs={}, response_metadata={}, id='9f050a0f-10d9-4255-8424-207ec27a7305')],\n",
       " 'context': \"This video is going to give you everything you need to go from knowing absolutely nothing about artificial intelligence and large language models to having a solid foundation of how these revolutionary technologies work. Over the past year, artificial intelligence has completely changed the world, with products like ChatGPT potentially appending every single industry and how people interact with technology in general. And in this video, I will be focusing on LLMs, how they work, ethical considerations, applications, and so much more. And this video was created in collaboration with an incredible program called AI Camp, in which high school students learn all about artificial intelligence. And I'll talk more about that later in the video. Let's go. So first, what is an LLM? Is it different from AI? And how is ChatGPT related to all of this? LLMs stand for large language models, which is a type of neural network that's trained on massive amounts of text data. It's generally trained on data that can be found online. Everything from web scraping to books to transcripts, anything that is text-based can be trained into a large language model. And taking a step back, what is a neural network? A neural network is essentially a series of algorithms that try to recognize patterns in data. And really what they're trying to do is simulate how the human brain works. And LLMs are a specific type of neural network that focus on understanding natural language. And as mentioned, LLMs learn by reading tons of books, articles, internet text, and there's really no limitation there. And so how do LLMs differ from traditional programming? Well, with traditional programming, it's instruction-based, which means if X, then Y. You're explicitly telling the computer what to do. You're giving it a set of instructions to execute. But with LLMs, it's a completely different story. You're teaching the computer not how to do things, but how to learn how to do things. And this is a much more flexible approach and is really good for a lot of different applications where previously traditional coding could not accomplish them. So one example application is image recognition. With image recognition, traditional programming would require you to hard code every single rule for how to, let's say, identify different letters. So A, B, C, D. But if you're handwriting these letters, everybody's handwritten letters look different. So how do you use traditional programming to identify every single possible variation? Well, that's where this AI approach comes in. Instead of giving a computer explicit instructions for how to identify a handwritten letter, you instead give it a bunch of examples of what handwritten letters look like, and then it can infer what a new handwritten letter looks like based on all of the examples that it has. What also sets machine learning and large language models apart in this new approach to programming is that they are much more flexible, much more adaptable, meaning they can learn from their mistakes and inaccuracies, and are thus so much more scalable than traditional programming. LLMs are incredibly powerful at a wide range of tasks, including summarization, text generation, creative writing, question and answer, programming, and if you've watched any of my videos, you know how powerful these large language models can be. And they're only getting better. Know that right now, large language models and AI in general are the worst they'll ever be. And as we're generating more data on the internet, and as we use synthetic data, which means data created by other large language models, these models are going to get better rapidly. And it's super exciting to think about what the future holds. Now let's talk a little bit about the history and evolution of large language models. We're going to cover just a few of the large language models today in this section. The history of LLMs traces all the way back to the ELISA model, which was from 1966, which was really the first language model. It had pre-programmed answers based on keywords. It had a very limited understanding of the English language. And like many early language models, you started to see holes in its logic after a few back and forths in a conversation. And then after that, language models really didn't evolve for a very long time. Although technically the first recurrent neural network was created in 1924 or RNN, they weren't really able to learn until 1972. And these new learning language models are a series of neural networks with layers and weights and a whole bunch of stuff that I'm not going to get into in this video. And RNNs were really the first technology that was able to predict the next word in a sentence rather than having everything pre-programmed for it. And that was really the basis for how current large language models work. And even after this and the advent of deep learning in the early 2000s, the field of AI evolved very slowly, with language models far behind what we see today. This all changed in 2017, where the Google DeepMind team released a research paper about a new technology called Transformers. And this paper was called Attention is All You Need. And a quick side note, I don't think Google even knew quite what they had published at that time. But that same paper is what led OpenAI to develop ChatGPT. So obviously other computer scientists saw the potential for the Transformers architecture. With this new Transformers architecture, it was far more advanced. It required decreased training time, and it had many other features like self-attention, which I'll cover later in this video. Transformers allowed for pre-trained large language models like GPT-1, which was developed by OpenAI in 2018. It had 117 million parameters, and it was completely revolutionary, but soon to be outclassed by other LLMs. Then after that, BERT was released, B-E-R-T, in 2018. That had 340 million parameters, and had bidirectionality, which means it had the ability to process text in both directions, which helped it have a better understanding of context. And as comparison, a unidirectional model only has an understanding of the words that came before the target text. And after this, LLMs didn't develop a lot of new technology, but they did increase greatly in scale. GPT-2 was released in early 2019, and had 2.5 billion parameters. Then GPT-3 in June of 2020, with 175 billion parameters. And it was at this point that the public started noticing large language models. GPT had a much better understanding of natural language than any of its predecessors. And this is the type of model that powers ChatGPT, which is probably the model that you're most familiar with. And ChatGPT became so popular because it was so much more accurate than anything anyone had ever seen before. And it was really because of its size. And because it was now built into this chatbot format, anybody could jump in and really understand how to interact with this model. ChatGPT 3.5 came out in December of 2022, and started this current wave of AI that we see today. Then in March 2023, GPT-4 was released, and it was incredible, and still is incredible to this day. It had a whopping reported 1.76 trillion parameters, and uses likely a mixture of experts approach, which means it has multiple models that are all fine-tuned for specific use cases. And then when somebody asks a question to it, it chooses which of those models to use. And then they added multi-modality and a bunch of other features. And that brings us to where we are today. All right, now let's talk about how LLMs actually work in a little bit more detail. The process of how large language models work can be split into three steps. The first of these steps is called tokenization. And there are neural networks that are trained to split long text into individual tokens. And a token is essentially about three fourths of a word. So if it's a shorter word like hi, or that, or there, it's probably just one token. But if you have a longer word like summarization, it's going to be split into multiple pieces. And the way that tokenization happens is actually different for every model. Some of them separate prefixes and suffixes. Let's look at an example. What is the tallest building? So what is the tallest building? Are all separate tokens. And so that separates the suffix off of tallest, but not building because it is taking the context into account. And this step is done so models can understand each word individually, just like humans. We understand each word individually and as groupings of words. And then the second step of LLMs is something called embeddings. The large language models turns those tokens into embedding vectors, turning those tokens into essentially a bunch of numerical representations of those tokens numbers. And this makes it significantly easier for the computer to read and understand each word and how the different words relate to each other. And these numbers all correspond with the position in an embeddings vector database. And then the final step in the process is transformers, which we'll get to in a little bit. But first, let's talk about vector databases. And I'm going to use the terms word and token interchangeably. So just keep that in mind because they're almost the same thing, not quite, but almost. And so these word embeddings that I've been talking about are placed into something called a vector database. These databases are storage and retrieval mechanisms that are highly optimized for vectors. And again, those are just numbers, long series of numbers. Because they're converted into these vectors, they can easily see which words are related to other words based on how similar they are, how close they are based on their embeddings. And that is how the large language model is able to predict the next word based on the previous words. Vector databases capture the relationship between data as vectors in multi-dimensional space. I know that sounds complicated, but it's really just a lot of numbers. Vectors are objects with a magnitude and a direction, which both influence how similar one vector is to another. And that is how LLMs represent words based on those numbers. Each word gets turned into a vector, capturing semantic meaning and its relationship to other words. So here's an example. The words book and worm, which independently might not look like they're related to each other, but they are related concepts because they frequently appear together. A bookworm, somebody who likes to read a lot. And because of that, they will have embeddings that look close to each other. And so models build up an understanding of natural language using these embeddings and looking for similarity of different words, terms, groupings of words, and all of these nuanced relationships. And the vector format helps models understand natural language better than other formats. And you can kind of think of all this like a map. If you have a map with two landmarks that are close to each other, they're likely going to have very similar coordinates. So it's kind of like that. Okay, now let's talk about transformers. Matrix representations can be made out of those vectors that we were just talking about. This is done by extracting some information out of the numbers and placing all of the information into a matrix through an algorithm called multi-head attention. The output of the multi-head attention algorithm is a set of numbers which tells the model how much the words and its order are contributing to the sentence as a whole. We transform the input matrix into an output matrix which will then correspond with a word having the same values as that output matrix. So basically we're taking that input matrix, converting it into an output matrix, and then converting it into natural language. And the word is the final output of this whole process. This transformation is done by the algorithm that was created during the training process. So the model's understanding of how to do this transformation is based on all of its knowledge that it was trained with, all of that text data from the internet, from books, from articles, etc. And it learned which sequences of words go together and their corresponding next words based on the weights determined during training. Transformers use an attention mechanism to understand the context of words within a sentence. It involves calculations with the dot product, which is essentially a number representing how much the word contributed to the sentence. It will find the difference between the dot products of words and give it correspondingly large values for attention. And it will take that word into account more if it has higher attention. Now let's talk about how large language models actually get trained. The first step of training a large language model is collecting the data. You need a lot of data. When I say billions of parameters, that is just a measure of how much data is actually going into training these models. And you need to find a really good data set. If you have really bad data going into a model, then you're going to have a really bad model. Garbage in, garbage out. So if a data set is incomplete or biased, the large language model will be also. And data sets are huge. We're talking about massive, massive amounts of data. They take data in from web pages, from books, from conversations, from reddit posts, from x posts, from youtube transcriptions. Basically anywhere where we can get some text data, that data is becoming so valuable. Let me put into context how massive the data sets we're talking about really are. So here's a little bit of text which is 276 tokens. That's it. Now if we zoom out, that one pixel is that many tokens. And now here's a representation of 285 million tokens, which is 0.02% of the 1.3 trillion tokens that some large language models take to train. And there's an entire science behind data pre-processing, which prepares the data to be used to train a model. Everything from looking at the data quality, to labeling consistency, data cleaning, data transformation, and data reduction. But I'm not going to go too deep into that. And this pre-processing can take a long time. And it depends on the type of machine being used, how much processing power you have, the size of the data set, the number of pre-processing steps, and a whole bunch of other factors that make it really difficult to know exactly how long pre-processing is going to take. But one thing that we know takes a long time is the actual training. Companies like NVIDIA are building hardware specifically tailored for the math behind large language models. And this hardware is constantly getting better. The software used to process these models are getting better also. And so the total time to process models is decreasing, but the size of the models is increasing. And to train these models, it is extremely expensive because you need a lot of processing power, electricity, and these chips are not cheap. And that is why NVIDIA stock price has skyrocketed. Their revenue growth has been extraordinary. And so with the process of training, we take this pre-processed text data that we talked about earlier, and it's fed into the model. And then using transformers, or whatever technology a model is actually based on, but most likely transformers, it will try to predict the next word based on the context of that data. And it's going to adjust the weights of the model to get the best possible output. And this process repeats millions and millions of times over and over again until we reach some optimal quality. And then the final step is evaluation. A small amount of the data is set aside for evaluation. And the model is tested on this data set for performance. And then the model is adjusted if necessary. The metric used to determine the effectiveness of the model is called perplexity. It will compare two words based on their similarity. And it will give a good score if the words are related and a bad score if it's not. And then we also use RLHF, reinforcement learning through human feedback. And that's when users or testers actually test the model and provide positive or negative scores based on the output. And then once again, the model is adjusted as necessary. All right, let's talk about fine-tuning now, which I think a lot of you are going to be interested in because it's something that the average person can get into quite easily. So we have these popular large language models that are trained on massive sets of data to build general language capabilities. And these pre-trained models, like BERT, like GPT, give developers a head start versus training models from scratch. But then in comes fine-tuning, which allows us to take these raw models, these foundation models, and fine-tune them for our specific use cases. So let's think about an example. Let's say you want to fine-tune a model to be able to take pizza orders, to be able to have conversations, answer questions about pizza, and finally be able to allow customers to buy pizza. You can take a pre-existing set of conversations that exemplify the back and forth between a pizza shop and a customer, load that in, fine-tune a model, and then all of a sudden that model is going to be much better at having conversations about pizza ordering. The model updates the weights to be better at understanding certain pizza terminology, questions, responses, tone, everything. And fine-tuning is much faster than a full training, and it produces much higher accuracy. And fine-tuning allows pre-trained models to be fine-tuned for real-world use cases. And finally, you can take a single foundational model and fine-tune it any number of times for any number of use cases. And there are a lot of great services out there that allow you to do that. And again, it's all about the quality of your data. So if you have a really good data set that you're going to fine-tune a model on, the model is going to be really, really good. And conversely, if you have a poor quality data set, it's not going to perform as well. All right, let me pause for a second and talk about AI Camp. So as mentioned earlier, this video, all of its content, the animations, have been created in collaboration with students from AI Camp. AI Camp is a learning experience for students that are age 13 and above. You work in small, personalized groups with experienced mentors. You work together to create an AI product using NLP, computer vision, and data science. AI Camp has both a three-week and a one-week program during summer that requires zero programming experience. And they also have a new program which is 10 weeks long during the school\\n\\nSo, let's get started, so I'll be talking about building LLMs today, so I think a lot of you have heard of LLMs before, but just as a quick recap, LLMs, standing for Large Language Models, are basically all the chatbots that you've been hearing about recently, so ChatGPT from OpenAI, Claude from Entropiq, Gemini, and Lama, and other type of models like this, and today we'll be talking about how do they actually work, so it's going to be an overview because it's only one lecture, and it's hard to compress everything, but hopefully I'll touch a little bit about all the components that are needed to train some of these LLMs. Also, if you have questions, please interrupt me and ask. If you have a question, most likely other people in the room or on Zoom have the same question, so please ask. Great, so what matters when training LLMs? So, there are a few key components that matter. One is the architecture, so as you probably all know, LLMs are neural networks, and when you think about neural networks, you have to think about what architecture you're using. Another component which is really important is the training loss and the training algorithm, so how you actually train these models. Then it's data, so what do you train these models on? The evaluation, which is how do you know whether you're actually making progress towards the goal of LLMs, and then the system component, so that is like how do you actually make these models run on modern hardware, which is really important because these models are really large, so now more than ever, systems are actually really an important topic for LLMs. So, those five components, you probably all know that LLMs, and if you don't know, LLMs are all based on transformers, or at least some version of transformers. I'm actually not going to talk about the architecture today, one, because I gave a lecture on transformers a few weeks ago, and two, because you can find so much information online on transformers, but I think you can- there's much less information about the other four topics, so I really want to talk about those. Another thing to say is that most of academia actually focuses on architecture and training algorithm and losses. As academics, and I've done that for a lot- a big part of my career, is simply we like thinking that this is like we make new architectures, new models, and it seems like it's very important, but in reality, honestly, what matters in practice is mostly the three other topics, so data, evaluation, and systems, which is what most of industry actually focuses on. So that's also one of the reasons why I don't want to talk too much about the architecture, because really the rest is super important. Great, so overview of the lecture, I'll be talking about pre-training. So pre-training, you probably heard that word, this is the general word, this is kind of the classical language modeling paradigm, where you basically train your language model to essentially model all of internet. And then there's a post-training, which is a more recent paradigm, which is taking these large language models and making them essentially AI assistants. So this is more of a recent trend since ChatGPT. So if you ever heard of GPT-3 or GPT-2, that's really pre-training land. If you heard of ChatGPT, which you probably have, this is really post-training land. So I'll be talking about both, but I'll start with pre-training. And specifically, I'll talk about what is the task of pre-training LLMs and what is the laws that people actually use. So language modeling, this is a quick recap. Language models at a high level are simply models of probability distribution over sequences of tokens or of words. So it's basically some model of p of x1 to xl, where x1 is basically word 1 and xl is the last word in the sequence or in the sentence. So very concretely, if you have a sentence like the mouse ate the cheese, what the language model gives you is simply a probability of this sentence being uttered by a human or being found online. So if you have another sentence like the mouse ate cheese, here there's grammatical mistakes. So the model should know that this should have some syntactic knowledge. So it should know that this has less likelihood of appearing online. If you have another sentence like the cheese ate the mouse, then the model should hopefully know about the fact that usually cheese don't eat mouse. So there's some semantic knowledge and this is less likely than the first sentence. So this is basically at a high level what language models are. One word that you probably have been hearing a lot in the news are generative models. So this is just something that can generate models that can generate sentences or can generate some data. The reason why we say language models are generative models is that once you have a model of a distribution, you can simply sample from this model and then we can generate data. So you can generate sentences using a language model. So the type of models that people are all currently using are what we call autoregressive language models. And the key idea of autoregressive language models is that you take this distribution over words and you basically decompose it into the distribution of the first word, multiply it by the distribution of the likelihood of the distribution of the second word given the first word, and multiply it by p of the third word given the first two words. So there's no approximation here. This is just the chain rule of probability, which hopefully you all know about, really no approximation. This is just one way of modeling a distribution. So slightly more concisely, you can write it as a product of p's of the next word given everything which happened in the past, so of the context. So this is what we call autoregressive language models. Again, this is really not the only way of modeling distribution, this is just one way. It has some benefits and some downsides. One downside of autoregressive language models is that when you actually sample from this autoregressive language model, you basically have a for loop which generates the next word, then conditions on that next word, and then regenerate the other word. So basically, if you have a longer sentence that you want to generate, it takes more time to generate it. So there are some downsides of this current paradigm, but that's what we currently have, so I'm going to talk about this one. Great. So autoregressive language models. At a high level, what the task of autoregressive language model is, is simply predicting the next word, as I just said. So if you have a sentence like she likely prefers, one potential next word might be dogs. And the way we do it is that we first tokenize. So you take these words or sub words, you tokenize them, and then you give an ID for each token. So here I have one, two, three. Then you pass it through this black box. As I already said, we're not going to talk about the architecture. You just pass it through a model, and you then get a distribution, a probability distribution over the next word, over the next token. And then you sample from this distribution, you get a new token, and then you de-tokenize. So you get a new ID, you de-tokenize, and that's how you basically sample from a language model. One thing which is important to note is that the last two steps are actually only needed during inference. When you do training, you just need to predict the most likely token, and you can just compare to the real token, which happened in practice, and then you basically change the weights of your model to increase the probability of generating that token. Great. So autoregressive neural language models. So to be slightly more specific, still without talking about the architecture, the first thing we do is that we have all of these- oh, sorry, yes? On the previous slide, when you're predicting the probability of the next token, does this mean that your final output vector has to be the same dimensionality as the number of tokens that you have? Yes. How do you deal with if you're adding more tokens to your co-presenters? Yeah. So we're going to talk about tokenization actually later, so you will get some sense of this. You basically can't deal with adding new tokens. I'm kind of exaggerating. There are methods for doing it, but essentially people don't do it. So it's really important to think about how you tokenize your text, and that's why we'll talk about that later. But it's a very good point to note is that basically the vocabulary size, so the number of tokens that you have, is essentially the output of your language model. So it's actually pretty large. Okay, so autoregressive neural language models. First thing you do is that you take every word or every token. You embed them, so you get some vector representation for each of these tokens. You pass them through some neural network, as we said, it's a transformer. Then you get a representation for all the words in the context. So it's basically a representation of the entire sentence. You pass it through a linear layer, as you just said, to basically map it to the number so that the output, the number of outputs is the number of tokens. You then pass it through some softmax, and you basically get a probability distribution over the next words given every word in the context. And the laws that you use is basically, it's essentially a task of classifying the next token. So it's a very simple kind of machine learning task. So you use the cross-entropy laws, where you basically look at the actual target that happened, which is a target distribution, which is a one-hot encoding, which here in this case says, I saw the real word that happened is cat. So that's a one-hot distribution over cat. And here, this is the distribution that you generated. And basically, you do cross-entropy, which really just increases the probability of generating cat and decreases the probability of generating all the other tokens. One thing to notice is that, as you all know, again, this is just equivalent to maximizing the text log likelihood, because you can just rewrite the max over the probability of this autoregressive language modeling task as just being this minimum of, I just added the log here and minus, which is just the minimum of the loss, which is the cross-entropy loss. So basically, minimizing the loss is the same thing as maximizing the likelihood of your text. Any questions? OK, tokenizer. So this is one thing that people usually don't talk that much about. Tokenizers are extremely important. So it's really important that you understand, at least, what they do at a high level. So why do we need tokenizers in the first place? First, it's more general than words. So one simple thing that you might think is, oh, we're just going to take every word that we will have. And you just say, every word is a token in its own. But then what happens is, if there's a typo in your word, then you might not have any token associated with this word with a typo. And then you don't know how to actually pass this word with a typo into a large language model. So what do you do next? And also, even if you think about words, words are fine with Latin-based languages. But if you think about a language like Thai, you won't have a simple way of tokenizing by spaces, because there are no spaces between words. So really, tokens are much more general than words. First thing. Second thing that you might think is that you might tokenize every sentence, character by character. You might say, A is one token, B is another token. That would actually work, and probably very well. The issue is that then your sequence becomes super long. And as you probably remember from the lecture on transformers, the complexity grows quadratically with the length of sequences. So you really don't want to have a super long sequence. So tokenizers basically try to deal with those two problems and give common sub-sequences a certain token. And usually, how you should be thinking about it is around an average of every token is around three, four letters. And there are many algorithms for tokenization. I'll just talk about one of them to give you a high level, which is what we call byte-pair encoding, which is actually pretty common, one of the two most common tokenizers. And the way that you train a tokenizer is that first, you start with a very large corpus of text. And here, I'm really not talking about training a large language model yet. This is purely for the tokenization step. So this is my large corpus of text with these five words. Then you associate every character in this corpus of text a different token. So here, I just split up every character with a different token, and I just color coded all of those tokens. And then what you do is that you go through your text, and every time you see pairs of tokens that are very common, the most common pair of token, you just merge them. So here, you see three times the tokens T and O next to each other, so you're just going to say this is a new token. And then you continue. You repeat that. So now you have T-O-K, TOK, which happens three times, TOK with an E, that happens two times, and TOKEN, which happens twice, and then EX, which also happens twice. So this is that if you were to train a tokenizer on this corpus of text, which is very small, that's how you would finish with a trained tokenizer. In reality, you do it on much larger corpuses of text. And this is the real tokenizer of, actually, I think this is GPT-3 or ChatGPT. And here, you see how it would actually separate these words. So basically, you see the same thing as what we gave in the previous example, TOKEN becomes its own token. So tokenizer is actually split up into two tokens, TOKEN and EIZER. So yeah, that's all about tokenizers. Any question on that? Yeah? How do you deal with spaces, and how do you deal with introduction? Yeah. So actually, there's a step before tokenizers, which is what we call pre-tokenizers, which is exactly what you just said. So this is mostly, in theory, there's no reason to deal with spaces and punctuation separately. You could just say every space gets its own token, every punctuation gets its own token, and you could just do all the merging. The problem is that, so there's an efficiency question. Actually, training these tokenizers takes a long time. So you're better off, because you have to consider every pair of token. So what you end up doing is saying, if there's a space, this is very, like pre-tokenizers are very English-specific. So you say, if there's a space, we're not going to start looking at the token that came before and the token that came afterwards. So you're not merging in between spaces. But this is just like a computation optimization. You could theoretically just deal with it the same way as you deal with any other character. Yeah? When you merge tokens, do you delete the tokens that you merged away, or do you keep the smaller tokens that you merged? You actually keep the smaller tokens. I mean, in reality, it doesn't matter much, because usually on large corpus of text, you will have actually everything. But you usually keep the small ones. And the reason why you want to do that is because if, in case there's a, as we said before, you have some grammatical mistakes or some typos, you still want to be able to represent these words by character. So yeah. Yes? Are the tokens unique? So, I mean, say, in this case, T-O-K-E-N, is there only one occurrence, or do you need to leave multiple occurrence so they can take on different meanings or something? Oh, oh, I see what you say. No, no, no. It's every token has its own unique ID. So a usual, this is a great question. For example, if you think about a bank, which could be bank for money or bank like water, they will have the same token, but the model will learn, the transformer will learn that based on the words that are around it, it should associate that, I'm saying, I'm being very hand-wavy here, but associate that with a representation that is either more like the bank money side or the bank water side. But that's the transformer that does that. It's not a tokenizer. Yes? Yeah, so you mentioned during tokenization, you keep the smaller tokens to start with, right? So if you start with a T, you keep the T, and then you tell your tokenizer to expand that amount of token. So let's say maybe you didn't train on token, but in your data, you are trying to encode token. So how does the tokenizer know to encode it with token or to do it with T? Yeah, it's a great question. You basically, when you, so when you tokenize, so that's after training of the tokenizer, when you actually apply the tokenizer, you basically always choose the largest token that you can apply. So if you can do token, you will never do T. You will always do token. But it's actually, so people don't usually talk that much about tokenizers, but there's a lot of computational benefits or computational tricks that you can do for making these things faster. So I really don't think we, and honestly, I think a lot of people think that we should just get away from tokenizers and just kind of tokenize\\n\\nIt's probably the main two things it's used for. So if you want your model shared, then you would do that. But if you want it just saved on your computer, do saved pre-trained for a local save. By the way, this only saves the LoRa adapters, meaning basically the things that were changed. It doesn't save the entire model with the change parameters, just the changes, right? So it's less memory and just faster to save. But if you want to save the LoRa adapters with the saved model, you can change this. If you want to load the LoRa adapters, we saved for inference, you would change this false to true. So simply changing this. And yeah, this is the model name. So obviously, you can change this. This is your model used for training. Here is just LoRa model, but you can name it Lama3. I don't know, copywriting, or Lama3 medical diagnosis, whatever your use case is, obviously. And then here, the Alpaca prompt. So yeah, this is the variable we declared earlier. So this is the importance. You can't just go into the Colab and try to running this cell. You have to run the cells from above. Otherwise, this will not work. So whenever you're using a Jupyter Notebook, such as Google Colab, always run all cells in order. Otherwise, it will not work. So yeah. So this is the same format, right, from earlier instruction input-output. At this point, you should be familiar with this. And that's just for this particular data set and for this style of prompting. So if you have a different one, then follow the different one. So obviously, here, what is a famous tower in Paris? Obviously, it's Eiffel Tower, blah, blah, blah. It gives some extra info about it. So you can also use Hugging Face AutoModel for PerfCasualLM. But Unslof does not recommend this because it's a lot slower than Unslof. So yeah, if possible, use Unslof for speed. And as the name suggests of Unslof, it's unslowing everything. It's making everything two to five times faster. So why not do that with 80% less memory? So yeah. OK, and then we're preparing to save our trained model in a more compact format and then upload it into a cloud platform, which allows for less storage and compression power. So again, I'm not going to even pretend I understand everything because this is honestly stepping outside of my comfort zone. But just building this and doing this fine-tuning taught me a lot. So if you want more technical videos like this, let me know. Next, we're ready to compress our model using various quantization methods, which means just making it easier to run on a machine. So maybe you cannot, like if you have a bad computer, maybe you cannot run the full model, but you definitely can run a quantized version of it. It makes it leaner, and then we upload it to the cloud for easy sharing. And this is what this piece of code does. And so we use the model-unslof.gguf file or the quantized version. So the Q4 means quantized in LlamaCCP. Or if you want a UI-based system, which probably you do, which is easier to use, you can use GPT4 or this other one. It's escaping me. But yeah, these are basically these UI-based systems that you can use to LLM anything. Yeah, I don't know if this supports it. But yeah, basically, these frameworks have a UI that's easy to chat with. And you can use open source model there. So if you fine-tune this, you can upload this to GPT4 and chat with your own model very easily. And yeah, that's it. You know how to fine-tune Llama3 for your own specific use case. Again, I'm going to leave these resources below the video. And if you have any questions regarding to unslof, join their Discord. So yeah, that's it. If you find this useful, then please subscribe. And again, if you want, during April, which is, what, like eight days, nine days left, if you join the community, you will get a personalized AI strategy to future-proof yourself and your business. So if that sounds valuable to you, then make sure to join. It's the first link in the description. Thank you for watching.\\n\\nMy name is David Andrzej and in this video, I'll teach you how to fine-tune Lama3 so that it performs 10 times better for your specific use case. Let's start with what even is fine-tuning and I made this explanation in plain English so that anybody can understand. Fine-tuning is adapting a pre-trained LLM like GPT-4 or in this case Lama3 to a specific task or domain. It involves adjusting a small portion of the parameters on a more focused dataset. So, you know, when a new model releases, what everybody needs to know is how many parameters it has. We have Lama3 8B and always that number like 8B or 70B, that's the number of parameters. So we're adjusting just a small number of them to make it more focused on a specific thing. Fine-tuning customizes the outputs to be more relevant and accurate for your use case. Here's the power of fine-tuning. Cost-effectiveness. It leverages the power of pre-trained LLMs which cost tens of millions of dollars, if not hundreds of millions, to train and we can just, you know, run a GPU for a few hours and fine-tune something for, I don't know, like cents, a few cents or a few dollars at most, which is just amazing. It gives you improved performance because you can enhance the LLM on your dataset and improve accuracy for specific tasks. And it also is more data efficient. You can achieve excellent results even with smaller datasets. So, you know, maybe even like 300, 500 entries while, you know, Lama3 was trained on 15 trillion tokens. I don't know about you, but I don't have nearly as much data as Zack. So that's why fine-tuning is great for people like you and me. So how does LLM fine-tuning actually work? First, you need to prepare your dataset. And this, you know, depending on how hardcore you want to go, this can take anywhere from 20 minutes to a few hours to weeks, potentially. Depends how far you want to take it. So you create a smaller, high-quality dataset tailored to your specific use case and label it appropriately, which I'll teach you in a bit. The pre-trained LLMs weights are updated incrementally using the optimization algorithms like gradient descent based on the new dataset. So we can only fine-tune LLMs that we have access to the weights, meaning open-source, open-weights LLMs. You cannot fine-tune GPT-4 if you are not OpenAI. OpenAI can do it, obviously, but me and you, we probably don't have GPT-4 just laying on our computer. Then you monitor and refine. You evaluate the model's performance on a validation set, preventing overfitting and guide adjustments. Now, here are some real-world use cases for fine-tuning. Fine-tuning an LLM on customer service transcripts can create a chatbot, like this one, that can address issue in a way specific to your company. So let's say you have a specific product, very niche, that there is not much data about it on the internet. And if somebody messages your customer support email, you want your chatbot to respond in a specific way based on the information of your product. And that data is proprietary. It's private. Only you have it. And you can fine-tune an LLM to respond based on that data. So, like, technically, if you have enough scripts, you can fine-tune an LLM to respond like you. And, you know, if you try chat GPT, if you even give, like, chat GPT some writing and tell it, continue in this writing style, it's terrible. So this is where fine-tuning could be better. Tailored content generation. So you can fine-tune an LLM on your posts and descriptions to create engaging summaries or marketing copy, again, in your writing style, tailored to your audience. Domain-specific analysis. So fine-tuning LLM on legal or medical text can make it much better for those specific benchmarks. So you might have a model that, let's say, it reaches 50 on some arbitrary benchmark. With fine-tuning, it can reach 70 or 80. Now let's dive into how to actually implement this on Llama3. So I created this Google collab. Well, actually, most of it was created by Anslov team. A huge shout out to Anslov because they did all the heavy lifting. So I'm going to also link their GitHub below. Now, first off, I added a component that's only available in April to the community. So if you join during April, you will get a personalized AI strategy to future-proof yourself and your business. So if you want to be among people who are building the future, if you want access to all the different courses, modules, and everything else in the community, and to two weekly calls, then consider joining. And especially if you want me to give you a personalized AI strategy to future-proof yourself. So if that sounds interesting to you, make sure to join the community. It's the first link in the description. Now let's fine-tune LLama3, shall we? So first thing, we check the GPU version available in the environment and install specific dependencies that are comparable with the detected GPU to prevent conflicts. So this is this cell. By the way, if you don't know how Google Colab works, which is, you know, the software I'm using right now, it's super simple. It's basically splitting the code into cells. It's called the Jupyter Notebook, but it's like much more easier to see. You can add text, you can add graphics, and it's great for like tutorials and explaining, right? So if you never use this, it's great because it's free. And Google actually gives you a GPU so you can use this T4 GPU to train this model for free. And if you want faster, you can obviously upgrade it, right? So I'm going to link this Colab below the video as well. So we run this cell, which does what I just explained. The next cell, we need to prepare to load a range of quantized language models, including the new 15 trillion LLama3 model, so trained on 15 trillion tokens. And it's optimized for efficiency with 4-bit quantization. I mean, I'm not going to even pretend I know everything about fine-tuning because I don't. So if it seems like I have gaps in my knowledge, it's because it is. I do have those gaps in my knowledge. So I try to make it as simple as possible. But if this proves something, it proves that you don't have to be a machine learning expert to fine-tune models. So just follow along. So here, this is the max sequence length. Obviously, LLama3 is up to 8,000. So I mean, 2,000 is plenty for this demonstration, but you can do anything. You can do 4,000 or 8,000. Here, I use 4-bit quantization to reduce memory usage, but it can be false as well. So here are the models. We can see, like, we have Mistral7B, LLama2, which is the old one, Gemma from Google. But obviously, we're interested in LLama3 8B. And by the way, we can also use LLama3 70B if you want, which obviously will take longer because it's a much bigger model. So in that case, you might want to buy the premium version of Collab or just wait for a while. But yeah, I mean, everything is the same. Just here, you would change the model to LLama3 70B. And if you want to use, like, gated models from HuggingFace, which gated means that you have to usually agree to some, you know, license or whatever, then here, just remove the comment and then put your HuggingFace token here. Super simple. Now, by the way, you always have to run this. So what do you do when you go to Google Collab? You click on runtime and click run all. That way, all of the cells run. But you can also do it one by one by clicking this button right here next to each cell. And it needs to have this little tick, green tick. That way, it was executed. Here, it's not because I, you know, removed the... I changed this. So anytime you make any change, it disappears. But that doesn't matter. It was still executed. So it's stored in the runtime. Next up, we integrate LoRa. Again, you don't have to understand what this is, but it's basically a way of fine-tuning into our model, which allows us to efficiently update just a fraction of the parameters, enhancing training speed and reducing computation load. So again, we are not training the model from scratch. We're just fine-tuning a few parameters for our specific use case. And here, you can change the R to any number greater than 0, 8, 16, 32, 64, up to you. And your goals, what you want to do with it. By the way, on Sloth, the reason I'm using it is because it makes fine-tuning much faster and consuming less memory. So it's actually a great framework for this. Dataprep. We now use the Alpaca dataset from Yama, which is this one, which has 50,000 rows. And I have it loaded in VS code here. Just that way, you see how it looks like in JSON formatting. So, you know, it's a lot of lines because for everyone, it's basically times five. Yeah, so like 250,000 lines. And it's like every one of them has an instruction. I should probably zoom it in. So every one entry has an instruction. Give three tips for staying healthy. Input, this is not mandatory because instruction is already enough context. And then output, this is what the LLM should say. And you do this enough times and the LLM learns. It basically learns, right? So we can see it probably better here. And if you want to use your own dataset, you have to format it the same way. So, you know, just having output, input and instructions, these three parameters. But yeah, just look at this. Not all of them have the input, which is fine. I mean, probably like 20% or 15% have the input. And that's just extra context. So yeah, I'm also going to link this dataset below. But if you want your own dataset, which, you know, if you want your own use case, just make sure to format it the same way. So, you know, instruction, some text, input, some extra context or empty, and output, how the model should respond. And, you know, if you're getting creative, you can definitely use LLMs to generate these large datasets much faster. I mean, maybe you create really like 20 high quality examples by hand. And then you run a team of agents for creating that dataset that can just, you know, use those 20 examples to create 50,000. Like in this dataset. But yeah, that's a topic for a whole nother video. So if you want me to make a video on how to make datasets for fine tuning, then let me know. But let's go back to our Colab. So then we define a system prompt, which is, you know, custom instruction system prompt, which you already know, hopefully. That formats tasks into instruction inputs and responses. So this has to fit with our dataset. And we apply it to our dataset for the model. And we add the EOS token to signal completion. So this token right here, here we define it. And here we add it because without this, the token generation continues forever. So we don't want that, obviously. So let's look at the system prompt. It's very simple. It says below is an instruction that describes a task paired with an input that provides further context. Write a response that appropriately completes the request. And that's our system prompt. And then we feed it the instruction, the input and response. And obviously you can change the system prompt if you want. Now, train the model. We do a 60 step. We do only 60 steps here to speed things up. You can, like, this is obviously very small because it's not even one epoch, training epoch. So if you want to like actually use something for production or your business, you probably want to train it for longer than 60 steps. And I'm going to show you how in this bit. So if you do multiple epochs, you have to turn max steps none. So here, okay. Number of trained epochs is not included in here. So what you would do is you would copy this and you would go in here and look at the steps, right? So we have the steps here. You would add this. Maybe you would do four or whatever, however many you want. The more, the better. But at a certain point, it starts to not yield better result. So max steps, you have to change it to none, right? So it's like 60 right now. So you do none. And this is where you would do like proper fine tuning. But, you know, I just added that 60 for demonstration. That way it's faster. And it still took like eight minutes. So I'm not going to replicate it. I'm just going to show you everything. But yeah, basically, you know, this is what you do. You decide how many epochs you want. And then at this stage, we're configuring our model's training setup where we define things like batch size and learning rate to teach our model effectively with the data we've prepared. So obviously you can like mess with stuff here. Again, I'm not going to pretend I understand everything. But the main things are, you know, packing like this can make it five times faster for short sequences. Obviously, the steps and the epochs. But yeah, I mean, if you're confused something, just take a screenshot. Boom, like this. And ask ShedGPD. Now, this is the current memory stats, right? So we're using the Tesla T4 GPU provided from Google for free. And the max memory is 14 gigabytes. And this is where the training begins. This is the magical part, right? So here we do this line of code, trainer stats, trainer.train. And this will give us the statistics as the model trains. So again, this is only 60 steps, which is like zero epochs. But yeah, you can see the training loss going down. So like basically, smaller number is better here. So you can see like at the start, we have 1.8. Like 1.9, and then it quickly starts dropping to like 0.9, you know, around 1, 0.8. So it fluctuates a bit, but it consistently goes down 0.7. But you can see it's reaching like a asymptote, right? Obviously, it's only 60 steps. So it really doesn't mean anything. But yeah, like we ended up like 0.8 from like 2. So it shows you like if the model is actually improving. So it shows you like if the model is actually improving. And this took like eight minutes. So you can see the stats here, right? So 476 seconds, almost exactly eight minutes. Peak reserve memory was 8.9 gigabytes. And for training was 3.3 gigabytes. So not like this is the power of Unslof. It's like really optimized for this to use, to run faster and to use less memory. So that way we can fine tune GPUs for cheaper. I mean, you know, I'm using a free T4 GPU from Google. So it's free, but it's faster. Like if you didn't use Unslof, it would be a lot slower. So, okay, so 60% of, we used 60% of max memory. So that's good because we didn't like hit the limit. So we still have like 40% reserved. And for training, it was only 22%, which is even better. Inference, which means here we actually run our new model that we fine tuned. And okay, so this data set is for like instructions. And this is basically when you see a model that is like instruct at the end of it, this is what they mean. It's just trained on a large data set of instructions. Because usually the models are more for like chatting, for text generation. You know, you give it some input and it's like gives you some output. It's, you know, for more conversational. Here for instructions, for the instruct models is to follow instructions. You give it a task and it completes it. So like we can see it probably here in VS code, like rewrite the sentence to change its meaning and then output the thief escaped. Compared to data subs, so this is like all tasks. It's all in instructions. And then it shows how the model should do it. So let's look at it, right? So now we've trained the model. This took like eight minutes to do. So all of you can do this. The beauty of using a Google Cloud is that obviously it doesn't matter what machine you have. Even if you have a terrible computer, this will take the exact same time because you're using the GPU and cloud. So obviously here you can change your prompt. I mean, this is, you know, I changed the prompts here. So this is my prompt. But always make sure to leave the output blank. So here, the first one is the instruction. Then this is the input, like the extra added context and the output, leave it blank because the model will generate it, right? So list the prime numbers contained within\\n\\nHello, and welcome to Introduction to Large Language Models. My name is John Ewald, and I'm a training developer here at Google Cloud. In this course, you'll learn to define large language models, or LLMs, describe LLM use cases, explain prompt tuning, and describe Google's GenAI development tools. Large language models, or LLMs, are a subset of deep learning. To find out more about deep learning, see our Introduction to Generative AI course video. LLMs and generative AI intersect, and they are both a part of deep learning. Another area of AI you may be hearing a lot about is generative AI. This is a type of artificial intelligence that can produce new content, including text, images, audio, and synthetic data. So what are large language models? Large language models refer to large, general-purpose language models that can be pre-trained and then fine-tuned for specific purposes. What do pre-trained and fine-tuned mean? Imagine training a dog. Often you train your dog basic commands, such as sit, come, down, and stay. These commands are normally sufficient for everyday life and help your dog become a good canine citizen. However, if you need a special service dog, such as a police dog, a guide dog, or a hunting dog, you add special trainings. This similar idea applies to large language models. These models are trained for general purposes to solve common language problems such as text classification, question answering, document summarization, and text generation across industries. The models can then be tailored to solve specific problems in different fields such as retail, finance, and entertainment using a relatively small size of field datasets. Let's further break down the concept into three major features of large language models. Large indicates two meanings. First is the enormous size of the training dataset, sometimes at the petabyte scale. Second, it refers to the parameter count. In ML, parameters are often called hyperparameters. Hyperparameters are basically the memories and the knowledge that the machine learned from the model training. Parameters define the skill of a model in solving a problem, such as predicting text. General purpose means that the models are sufficient to solve common problems. Two reasons lead to this idea. First is the commonality of a human language, regardless of the specific tasks, and second is the resource restriction. Only certain organizations have the capability to train such large language models with huge datasets and a tremendous number of parameters. How about letting them create fundamental language models for others to use? This leads to the last point, pre-trained and fine-tuned, meaning to pre-train a large language model for a general purpose with a large dataset and then fine-tune it for specific aims with a much smaller dataset. The benefits of using large language models are straightforward. First, a single model can be used for different tasks. This is a dream come true. These large language models that are trained with petabytes of data and generate billions of parameters are smart enough to solve different tasks including language translation, sentence completion, text classification, question answering, and more. Second, large language models require minimal field training data when you tailor them to solve your specific problem. Large language models obtain decent performance even with little domain training data. In other words, they can be used for few-shot or even zero-shot scenarios. In machine learning, few-shot refers to training a model with minimal data, and zero-shot implies that a model can recognize things that have not explicitly been taught in the training before. Third, the performance of large language models is continuously growing when you add more data and parameters. Let's take Palm as an example. In April 2022, Google released Palm, short for Pathways Language Model, a 540 billion parameter model that achieves a state-of-the-art performance across multiple language tasks. Palm is a dense decoder-only transformer model. It has 540 billion parameters. It leverages the new Pathways system, which has enabled Google to efficiently train a single model across multiple TPUv4 pods. Pathway is a new AI architecture that will handle many tasks at once, learn new tasks quickly, and reflect a better understanding of the world. The system enables Palm to orchestrate distributed computation for accelerators. We previously mentioned that Palm is a transformer model. A transformer model consists of encoder and decoder. The encoder encodes the input sequence and passes it to the decoder, which learns how to decode the representations for a relevant task. We've come a long way from traditional programming to neural networks to generative models. In traditional programming, we used to have to hard-code the rules for distinguishing a cat. Type? Animal. Legs? Four. Ears? Two. Fur? Yes. Likes? Yarn. Catnip. In the wave of neural networks, we could give the network pictures of cats and dogs and ask, is this a cat? And it would predict a cat. In the generative wave, we, as users, can generate our own content, whether it be text, images, audio, video, or other. For example, models like Palm or Lambda, or language model for dialogue applications, ingest very, very large data from multiple sources across the internet, and build foundation language models we can use simply by asking a question, whether typing it into a prompt or verbally talking into the prompt. So when you ask it, what's a cat? It can give you everything it has learned about a cat. Let's compare LLM development using pre-trained models with traditional ML development. First, with LLM development, you don't need to be an expert. You don't need training examples, and there is no need to train a model. All you need to do is think about prompt design, which is the process of creating a prompt that is clear, concise, and informative. It is an important part of natural language processing. In traditional machine learning, you need training examples to train a model. You also need compute time and hardware. Let's take a look at an example of a text generation use case. Question answering, or QA, is a subfield of natural language processing that deals with the task of automatically answering questions posed in natural language. QA systems are typically trained on a large amount of text and code, and they are able to answer a wide range of questions including factual, definitional, and opinion-based questions. The key here is that you need domain knowledge to develop these question answering models. For example, domain knowledge is required to develop a question answering model for customer IT support, or healthcare, or supply chain. Using generative QA, the model generates free text directly based on the context. There is no need for domain knowledge. Let's look at three questions given to BARD, a large-language model chatbot developed by Google AI. Question 1. This year's sales are $100,000. Expenses are $60,000. How much is net profit? BARD first shares how net profit is calculated, then performs the calculation. Then BARD provides the definition of net profit. Here's another question. Inventory on hand is 6,000 units. A new order requires 8,000 units. How many units do I need to fill to complete the order? Again, BARD answers the question by performing the calculation. And our last example, we have 1,000 sensors in 10 geographic regions. How many sensors do we have on average in each region? BARD answers the question with an example on how to solve the problem and some additional context. In each of our questions, a desired response was obtained. This is due to prompt design. Prompt design and prompt engineering are two closely related concepts in natural language processing. Both involve the process of creating a prompt that is clear, concise, and informative. However, there are some key differences between the two. Prompt design is the process of creating a prompt that is tailored to the specific task that this system is being asked to perform. For example, if the system is being asked to translate a text from English to French, the prompt should be written in English and should specify that the translation should be in French. Prompt engineering is the process of creating a prompt that is designed to improve performance. This may involve using domain-specific knowledge, providing examples of the desired output, or using keywords that are known to be effective for the specific system. Prompt design is a more general concept while prompt engineering is a more specialized concept. Prompt design is essential while prompt engineering is only necessary for systems that require a high degree of accuracy or performance. There are three kinds of large language models, generic language models, instruction-tuned, and dialogue-tuned. Each needs prompting in a different way. Generic language models predict the next word based on the language in the training data. This is an example of a generic language model. The next word is a token based on the language in the training data. In this example, the cat sat on, the next word should be the, and you can see that the is the most likely next word. Think of this type as an autocomplete in search. In instruction-tuned, the model is trained to predict a response to the instructions given in the input. For example, summarize a text of x, generate a poem in the style of x, give me a list of keywords based on semantic similarity for x. And in this example, classify the text into neutral, negative, or positive. In dialogue-tuned, the model is trained to have a dialogue by the next response. Dialogue-tuned models are a special case of instruction-tuned where requests are typically framed as questions to a chatbot. Dialogue-tuning is expected to be in the context of a longer back-and-forth conversation and typically works better with natural question-like phrasings. Chain of thought reasoning is the observation that models are better at getting the right answer when they first output text that explains the reason for the answer. Let's look at the question. Roger has five tennis balls. He buys two more cans of tennis balls. Each can has three tennis balls. How many tennis balls does he have now? This question is posed initially with no response. The model is less likely to get the correct answer directly. However, by the time the second question is asked, the output is more likely to end with the correct answer. A model that can do everything has practical limitations. Task-specific tuning can make LLMs more reliable. FlexAI provides task-specific foundation models. Let's say you have a use case where you need to gather sentiments, or how your customers are feeling about your product or service. You can use the classification task Sentiment Analysis Task Model. Same for vision tasks. If you need to perform occupancy analytics, there is a task-specific model for your use case. Tuning a model enables you to customize the model response based on examples of the task that you want the model to perform. It is essentially the process of adapting a model to a new domain or set of custom use cases by training the model on new data. For example, we may collect training data and tune the model specifically for the legal or medical domain. You can also further tune the model by fine-tuning, where you bring your own dataset and retrain the model by tuning every weight in the LLM. This requires a big training job and hosting your own fine-tuned model. Here's an example of a medical foundation model trained on healthcare data. The tasks include question answering, image analysis, finding similar patients, and so forth. Fine-tuning is expensive and not realistic in many cases. So are there more efficient methods of tuning? Yes, Parameter Efficient Tuning Methods, or PETM, are methods for tuning a large-language model on your own custom data without duplicating the model. The base model itself is not altered. Instead, a small number of add-on layers are tuned, which can be swapped in and out at inference time. Generative AI Studio lets you quickly explore and customize generative AI models that you can leverage in your applications on Google Cloud. Generative AI Studio helps developers create and deploy generative AI models by providing a variety of tools and resources that make it easy to get started. For example, there's a library of pre-trained models, a tool for fine-tuning models, a tool for deploying models to production, and a community forum for developers to share ideas and collaborate. Generative AI App Builder lets you create Gen AI apps without having to write any code. Gen AI App Builder has a drag-and-drop interface that makes it easy to design and build apps, a visual editor that makes it easy to create and edit app content, a built-in search engine that allows users to search for information within the app, and a conversational AI engine that allows users to interact with the app using natural language. You can create your own chatbots, digital assistants, custom search engines, knowledge bases, training applications, and more. Palm API lets you test and experiment with Google's large language models and Gen AI tools. To make prototyping quick and more accessible, developers can integrate Palm API with Makersuite and use it to access the API using a graphical user interface. The suite includes a number of different tools, such as a model training tool, a model deployment tool, and a model monitoring tool. The model training tool helps developers train ML models on their data using different algorithms. The model deployment tool helps developers deploy ML models to production with a number of different deployment options, and the model monitoring tool helps developers monitor the performance of their ML models in production using a dashboard and a number of different metrics. That's all for now. Thanks for watching this course, Introduction to Large Language Models.\",\n",
       " 'answer': 'The purpose of a Large Language Model (LLM) is to understand and generate human language based on patterns it has learned from vast amounts of text data. LLMs are a type of neural network designed to handle a wide range of tasks, including summarization, text generation, creative writing, question-answering, programming, and more. They are trained on massive datasets from various sources like web pages, books, and transcripts, allowing them to learn and adapt to different contexts and tasks.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the configuration for the workflow\n",
    "config = {\"configurable\": {\"thread_id\": \"test_thread\"}}\n",
    "\n",
    "query = \"Hi my name is Obi-Wan, what is the purpose of an LLM?\"\n",
    "\n",
    "# Run the workflow\n",
    "output = app.invoke({\"input\": query}, config)\n",
    "\n",
    "# Display the result\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step X - Limits of your app \n",
    "\n",
    "Now your application works! ðŸ‘ Though this solution is great, it has some drawbacks. For example, try to insert unrelated information to the topic in your prompt and then test whether your LLM remembers that information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Hi my name is Obi-Wan, what is the purpose of an LLM?', 'chat_history': [HumanMessage(content='Hi my name is Obi-Wan, what is the purpose of an LLM?', additional_kwargs={}, response_metadata={}, id='df4b4ce7-3a69-4865-b825-902c8c2a9c06'), AIMessage(content='The purpose of a Large Language Model (LLM) is to understand and generate human language based on patterns it has learned from vast amounts of text data. LLMs are a type of neural network designed to handle a wide range of tasks, including summarization, text generation, creative writing, question-answering, programming, and more. They are trained on massive datasets from various sources like web pages, books, and transcripts, allowing them to learn and adapt to different contexts and tasks.', additional_kwargs={}, response_metadata={}, id='9f050a0f-10d9-4255-8424-207ec27a7305')], 'context': \"This video is going to give you everything you need to go from knowing absolutely nothing about artificial intelligence and large language models to having a solid foundation of how these revolutionary technologies work. Over the past year, artificial intelligence has completely changed the world, with products like ChatGPT potentially appending every single industry and how people interact with technology in general. And in this video, I will be focusing on LLMs, how they work, ethical considerations, applications, and so much more. And this video was created in collaboration with an incredible program called AI Camp, in which high school students learn all about artificial intelligence. And I'll talk more about that later in the video. Let's go. So first, what is an LLM? Is it different from AI? And how is ChatGPT related to all of this? LLMs stand for large language models, which is a type of neural network that's trained on massive amounts of text data. It's generally trained on data that can be found online. Everything from web scraping to books to transcripts, anything that is text-based can be trained into a large language model. And taking a step back, what is a neural network? A neural network is essentially a series of algorithms that try to recognize patterns in data. And really what they're trying to do is simulate how the human brain works. And LLMs are a specific type of neural network that focus on understanding natural language. And as mentioned, LLMs learn by reading tons of books, articles, internet text, and there's really no limitation there. And so how do LLMs differ from traditional programming? Well, with traditional programming, it's instruction-based, which means if X, then Y. You're explicitly telling the computer what to do. You're giving it a set of instructions to execute. But with LLMs, it's a completely different story. You're teaching the computer not how to do things, but how to learn how to do things. And this is a much more flexible approach and is really good for a lot of different applications where previously traditional coding could not accomplish them. So one example application is image recognition. With image recognition, traditional programming would require you to hard code every single rule for how to, let's say, identify different letters. So A, B, C, D. But if you're handwriting these letters, everybody's handwritten letters look different. So how do you use traditional programming to identify every single possible variation? Well, that's where this AI approach comes in. Instead of giving a computer explicit instructions for how to identify a handwritten letter, you instead give it a bunch of examples of what handwritten letters look like, and then it can infer what a new handwritten letter looks like based on all of the examples that it has. What also sets machine learning and large language models apart in this new approach to programming is that they are much more flexible, much more adaptable, meaning they can learn from their mistakes and inaccuracies, and are thus so much more scalable than traditional programming. LLMs are incredibly powerful at a wide range of tasks, including summarization, text generation, creative writing, question and answer, programming, and if you've watched any of my videos, you know how powerful these large language models can be. And they're only getting better. Know that right now, large language models and AI in general are the worst they'll ever be. And as we're generating more data on the internet, and as we use synthetic data, which means data created by other large language models, these models are going to get better rapidly. And it's super exciting to think about what the future holds. Now let's talk a little bit about the history and evolution of large language models. We're going to cover just a few of the large language models today in this section. The history of LLMs traces all the way back to the ELISA model, which was from 1966, which was really the first language model. It had pre-programmed answers based on keywords. It had a very limited understanding of the English language. And like many early language models, you started to see holes in its logic after a few back and forths in a conversation. And then after that, language models really didn't evolve for a very long time. Although technically the first recurrent neural network was created in 1924 or RNN, they weren't really able to learn until 1972. And these new learning language models are a series of neural networks with layers and weights and a whole bunch of stuff that I'm not going to get into in this video. And RNNs were really the first technology that was able to predict the next word in a sentence rather than having everything pre-programmed for it. And that was really the basis for how current large language models work. And even after this and the advent of deep learning in the early 2000s, the field of AI evolved very slowly, with language models far behind what we see today. This all changed in 2017, where the Google DeepMind team released a research paper about a new technology called Transformers. And this paper was called Attention is All You Need. And a quick side note, I don't think Google even knew quite what they had published at that time. But that same paper is what led OpenAI to develop ChatGPT. So obviously other computer scientists saw the potential for the Transformers architecture. With this new Transformers architecture, it was far more advanced. It required decreased training time, and it had many other features like self-attention, which I'll cover later in this video. Transformers allowed for pre-trained large language models like GPT-1, which was developed by OpenAI in 2018. It had 117 million parameters, and it was completely revolutionary, but soon to be outclassed by other LLMs. Then after that, BERT was released, B-E-R-T, in 2018. That had 340 million parameters, and had bidirectionality, which means it had the ability to process text in both directions, which helped it have a better understanding of context. And as comparison, a unidirectional model only has an understanding of the words that came before the target text. And after this, LLMs didn't develop a lot of new technology, but they did increase greatly in scale. GPT-2 was released in early 2019, and had 2.5 billion parameters. Then GPT-3 in June of 2020, with 175 billion parameters. And it was at this point that the public started noticing large language models. GPT had a much better understanding of natural language than any of its predecessors. And this is the type of model that powers ChatGPT, which is probably the model that you're most familiar with. And ChatGPT became so popular because it was so much more accurate than anything anyone had ever seen before. And it was really because of its size. And because it was now built into this chatbot format, anybody could jump in and really understand how to interact with this model. ChatGPT 3.5 came out in December of 2022, and started this current wave of AI that we see today. Then in March 2023, GPT-4 was released, and it was incredible, and still is incredible to this day. It had a whopping reported 1.76 trillion parameters, and uses likely a mixture of experts approach, which means it has multiple models that are all fine-tuned for specific use cases. And then when somebody asks a question to it, it chooses which of those models to use. And then they added multi-modality and a bunch of other features. And that brings us to where we are today. All right, now let's talk about how LLMs actually work in a little bit more detail. The process of how large language models work can be split into three steps. The first of these steps is called tokenization. And there are neural networks that are trained to split long text into individual tokens. And a token is essentially about three fourths of a word. So if it's a shorter word like hi, or that, or there, it's probably just one token. But if you have a longer word like summarization, it's going to be split into multiple pieces. And the way that tokenization happens is actually different for every model. Some of them separate prefixes and suffixes. Let's look at an example. What is the tallest building? So what is the tallest building? Are all separate tokens. And so that separates the suffix off of tallest, but not building because it is taking the context into account. And this step is done so models can understand each word individually, just like humans. We understand each word individually and as groupings of words. And then the second step of LLMs is something called embeddings. The large language models turns those tokens into embedding vectors, turning those tokens into essentially a bunch of numerical representations of those tokens numbers. And this makes it significantly easier for the computer to read and understand each word and how the different words relate to each other. And these numbers all correspond with the position in an embeddings vector database. And then the final step in the process is transformers, which we'll get to in a little bit. But first, let's talk about vector databases. And I'm going to use the terms word and token interchangeably. So just keep that in mind because they're almost the same thing, not quite, but almost. And so these word embeddings that I've been talking about are placed into something called a vector database. These databases are storage and retrieval mechanisms that are highly optimized for vectors. And again, those are just numbers, long series of numbers. Because they're converted into these vectors, they can easily see which words are related to other words based on how similar they are, how close they are based on their embeddings. And that is how the large language model is able to predict the next word based on the previous words. Vector databases capture the relationship between data as vectors in multi-dimensional space. I know that sounds complicated, but it's really just a lot of numbers. Vectors are objects with a magnitude and a direction, which both influence how similar one vector is to another. And that is how LLMs represent words based on those numbers. Each word gets turned into a vector, capturing semantic meaning and its relationship to other words. So here's an example. The words book and worm, which independently might not look like they're related to each other, but they are related concepts because they frequently appear together. A bookworm, somebody who likes to read a lot. And because of that, they will have embeddings that look close to each other. And so models build up an understanding of natural language using these embeddings and looking for similarity of different words, terms, groupings of words, and all of these nuanced relationships. And the vector format helps models understand natural language better than other formats. And you can kind of think of all this like a map. If you have a map with two landmarks that are close to each other, they're likely going to have very similar coordinates. So it's kind of like that. Okay, now let's talk about transformers. Matrix representations can be made out of those vectors that we were just talking about. This is done by extracting some information out of the numbers and placing all of the information into a matrix through an algorithm called multi-head attention. The output of the multi-head attention algorithm is a set of numbers which tells the model how much the words and its order are contributing to the sentence as a whole. We transform the input matrix into an output matrix which will then correspond with a word having the same values as that output matrix. So basically we're taking that input matrix, converting it into an output matrix, and then converting it into natural language. And the word is the final output of this whole process. This transformation is done by the algorithm that was created during the training process. So the model's understanding of how to do this transformation is based on all of its knowledge that it was trained with, all of that text data from the internet, from books, from articles, etc. And it learned which sequences of words go together and their corresponding next words based on the weights determined during training. Transformers use an attention mechanism to understand the context of words within a sentence. It involves calculations with the dot product, which is essentially a number representing how much the word contributed to the sentence. It will find the difference between the dot products of words and give it correspondingly large values for attention. And it will take that word into account more if it has higher attention. Now let's talk about how large language models actually get trained. The first step of training a large language model is collecting the data. You need a lot of data. When I say billions of parameters, that is just a measure of how much data is actually going into training these models. And you need to find a really good data set. If you have really bad data going into a model, then you're going to have a really bad model. Garbage in, garbage out. So if a data set is incomplete or biased, the large language model will be also. And data sets are huge. We're talking about massive, massive amounts of data. They take data in from web pages, from books, from conversations, from reddit posts, from x posts, from youtube transcriptions. Basically anywhere where we can get some text data, that data is becoming so valuable. Let me put into context how massive the data sets we're talking about really are. So here's a little bit of text which is 276 tokens. That's it. Now if we zoom out, that one pixel is that many tokens. And now here's a representation of 285 million tokens, which is 0.02% of the 1.3 trillion tokens that some large language models take to train. And there's an entire science behind data pre-processing, which prepares the data to be used to train a model. Everything from looking at the data quality, to labeling consistency, data cleaning, data transformation, and data reduction. But I'm not going to go too deep into that. And this pre-processing can take a long time. And it depends on the type of machine being used, how much processing power you have, the size of the data set, the number of pre-processing steps, and a whole bunch of other factors that make it really difficult to know exactly how long pre-processing is going to take. But one thing that we know takes a long time is the actual training. Companies like NVIDIA are building hardware specifically tailored for the math behind large language models. And this hardware is constantly getting better. The software used to process these models are getting better also. And so the total time to process models is decreasing, but the size of the models is increasing. And to train these models, it is extremely expensive because you need a lot of processing power, electricity, and these chips are not cheap. And that is why NVIDIA stock price has skyrocketed. Their revenue growth has been extraordinary. And so with the process of training, we take this pre-processed text data that we talked about earlier, and it's fed into the model. And then using transformers, or whatever technology a model is actually based on, but most likely transformers, it will try to predict the next word based on the context of that data. And it's going to adjust the weights of the model to get the best possible output. And this process repeats millions and millions of times over and over again until we reach some optimal quality. And then the final step is evaluation. A small amount of the data is set aside for evaluation. And the model is tested on this data set for performance. And then the model is adjusted if necessary. The metric used to determine the effectiveness of the model is called perplexity. It will compare two words based on their similarity. And it will give a good score if the words are related and a bad score if it's not. And then we also use RLHF, reinforcement learning through human feedback. And that's when users or testers actually test the model and provide positive or negative scores based on the output. And then once again, the model is adjusted as necessary. All right, let's talk about fine-tuning now, which I think a lot of you are going to be interested in because it's something that the average person can get into quite easily. So we have these popular large language models that are trained on massive sets of data to build general language capabilities. And these pre-trained models, like BERT, like GPT, give developers a head start versus training models from scratch. But then in comes fine-tuning, which allows us to take these raw models, these foundation models, and fine-tune them for our specific use cases. So let's think about an example. Let's say you want to fine-tune a model to be able to take pizza orders, to be able to have conversations, answer questions about pizza, and finally be able to allow customers to buy pizza. You can take a pre-existing set of conversations that exemplify the back and forth between a pizza shop and a customer, load that in, fine-tune a model, and then all of a sudden that model is going to be much better at having conversations about pizza ordering. The model updates the weights to be better at understanding certain pizza terminology, questions, responses, tone, everything. And fine-tuning is much faster than a full training, and it produces much higher accuracy. And fine-tuning allows pre-trained models to be fine-tuned for real-world use cases. And finally, you can take a single foundational model and fine-tune it any number of times for any number of use cases. And there are a lot of great services out there that allow you to do that. And again, it's all about the quality of your data. So if you have a really good data set that you're going to fine-tune a model on, the model is going to be really, really good. And conversely, if you have a poor quality data set, it's not going to perform as well. All right, let me pause for a second and talk about AI Camp. So as mentioned earlier, this video, all of its content, the animations, have been created in collaboration with students from AI Camp. AI Camp is a learning experience for students that are age 13 and above. You work in small, personalized groups with experienced mentors. You work together to create an AI product using NLP, computer vision, and data science. AI Camp has both a three-week and a one-week program during summer that requires zero programming experience. And they also have a new program which is 10 weeks long during the school\\n\\nSo, let's get started, so I'll be talking about building LLMs today, so I think a lot of you have heard of LLMs before, but just as a quick recap, LLMs, standing for Large Language Models, are basically all the chatbots that you've been hearing about recently, so ChatGPT from OpenAI, Claude from Entropiq, Gemini, and Lama, and other type of models like this, and today we'll be talking about how do they actually work, so it's going to be an overview because it's only one lecture, and it's hard to compress everything, but hopefully I'll touch a little bit about all the components that are needed to train some of these LLMs. Also, if you have questions, please interrupt me and ask. If you have a question, most likely other people in the room or on Zoom have the same question, so please ask. Great, so what matters when training LLMs? So, there are a few key components that matter. One is the architecture, so as you probably all know, LLMs are neural networks, and when you think about neural networks, you have to think about what architecture you're using. Another component which is really important is the training loss and the training algorithm, so how you actually train these models. Then it's data, so what do you train these models on? The evaluation, which is how do you know whether you're actually making progress towards the goal of LLMs, and then the system component, so that is like how do you actually make these models run on modern hardware, which is really important because these models are really large, so now more than ever, systems are actually really an important topic for LLMs. So, those five components, you probably all know that LLMs, and if you don't know, LLMs are all based on transformers, or at least some version of transformers. I'm actually not going to talk about the architecture today, one, because I gave a lecture on transformers a few weeks ago, and two, because you can find so much information online on transformers, but I think you can- there's much less information about the other four topics, so I really want to talk about those. Another thing to say is that most of academia actually focuses on architecture and training algorithm and losses. As academics, and I've done that for a lot- a big part of my career, is simply we like thinking that this is like we make new architectures, new models, and it seems like it's very important, but in reality, honestly, what matters in practice is mostly the three other topics, so data, evaluation, and systems, which is what most of industry actually focuses on. So that's also one of the reasons why I don't want to talk too much about the architecture, because really the rest is super important. Great, so overview of the lecture, I'll be talking about pre-training. So pre-training, you probably heard that word, this is the general word, this is kind of the classical language modeling paradigm, where you basically train your language model to essentially model all of internet. And then there's a post-training, which is a more recent paradigm, which is taking these large language models and making them essentially AI assistants. So this is more of a recent trend since ChatGPT. So if you ever heard of GPT-3 or GPT-2, that's really pre-training land. If you heard of ChatGPT, which you probably have, this is really post-training land. So I'll be talking about both, but I'll start with pre-training. And specifically, I'll talk about what is the task of pre-training LLMs and what is the laws that people actually use. So language modeling, this is a quick recap. Language models at a high level are simply models of probability distribution over sequences of tokens or of words. So it's basically some model of p of x1 to xl, where x1 is basically word 1 and xl is the last word in the sequence or in the sentence. So very concretely, if you have a sentence like the mouse ate the cheese, what the language model gives you is simply a probability of this sentence being uttered by a human or being found online. So if you have another sentence like the mouse ate cheese, here there's grammatical mistakes. So the model should know that this should have some syntactic knowledge. So it should know that this has less likelihood of appearing online. If you have another sentence like the cheese ate the mouse, then the model should hopefully know about the fact that usually cheese don't eat mouse. So there's some semantic knowledge and this is less likely than the first sentence. So this is basically at a high level what language models are. One word that you probably have been hearing a lot in the news are generative models. So this is just something that can generate models that can generate sentences or can generate some data. The reason why we say language models are generative models is that once you have a model of a distribution, you can simply sample from this model and then we can generate data. So you can generate sentences using a language model. So the type of models that people are all currently using are what we call autoregressive language models. And the key idea of autoregressive language models is that you take this distribution over words and you basically decompose it into the distribution of the first word, multiply it by the distribution of the likelihood of the distribution of the second word given the first word, and multiply it by p of the third word given the first two words. So there's no approximation here. This is just the chain rule of probability, which hopefully you all know about, really no approximation. This is just one way of modeling a distribution. So slightly more concisely, you can write it as a product of p's of the next word given everything which happened in the past, so of the context. So this is what we call autoregressive language models. Again, this is really not the only way of modeling distribution, this is just one way. It has some benefits and some downsides. One downside of autoregressive language models is that when you actually sample from this autoregressive language model, you basically have a for loop which generates the next word, then conditions on that next word, and then regenerate the other word. So basically, if you have a longer sentence that you want to generate, it takes more time to generate it. So there are some downsides of this current paradigm, but that's what we currently have, so I'm going to talk about this one. Great. So autoregressive language models. At a high level, what the task of autoregressive language model is, is simply predicting the next word, as I just said. So if you have a sentence like she likely prefers, one potential next word might be dogs. And the way we do it is that we first tokenize. So you take these words or sub words, you tokenize them, and then you give an ID for each token. So here I have one, two, three. Then you pass it through this black box. As I already said, we're not going to talk about the architecture. You just pass it through a model, and you then get a distribution, a probability distribution over the next word, over the next token. And then you sample from this distribution, you get a new token, and then you de-tokenize. So you get a new ID, you de-tokenize, and that's how you basically sample from a language model. One thing which is important to note is that the last two steps are actually only needed during inference. When you do training, you just need to predict the most likely token, and you can just compare to the real token, which happened in practice, and then you basically change the weights of your model to increase the probability of generating that token. Great. So autoregressive neural language models. So to be slightly more specific, still without talking about the architecture, the first thing we do is that we have all of these- oh, sorry, yes? On the previous slide, when you're predicting the probability of the next token, does this mean that your final output vector has to be the same dimensionality as the number of tokens that you have? Yes. How do you deal with if you're adding more tokens to your co-presenters? Yeah. So we're going to talk about tokenization actually later, so you will get some sense of this. You basically can't deal with adding new tokens. I'm kind of exaggerating. There are methods for doing it, but essentially people don't do it. So it's really important to think about how you tokenize your text, and that's why we'll talk about that later. But it's a very good point to note is that basically the vocabulary size, so the number of tokens that you have, is essentially the output of your language model. So it's actually pretty large. Okay, so autoregressive neural language models. First thing you do is that you take every word or every token. You embed them, so you get some vector representation for each of these tokens. You pass them through some neural network, as we said, it's a transformer. Then you get a representation for all the words in the context. So it's basically a representation of the entire sentence. You pass it through a linear layer, as you just said, to basically map it to the number so that the output, the number of outputs is the number of tokens. You then pass it through some softmax, and you basically get a probability distribution over the next words given every word in the context. And the laws that you use is basically, it's essentially a task of classifying the next token. So it's a very simple kind of machine learning task. So you use the cross-entropy laws, where you basically look at the actual target that happened, which is a target distribution, which is a one-hot encoding, which here in this case says, I saw the real word that happened is cat. So that's a one-hot distribution over cat. And here, this is the distribution that you generated. And basically, you do cross-entropy, which really just increases the probability of generating cat and decreases the probability of generating all the other tokens. One thing to notice is that, as you all know, again, this is just equivalent to maximizing the text log likelihood, because you can just rewrite the max over the probability of this autoregressive language modeling task as just being this minimum of, I just added the log here and minus, which is just the minimum of the loss, which is the cross-entropy loss. So basically, minimizing the loss is the same thing as maximizing the likelihood of your text. Any questions? OK, tokenizer. So this is one thing that people usually don't talk that much about. Tokenizers are extremely important. So it's really important that you understand, at least, what they do at a high level. So why do we need tokenizers in the first place? First, it's more general than words. So one simple thing that you might think is, oh, we're just going to take every word that we will have. And you just say, every word is a token in its own. But then what happens is, if there's a typo in your word, then you might not have any token associated with this word with a typo. And then you don't know how to actually pass this word with a typo into a large language model. So what do you do next? And also, even if you think about words, words are fine with Latin-based languages. But if you think about a language like Thai, you won't have a simple way of tokenizing by spaces, because there are no spaces between words. So really, tokens are much more general than words. First thing. Second thing that you might think is that you might tokenize every sentence, character by character. You might say, A is one token, B is another token. That would actually work, and probably very well. The issue is that then your sequence becomes super long. And as you probably remember from the lecture on transformers, the complexity grows quadratically with the length of sequences. So you really don't want to have a super long sequence. So tokenizers basically try to deal with those two problems and give common sub-sequences a certain token. And usually, how you should be thinking about it is around an average of every token is around three, four letters. And there are many algorithms for tokenization. I'll just talk about one of them to give you a high level, which is what we call byte-pair encoding, which is actually pretty common, one of the two most common tokenizers. And the way that you train a tokenizer is that first, you start with a very large corpus of text. And here, I'm really not talking about training a large language model yet. This is purely for the tokenization step. So this is my large corpus of text with these five words. Then you associate every character in this corpus of text a different token. So here, I just split up every character with a different token, and I just color coded all of those tokens. And then what you do is that you go through your text, and every time you see pairs of tokens that are very common, the most common pair of token, you just merge them. So here, you see three times the tokens T and O next to each other, so you're just going to say this is a new token. And then you continue. You repeat that. So now you have T-O-K, TOK, which happens three times, TOK with an E, that happens two times, and TOKEN, which happens twice, and then EX, which also happens twice. So this is that if you were to train a tokenizer on this corpus of text, which is very small, that's how you would finish with a trained tokenizer. In reality, you do it on much larger corpuses of text. And this is the real tokenizer of, actually, I think this is GPT-3 or ChatGPT. And here, you see how it would actually separate these words. So basically, you see the same thing as what we gave in the previous example, TOKEN becomes its own token. So tokenizer is actually split up into two tokens, TOKEN and EIZER. So yeah, that's all about tokenizers. Any question on that? Yeah? How do you deal with spaces, and how do you deal with introduction? Yeah. So actually, there's a step before tokenizers, which is what we call pre-tokenizers, which is exactly what you just said. So this is mostly, in theory, there's no reason to deal with spaces and punctuation separately. You could just say every space gets its own token, every punctuation gets its own token, and you could just do all the merging. The problem is that, so there's an efficiency question. Actually, training these tokenizers takes a long time. So you're better off, because you have to consider every pair of token. So what you end up doing is saying, if there's a space, this is very, like pre-tokenizers are very English-specific. So you say, if there's a space, we're not going to start looking at the token that came before and the token that came afterwards. So you're not merging in between spaces. But this is just like a computation optimization. You could theoretically just deal with it the same way as you deal with any other character. Yeah? When you merge tokens, do you delete the tokens that you merged away, or do you keep the smaller tokens that you merged? You actually keep the smaller tokens. I mean, in reality, it doesn't matter much, because usually on large corpus of text, you will have actually everything. But you usually keep the small ones. And the reason why you want to do that is because if, in case there's a, as we said before, you have some grammatical mistakes or some typos, you still want to be able to represent these words by character. So yeah. Yes? Are the tokens unique? So, I mean, say, in this case, T-O-K-E-N, is there only one occurrence, or do you need to leave multiple occurrence so they can take on different meanings or something? Oh, oh, I see what you say. No, no, no. It's every token has its own unique ID. So a usual, this is a great question. For example, if you think about a bank, which could be bank for money or bank like water, they will have the same token, but the model will learn, the transformer will learn that based on the words that are around it, it should associate that, I'm saying, I'm being very hand-wavy here, but associate that with a representation that is either more like the bank money side or the bank water side. But that's the transformer that does that. It's not a tokenizer. Yes? Yeah, so you mentioned during tokenization, you keep the smaller tokens to start with, right? So if you start with a T, you keep the T, and then you tell your tokenizer to expand that amount of token. So let's say maybe you didn't train on token, but in your data, you are trying to encode token. So how does the tokenizer know to encode it with token or to do it with T? Yeah, it's a great question. You basically, when you, so when you tokenize, so that's after training of the tokenizer, when you actually apply the tokenizer, you basically always choose the largest token that you can apply. So if you can do token, you will never do T. You will always do token. But it's actually, so people don't usually talk that much about tokenizers, but there's a lot of computational benefits or computational tricks that you can do for making these things faster. So I really don't think we, and honestly, I think a lot of people think that we should just get away from tokenizers and just kind of tokenize\\n\\nIt's probably the main two things it's used for. So if you want your model shared, then you would do that. But if you want it just saved on your computer, do saved pre-trained for a local save. By the way, this only saves the LoRa adapters, meaning basically the things that were changed. It doesn't save the entire model with the change parameters, just the changes, right? So it's less memory and just faster to save. But if you want to save the LoRa adapters with the saved model, you can change this. If you want to load the LoRa adapters, we saved for inference, you would change this false to true. So simply changing this. And yeah, this is the model name. So obviously, you can change this. This is your model used for training. Here is just LoRa model, but you can name it Lama3. I don't know, copywriting, or Lama3 medical diagnosis, whatever your use case is, obviously. And then here, the Alpaca prompt. So yeah, this is the variable we declared earlier. So this is the importance. You can't just go into the Colab and try to running this cell. You have to run the cells from above. Otherwise, this will not work. So whenever you're using a Jupyter Notebook, such as Google Colab, always run all cells in order. Otherwise, it will not work. So yeah. So this is the same format, right, from earlier instruction input-output. At this point, you should be familiar with this. And that's just for this particular data set and for this style of prompting. So if you have a different one, then follow the different one. So obviously, here, what is a famous tower in Paris? Obviously, it's Eiffel Tower, blah, blah, blah. It gives some extra info about it. So you can also use Hugging Face AutoModel for PerfCasualLM. But Unslof does not recommend this because it's a lot slower than Unslof. So yeah, if possible, use Unslof for speed. And as the name suggests of Unslof, it's unslowing everything. It's making everything two to five times faster. So why not do that with 80% less memory? So yeah. OK, and then we're preparing to save our trained model in a more compact format and then upload it into a cloud platform, which allows for less storage and compression power. So again, I'm not going to even pretend I understand everything because this is honestly stepping outside of my comfort zone. But just building this and doing this fine-tuning taught me a lot. So if you want more technical videos like this, let me know. Next, we're ready to compress our model using various quantization methods, which means just making it easier to run on a machine. So maybe you cannot, like if you have a bad computer, maybe you cannot run the full model, but you definitely can run a quantized version of it. It makes it leaner, and then we upload it to the cloud for easy sharing. And this is what this piece of code does. And so we use the model-unslof.gguf file or the quantized version. So the Q4 means quantized in LlamaCCP. Or if you want a UI-based system, which probably you do, which is easier to use, you can use GPT4 or this other one. It's escaping me. But yeah, these are basically these UI-based systems that you can use to LLM anything. Yeah, I don't know if this supports it. But yeah, basically, these frameworks have a UI that's easy to chat with. And you can use open source model there. So if you fine-tune this, you can upload this to GPT4 and chat with your own model very easily. And yeah, that's it. You know how to fine-tune Llama3 for your own specific use case. Again, I'm going to leave these resources below the video. And if you have any questions regarding to unslof, join their Discord. So yeah, that's it. If you find this useful, then please subscribe. And again, if you want, during April, which is, what, like eight days, nine days left, if you join the community, you will get a personalized AI strategy to future-proof yourself and your business. So if that sounds valuable to you, then make sure to join. It's the first link in the description. Thank you for watching.\\n\\nMy name is David Andrzej and in this video, I'll teach you how to fine-tune Lama3 so that it performs 10 times better for your specific use case. Let's start with what even is fine-tuning and I made this explanation in plain English so that anybody can understand. Fine-tuning is adapting a pre-trained LLM like GPT-4 or in this case Lama3 to a specific task or domain. It involves adjusting a small portion of the parameters on a more focused dataset. So, you know, when a new model releases, what everybody needs to know is how many parameters it has. We have Lama3 8B and always that number like 8B or 70B, that's the number of parameters. So we're adjusting just a small number of them to make it more focused on a specific thing. Fine-tuning customizes the outputs to be more relevant and accurate for your use case. Here's the power of fine-tuning. Cost-effectiveness. It leverages the power of pre-trained LLMs which cost tens of millions of dollars, if not hundreds of millions, to train and we can just, you know, run a GPU for a few hours and fine-tune something for, I don't know, like cents, a few cents or a few dollars at most, which is just amazing. It gives you improved performance because you can enhance the LLM on your dataset and improve accuracy for specific tasks. And it also is more data efficient. You can achieve excellent results even with smaller datasets. So, you know, maybe even like 300, 500 entries while, you know, Lama3 was trained on 15 trillion tokens. I don't know about you, but I don't have nearly as much data as Zack. So that's why fine-tuning is great for people like you and me. So how does LLM fine-tuning actually work? First, you need to prepare your dataset. And this, you know, depending on how hardcore you want to go, this can take anywhere from 20 minutes to a few hours to weeks, potentially. Depends how far you want to take it. So you create a smaller, high-quality dataset tailored to your specific use case and label it appropriately, which I'll teach you in a bit. The pre-trained LLMs weights are updated incrementally using the optimization algorithms like gradient descent based on the new dataset. So we can only fine-tune LLMs that we have access to the weights, meaning open-source, open-weights LLMs. You cannot fine-tune GPT-4 if you are not OpenAI. OpenAI can do it, obviously, but me and you, we probably don't have GPT-4 just laying on our computer. Then you monitor and refine. You evaluate the model's performance on a validation set, preventing overfitting and guide adjustments. Now, here are some real-world use cases for fine-tuning. Fine-tuning an LLM on customer service transcripts can create a chatbot, like this one, that can address issue in a way specific to your company. So let's say you have a specific product, very niche, that there is not much data about it on the internet. And if somebody messages your customer support email, you want your chatbot to respond in a specific way based on the information of your product. And that data is proprietary. It's private. Only you have it. And you can fine-tune an LLM to respond based on that data. So, like, technically, if you have enough scripts, you can fine-tune an LLM to respond like you. And, you know, if you try chat GPT, if you even give, like, chat GPT some writing and tell it, continue in this writing style, it's terrible. So this is where fine-tuning could be better. Tailored content generation. So you can fine-tune an LLM on your posts and descriptions to create engaging summaries or marketing copy, again, in your writing style, tailored to your audience. Domain-specific analysis. So fine-tuning LLM on legal or medical text can make it much better for those specific benchmarks. So you might have a model that, let's say, it reaches 50 on some arbitrary benchmark. With fine-tuning, it can reach 70 or 80. Now let's dive into how to actually implement this on Llama3. So I created this Google collab. Well, actually, most of it was created by Anslov team. A huge shout out to Anslov because they did all the heavy lifting. So I'm going to also link their GitHub below. Now, first off, I added a component that's only available in April to the community. So if you join during April, you will get a personalized AI strategy to future-proof yourself and your business. So if you want to be among people who are building the future, if you want access to all the different courses, modules, and everything else in the community, and to two weekly calls, then consider joining. And especially if you want me to give you a personalized AI strategy to future-proof yourself. So if that sounds interesting to you, make sure to join the community. It's the first link in the description. Now let's fine-tune LLama3, shall we? So first thing, we check the GPU version available in the environment and install specific dependencies that are comparable with the detected GPU to prevent conflicts. So this is this cell. By the way, if you don't know how Google Colab works, which is, you know, the software I'm using right now, it's super simple. It's basically splitting the code into cells. It's called the Jupyter Notebook, but it's like much more easier to see. You can add text, you can add graphics, and it's great for like tutorials and explaining, right? So if you never use this, it's great because it's free. And Google actually gives you a GPU so you can use this T4 GPU to train this model for free. And if you want faster, you can obviously upgrade it, right? So I'm going to link this Colab below the video as well. So we run this cell, which does what I just explained. The next cell, we need to prepare to load a range of quantized language models, including the new 15 trillion LLama3 model, so trained on 15 trillion tokens. And it's optimized for efficiency with 4-bit quantization. I mean, I'm not going to even pretend I know everything about fine-tuning because I don't. So if it seems like I have gaps in my knowledge, it's because it is. I do have those gaps in my knowledge. So I try to make it as simple as possible. But if this proves something, it proves that you don't have to be a machine learning expert to fine-tune models. So just follow along. So here, this is the max sequence length. Obviously, LLama3 is up to 8,000. So I mean, 2,000 is plenty for this demonstration, but you can do anything. You can do 4,000 or 8,000. Here, I use 4-bit quantization to reduce memory usage, but it can be false as well. So here are the models. We can see, like, we have Mistral7B, LLama2, which is the old one, Gemma from Google. But obviously, we're interested in LLama3 8B. And by the way, we can also use LLama3 70B if you want, which obviously will take longer because it's a much bigger model. So in that case, you might want to buy the premium version of Collab or just wait for a while. But yeah, I mean, everything is the same. Just here, you would change the model to LLama3 70B. And if you want to use, like, gated models from HuggingFace, which gated means that you have to usually agree to some, you know, license or whatever, then here, just remove the comment and then put your HuggingFace token here. Super simple. Now, by the way, you always have to run this. So what do you do when you go to Google Collab? You click on runtime and click run all. That way, all of the cells run. But you can also do it one by one by clicking this button right here next to each cell. And it needs to have this little tick, green tick. That way, it was executed. Here, it's not because I, you know, removed the... I changed this. So anytime you make any change, it disappears. But that doesn't matter. It was still executed. So it's stored in the runtime. Next up, we integrate LoRa. Again, you don't have to understand what this is, but it's basically a way of fine-tuning into our model, which allows us to efficiently update just a fraction of the parameters, enhancing training speed and reducing computation load. So again, we are not training the model from scratch. We're just fine-tuning a few parameters for our specific use case. And here, you can change the R to any number greater than 0, 8, 16, 32, 64, up to you. And your goals, what you want to do with it. By the way, on Sloth, the reason I'm using it is because it makes fine-tuning much faster and consuming less memory. So it's actually a great framework for this. Dataprep. We now use the Alpaca dataset from Yama, which is this one, which has 50,000 rows. And I have it loaded in VS code here. Just that way, you see how it looks like in JSON formatting. So, you know, it's a lot of lines because for everyone, it's basically times five. Yeah, so like 250,000 lines. And it's like every one of them has an instruction. I should probably zoom it in. So every one entry has an instruction. Give three tips for staying healthy. Input, this is not mandatory because instruction is already enough context. And then output, this is what the LLM should say. And you do this enough times and the LLM learns. It basically learns, right? So we can see it probably better here. And if you want to use your own dataset, you have to format it the same way. So, you know, just having output, input and instructions, these three parameters. But yeah, just look at this. Not all of them have the input, which is fine. I mean, probably like 20% or 15% have the input. And that's just extra context. So yeah, I'm also going to link this dataset below. But if you want your own dataset, which, you know, if you want your own use case, just make sure to format it the same way. So, you know, instruction, some text, input, some extra context or empty, and output, how the model should respond. And, you know, if you're getting creative, you can definitely use LLMs to generate these large datasets much faster. I mean, maybe you create really like 20 high quality examples by hand. And then you run a team of agents for creating that dataset that can just, you know, use those 20 examples to create 50,000. Like in this dataset. But yeah, that's a topic for a whole nother video. So if you want me to make a video on how to make datasets for fine tuning, then let me know. But let's go back to our Colab. So then we define a system prompt, which is, you know, custom instruction system prompt, which you already know, hopefully. That formats tasks into instruction inputs and responses. So this has to fit with our dataset. And we apply it to our dataset for the model. And we add the EOS token to signal completion. So this token right here, here we define it. And here we add it because without this, the token generation continues forever. So we don't want that, obviously. So let's look at the system prompt. It's very simple. It says below is an instruction that describes a task paired with an input that provides further context. Write a response that appropriately completes the request. And that's our system prompt. And then we feed it the instruction, the input and response. And obviously you can change the system prompt if you want. Now, train the model. We do a 60 step. We do only 60 steps here to speed things up. You can, like, this is obviously very small because it's not even one epoch, training epoch. So if you want to like actually use something for production or your business, you probably want to train it for longer than 60 steps. And I'm going to show you how in this bit. So if you do multiple epochs, you have to turn max steps none. So here, okay. Number of trained epochs is not included in here. So what you would do is you would copy this and you would go in here and look at the steps, right? So we have the steps here. You would add this. Maybe you would do four or whatever, however many you want. The more, the better. But at a certain point, it starts to not yield better result. So max steps, you have to change it to none, right? So it's like 60 right now. So you do none. And this is where you would do like proper fine tuning. But, you know, I just added that 60 for demonstration. That way it's faster. And it still took like eight minutes. So I'm not going to replicate it. I'm just going to show you everything. But yeah, basically, you know, this is what you do. You decide how many epochs you want. And then at this stage, we're configuring our model's training setup where we define things like batch size and learning rate to teach our model effectively with the data we've prepared. So obviously you can like mess with stuff here. Again, I'm not going to pretend I understand everything. But the main things are, you know, packing like this can make it five times faster for short sequences. Obviously, the steps and the epochs. But yeah, I mean, if you're confused something, just take a screenshot. Boom, like this. And ask ShedGPD. Now, this is the current memory stats, right? So we're using the Tesla T4 GPU provided from Google for free. And the max memory is 14 gigabytes. And this is where the training begins. This is the magical part, right? So here we do this line of code, trainer stats, trainer.train. And this will give us the statistics as the model trains. So again, this is only 60 steps, which is like zero epochs. But yeah, you can see the training loss going down. So like basically, smaller number is better here. So you can see like at the start, we have 1.8. Like 1.9, and then it quickly starts dropping to like 0.9, you know, around 1, 0.8. So it fluctuates a bit, but it consistently goes down 0.7. But you can see it's reaching like a asymptote, right? Obviously, it's only 60 steps. So it really doesn't mean anything. But yeah, like we ended up like 0.8 from like 2. So it shows you like if the model is actually improving. So it shows you like if the model is actually improving. And this took like eight minutes. So you can see the stats here, right? So 476 seconds, almost exactly eight minutes. Peak reserve memory was 8.9 gigabytes. And for training was 3.3 gigabytes. So not like this is the power of Unslof. It's like really optimized for this to use, to run faster and to use less memory. So that way we can fine tune GPUs for cheaper. I mean, you know, I'm using a free T4 GPU from Google. So it's free, but it's faster. Like if you didn't use Unslof, it would be a lot slower. So, okay, so 60% of, we used 60% of max memory. So that's good because we didn't like hit the limit. So we still have like 40% reserved. And for training, it was only 22%, which is even better. Inference, which means here we actually run our new model that we fine tuned. And okay, so this data set is for like instructions. And this is basically when you see a model that is like instruct at the end of it, this is what they mean. It's just trained on a large data set of instructions. Because usually the models are more for like chatting, for text generation. You know, you give it some input and it's like gives you some output. It's, you know, for more conversational. Here for instructions, for the instruct models is to follow instructions. You give it a task and it completes it. So like we can see it probably here in VS code, like rewrite the sentence to change its meaning and then output the thief escaped. Compared to data subs, so this is like all tasks. It's all in instructions. And then it shows how the model should do it. So let's look at it, right? So now we've trained the model. This took like eight minutes to do. So all of you can do this. The beauty of using a Google Cloud is that obviously it doesn't matter what machine you have. Even if you have a terrible computer, this will take the exact same time because you're using the GPU and cloud. So obviously here you can change your prompt. I mean, this is, you know, I changed the prompts here. So this is my prompt. But always make sure to leave the output blank. So here, the first one is the instruction. Then this is the input, like the extra added context and the output, leave it blank because the model will generate it, right? So list the prime numbers contained within\\n\\nHello, and welcome to Introduction to Large Language Models. My name is John Ewald, and I'm a training developer here at Google Cloud. In this course, you'll learn to define large language models, or LLMs, describe LLM use cases, explain prompt tuning, and describe Google's GenAI development tools. Large language models, or LLMs, are a subset of deep learning. To find out more about deep learning, see our Introduction to Generative AI course video. LLMs and generative AI intersect, and they are both a part of deep learning. Another area of AI you may be hearing a lot about is generative AI. This is a type of artificial intelligence that can produce new content, including text, images, audio, and synthetic data. So what are large language models? Large language models refer to large, general-purpose language models that can be pre-trained and then fine-tuned for specific purposes. What do pre-trained and fine-tuned mean? Imagine training a dog. Often you train your dog basic commands, such as sit, come, down, and stay. These commands are normally sufficient for everyday life and help your dog become a good canine citizen. However, if you need a special service dog, such as a police dog, a guide dog, or a hunting dog, you add special trainings. This similar idea applies to large language models. These models are trained for general purposes to solve common language problems such as text classification, question answering, document summarization, and text generation across industries. The models can then be tailored to solve specific problems in different fields such as retail, finance, and entertainment using a relatively small size of field datasets. Let's further break down the concept into three major features of large language models. Large indicates two meanings. First is the enormous size of the training dataset, sometimes at the petabyte scale. Second, it refers to the parameter count. In ML, parameters are often called hyperparameters. Hyperparameters are basically the memories and the knowledge that the machine learned from the model training. Parameters define the skill of a model in solving a problem, such as predicting text. General purpose means that the models are sufficient to solve common problems. Two reasons lead to this idea. First is the commonality of a human language, regardless of the specific tasks, and second is the resource restriction. Only certain organizations have the capability to train such large language models with huge datasets and a tremendous number of parameters. How about letting them create fundamental language models for others to use? This leads to the last point, pre-trained and fine-tuned, meaning to pre-train a large language model for a general purpose with a large dataset and then fine-tune it for specific aims with a much smaller dataset. The benefits of using large language models are straightforward. First, a single model can be used for different tasks. This is a dream come true. These large language models that are trained with petabytes of data and generate billions of parameters are smart enough to solve different tasks including language translation, sentence completion, text classification, question answering, and more. Second, large language models require minimal field training data when you tailor them to solve your specific problem. Large language models obtain decent performance even with little domain training data. In other words, they can be used for few-shot or even zero-shot scenarios. In machine learning, few-shot refers to training a model with minimal data, and zero-shot implies that a model can recognize things that have not explicitly been taught in the training before. Third, the performance of large language models is continuously growing when you add more data and parameters. Let's take Palm as an example. In April 2022, Google released Palm, short for Pathways Language Model, a 540 billion parameter model that achieves a state-of-the-art performance across multiple language tasks. Palm is a dense decoder-only transformer model. It has 540 billion parameters. It leverages the new Pathways system, which has enabled Google to efficiently train a single model across multiple TPUv4 pods. Pathway is a new AI architecture that will handle many tasks at once, learn new tasks quickly, and reflect a better understanding of the world. The system enables Palm to orchestrate distributed computation for accelerators. We previously mentioned that Palm is a transformer model. A transformer model consists of encoder and decoder. The encoder encodes the input sequence and passes it to the decoder, which learns how to decode the representations for a relevant task. We've come a long way from traditional programming to neural networks to generative models. In traditional programming, we used to have to hard-code the rules for distinguishing a cat. Type? Animal. Legs? Four. Ears? Two. Fur? Yes. Likes? Yarn. Catnip. In the wave of neural networks, we could give the network pictures of cats and dogs and ask, is this a cat? And it would predict a cat. In the generative wave, we, as users, can generate our own content, whether it be text, images, audio, video, or other. For example, models like Palm or Lambda, or language model for dialogue applications, ingest very, very large data from multiple sources across the internet, and build foundation language models we can use simply by asking a question, whether typing it into a prompt or verbally talking into the prompt. So when you ask it, what's a cat? It can give you everything it has learned about a cat. Let's compare LLM development using pre-trained models with traditional ML development. First, with LLM development, you don't need to be an expert. You don't need training examples, and there is no need to train a model. All you need to do is think about prompt design, which is the process of creating a prompt that is clear, concise, and informative. It is an important part of natural language processing. In traditional machine learning, you need training examples to train a model. You also need compute time and hardware. Let's take a look at an example of a text generation use case. Question answering, or QA, is a subfield of natural language processing that deals with the task of automatically answering questions posed in natural language. QA systems are typically trained on a large amount of text and code, and they are able to answer a wide range of questions including factual, definitional, and opinion-based questions. The key here is that you need domain knowledge to develop these question answering models. For example, domain knowledge is required to develop a question answering model for customer IT support, or healthcare, or supply chain. Using generative QA, the model generates free text directly based on the context. There is no need for domain knowledge. Let's look at three questions given to BARD, a large-language model chatbot developed by Google AI. Question 1. This year's sales are $100,000. Expenses are $60,000. How much is net profit? BARD first shares how net profit is calculated, then performs the calculation. Then BARD provides the definition of net profit. Here's another question. Inventory on hand is 6,000 units. A new order requires 8,000 units. How many units do I need to fill to complete the order? Again, BARD answers the question by performing the calculation. And our last example, we have 1,000 sensors in 10 geographic regions. How many sensors do we have on average in each region? BARD answers the question with an example on how to solve the problem and some additional context. In each of our questions, a desired response was obtained. This is due to prompt design. Prompt design and prompt engineering are two closely related concepts in natural language processing. Both involve the process of creating a prompt that is clear, concise, and informative. However, there are some key differences between the two. Prompt design is the process of creating a prompt that is tailored to the specific task that this system is being asked to perform. For example, if the system is being asked to translate a text from English to French, the prompt should be written in English and should specify that the translation should be in French. Prompt engineering is the process of creating a prompt that is designed to improve performance. This may involve using domain-specific knowledge, providing examples of the desired output, or using keywords that are known to be effective for the specific system. Prompt design is a more general concept while prompt engineering is a more specialized concept. Prompt design is essential while prompt engineering is only necessary for systems that require a high degree of accuracy or performance. There are three kinds of large language models, generic language models, instruction-tuned, and dialogue-tuned. Each needs prompting in a different way. Generic language models predict the next word based on the language in the training data. This is an example of a generic language model. The next word is a token based on the language in the training data. In this example, the cat sat on, the next word should be the, and you can see that the is the most likely next word. Think of this type as an autocomplete in search. In instruction-tuned, the model is trained to predict a response to the instructions given in the input. For example, summarize a text of x, generate a poem in the style of x, give me a list of keywords based on semantic similarity for x. And in this example, classify the text into neutral, negative, or positive. In dialogue-tuned, the model is trained to have a dialogue by the next response. Dialogue-tuned models are a special case of instruction-tuned where requests are typically framed as questions to a chatbot. Dialogue-tuning is expected to be in the context of a longer back-and-forth conversation and typically works better with natural question-like phrasings. Chain of thought reasoning is the observation that models are better at getting the right answer when they first output text that explains the reason for the answer. Let's look at the question. Roger has five tennis balls. He buys two more cans of tennis balls. Each can has three tennis balls. How many tennis balls does he have now? This question is posed initially with no response. The model is less likely to get the correct answer directly. However, by the time the second question is asked, the output is more likely to end with the correct answer. A model that can do everything has practical limitations. Task-specific tuning can make LLMs more reliable. FlexAI provides task-specific foundation models. Let's say you have a use case where you need to gather sentiments, or how your customers are feeling about your product or service. You can use the classification task Sentiment Analysis Task Model. Same for vision tasks. If you need to perform occupancy analytics, there is a task-specific model for your use case. Tuning a model enables you to customize the model response based on examples of the task that you want the model to perform. It is essentially the process of adapting a model to a new domain or set of custom use cases by training the model on new data. For example, we may collect training data and tune the model specifically for the legal or medical domain. You can also further tune the model by fine-tuning, where you bring your own dataset and retrain the model by tuning every weight in the LLM. This requires a big training job and hosting your own fine-tuned model. Here's an example of a medical foundation model trained on healthcare data. The tasks include question answering, image analysis, finding similar patients, and so forth. Fine-tuning is expensive and not realistic in many cases. So are there more efficient methods of tuning? Yes, Parameter Efficient Tuning Methods, or PETM, are methods for tuning a large-language model on your own custom data without duplicating the model. The base model itself is not altered. Instead, a small number of add-on layers are tuned, which can be swapped in and out at inference time. Generative AI Studio lets you quickly explore and customize generative AI models that you can leverage in your applications on Google Cloud. Generative AI Studio helps developers create and deploy generative AI models by providing a variety of tools and resources that make it easy to get started. For example, there's a library of pre-trained models, a tool for fine-tuning models, a tool for deploying models to production, and a community forum for developers to share ideas and collaborate. Generative AI App Builder lets you create Gen AI apps without having to write any code. Gen AI App Builder has a drag-and-drop interface that makes it easy to design and build apps, a visual editor that makes it easy to create and edit app content, a built-in search engine that allows users to search for information within the app, and a conversational AI engine that allows users to interact with the app using natural language. You can create your own chatbots, digital assistants, custom search engines, knowledge bases, training applications, and more. Palm API lets you test and experiment with Google's large language models and Gen AI tools. To make prototyping quick and more accessible, developers can integrate Palm API with Makersuite and use it to access the API using a graphical user interface. The suite includes a number of different tools, such as a model training tool, a model deployment tool, and a model monitoring tool. The model training tool helps developers train ML models on their data using different algorithms. The model deployment tool helps developers deploy ML models to production with a number of different deployment options, and the model monitoring tool helps developers monitor the performance of their ML models in production using a dashboard and a number of different metrics. That's all for now. Thanks for watching this course, Introduction to Large Language Models.\", 'answer': 'The purpose of a Large Language Model (LLM) is to understand and generate human language based on patterns it has learned from vast amounts of text data. LLMs are a type of neural network designed to handle a wide range of tasks, including summarization, text generation, creative writing, question-answering, programming, and more. They are trained on massive datasets from various sources like web pages, books, and transcripts, allowing them to learn and adapt to different contexts and tasks.'}\n",
      "{'input': 'How can I train them?', 'chat_history': [HumanMessage(content='Hi my name is Obi-Wan, what is the purpose of an LLM?', additional_kwargs={}, response_metadata={}, id='df4b4ce7-3a69-4865-b825-902c8c2a9c06'), AIMessage(content='The purpose of a Large Language Model (LLM) is to understand and generate human language based on patterns it has learned from vast amounts of text data. LLMs are a type of neural network designed to handle a wide range of tasks, including summarization, text generation, creative writing, question-answering, programming, and more. They are trained on massive datasets from various sources like web pages, books, and transcripts, allowing them to learn and adapt to different contexts and tasks.', additional_kwargs={}, response_metadata={}, id='9f050a0f-10d9-4255-8424-207ec27a7305'), HumanMessage(content='Hi my name is Obi-Wan, what is the purpose of an LLM?', additional_kwargs={}, response_metadata={}, id='cee5cdce-60ca-4b17-a9a7-f161deb66fa0'), AIMessage(content='The purpose of a Large Language Model (LLM) is to perform a variety of natural language processing tasks such as text classification, question answering, document summarization, and text generation. LLMs are pre-trained on large datasets and can be fine-tuned for specific purposes, making them versatile tools for various applications. They are designed to understand and generate human language effectively.', additional_kwargs={}, response_metadata={}, id='9ef736e6-f305-41ba-98d4-f5d5b9b4e303')], 'context': \"Hello, and welcome to Introduction to Large Language Models. My name is John Ewald, and I'm a training developer here at Google Cloud. In this course, you'll learn to define large language models, or LLMs, describe LLM use cases, explain prompt tuning, and describe Google's GenAI development tools. Large language models, or LLMs, are a subset of deep learning. To find out more about deep learning, see our Introduction to Generative AI course video. LLMs and generative AI intersect, and they are both a part of deep learning. Another area of AI you may be hearing a lot about is generative AI. This is a type of artificial intelligence that can produce new content, including text, images, audio, and synthetic data. So what are large language models? Large language models refer to large, general-purpose language models that can be pre-trained and then fine-tuned for specific purposes. What do pre-trained and fine-tuned mean? Imagine training a dog. Often you train your dog basic commands, such as sit, come, down, and stay. These commands are normally sufficient for everyday life and help your dog become a good canine citizen. However, if you need a special service dog, such as a police dog, a guide dog, or a hunting dog, you add special trainings. This similar idea applies to large language models. These models are trained for general purposes to solve common language problems such as text classification, question answering, document summarization, and text generation across industries. The models can then be tailored to solve specific problems in different fields such as retail, finance, and entertainment using a relatively small size of field datasets. Let's further break down the concept into three major features of large language models. Large indicates two meanings. First is the enormous size of the training dataset, sometimes at the petabyte scale. Second, it refers to the parameter count. In ML, parameters are often called hyperparameters. Hyperparameters are basically the memories and the knowledge that the machine learned from the model training. Parameters define the skill of a model in solving a problem, such as predicting text. General purpose means that the models are sufficient to solve common problems. Two reasons lead to this idea. First is the commonality of a human language, regardless of the specific tasks, and second is the resource restriction. Only certain organizations have the capability to train such large language models with huge datasets and a tremendous number of parameters. How about letting them create fundamental language models for others to use? This leads to the last point, pre-trained and fine-tuned, meaning to pre-train a large language model for a general purpose with a large dataset and then fine-tune it for specific aims with a much smaller dataset. The benefits of using large language models are straightforward. First, a single model can be used for different tasks. This is a dream come true. These large language models that are trained with petabytes of data and generate billions of parameters are smart enough to solve different tasks including language translation, sentence completion, text classification, question answering, and more. Second, large language models require minimal field training data when you tailor them to solve your specific problem. Large language models obtain decent performance even with little domain training data. In other words, they can be used for few-shot or even zero-shot scenarios. In machine learning, few-shot refers to training a model with minimal data, and zero-shot implies that a model can recognize things that have not explicitly been taught in the training before. Third, the performance of large language models is continuously growing when you add more data and parameters. Let's take Palm as an example. In April 2022, Google released Palm, short for Pathways Language Model, a 540 billion parameter model that achieves a state-of-the-art performance across multiple language tasks. Palm is a dense decoder-only transformer model. It has 540 billion parameters. It leverages the new Pathways system, which has enabled Google to efficiently train a single model across multiple TPUv4 pods. Pathway is a new AI architecture that will handle many tasks at once, learn new tasks quickly, and reflect a better understanding of the world. The system enables Palm to orchestrate distributed computation for accelerators. We previously mentioned that Palm is a transformer model. A transformer model consists of encoder and decoder. The encoder encodes the input sequence and passes it to the decoder, which learns how to decode the representations for a relevant task. We've come a long way from traditional programming to neural networks to generative models. In traditional programming, we used to have to hard-code the rules for distinguishing a cat. Type? Animal. Legs? Four. Ears? Two. Fur? Yes. Likes? Yarn. Catnip. In the wave of neural networks, we could give the network pictures of cats and dogs and ask, is this a cat? And it would predict a cat. In the generative wave, we, as users, can generate our own content, whether it be text, images, audio, video, or other. For example, models like Palm or Lambda, or language model for dialogue applications, ingest very, very large data from multiple sources across the internet, and build foundation language models we can use simply by asking a question, whether typing it into a prompt or verbally talking into the prompt. So when you ask it, what's a cat? It can give you everything it has learned about a cat. Let's compare LLM development using pre-trained models with traditional ML development. First, with LLM development, you don't need to be an expert. You don't need training examples, and there is no need to train a model. All you need to do is think about prompt design, which is the process of creating a prompt that is clear, concise, and informative. It is an important part of natural language processing. In traditional machine learning, you need training examples to train a model. You also need compute time and hardware. Let's take a look at an example of a text generation use case. Question answering, or QA, is a subfield of natural language processing that deals with the task of automatically answering questions posed in natural language. QA systems are typically trained on a large amount of text and code, and they are able to answer a wide range of questions including factual, definitional, and opinion-based questions. The key here is that you need domain knowledge to develop these question answering models. For example, domain knowledge is required to develop a question answering model for customer IT support, or healthcare, or supply chain. Using generative QA, the model generates free text directly based on the context. There is no need for domain knowledge. Let's look at three questions given to BARD, a large-language model chatbot developed by Google AI. Question 1. This year's sales are $100,000. Expenses are $60,000. How much is net profit? BARD first shares how net profit is calculated, then performs the calculation. Then BARD provides the definition of net profit. Here's another question. Inventory on hand is 6,000 units. A new order requires 8,000 units. How many units do I need to fill to complete the order? Again, BARD answers the question by performing the calculation. And our last example, we have 1,000 sensors in 10 geographic regions. How many sensors do we have on average in each region? BARD answers the question with an example on how to solve the problem and some additional context. In each of our questions, a desired response was obtained. This is due to prompt design. Prompt design and prompt engineering are two closely related concepts in natural language processing. Both involve the process of creating a prompt that is clear, concise, and informative. However, there are some key differences between the two. Prompt design is the process of creating a prompt that is tailored to the specific task that this system is being asked to perform. For example, if the system is being asked to translate a text from English to French, the prompt should be written in English and should specify that the translation should be in French. Prompt engineering is the process of creating a prompt that is designed to improve performance. This may involve using domain-specific knowledge, providing examples of the desired output, or using keywords that are known to be effective for the specific system. Prompt design is a more general concept while prompt engineering is a more specialized concept. Prompt design is essential while prompt engineering is only necessary for systems that require a high degree of accuracy or performance. There are three kinds of large language models, generic language models, instruction-tuned, and dialogue-tuned. Each needs prompting in a different way. Generic language models predict the next word based on the language in the training data. This is an example of a generic language model. The next word is a token based on the language in the training data. In this example, the cat sat on, the next word should be the, and you can see that the is the most likely next word. Think of this type as an autocomplete in search. In instruction-tuned, the model is trained to predict a response to the instructions given in the input. For example, summarize a text of x, generate a poem in the style of x, give me a list of keywords based on semantic similarity for x. And in this example, classify the text into neutral, negative, or positive. In dialogue-tuned, the model is trained to have a dialogue by the next response. Dialogue-tuned models are a special case of instruction-tuned where requests are typically framed as questions to a chatbot. Dialogue-tuning is expected to be in the context of a longer back-and-forth conversation and typically works better with natural question-like phrasings. Chain of thought reasoning is the observation that models are better at getting the right answer when they first output text that explains the reason for the answer. Let's look at the question. Roger has five tennis balls. He buys two more cans of tennis balls. Each can has three tennis balls. How many tennis balls does he have now? This question is posed initially with no response. The model is less likely to get the correct answer directly. However, by the time the second question is asked, the output is more likely to end with the correct answer. A model that can do everything has practical limitations. Task-specific tuning can make LLMs more reliable. FlexAI provides task-specific foundation models. Let's say you have a use case where you need to gather sentiments, or how your customers are feeling about your product or service. You can use the classification task Sentiment Analysis Task Model. Same for vision tasks. If you need to perform occupancy analytics, there is a task-specific model for your use case. Tuning a model enables you to customize the model response based on examples of the task that you want the model to perform. It is essentially the process of adapting a model to a new domain or set of custom use cases by training the model on new data. For example, we may collect training data and tune the model specifically for the legal or medical domain. You can also further tune the model by fine-tuning, where you bring your own dataset and retrain the model by tuning every weight in the LLM. This requires a big training job and hosting your own fine-tuned model. Here's an example of a medical foundation model trained on healthcare data. The tasks include question answering, image analysis, finding similar patients, and so forth. Fine-tuning is expensive and not realistic in many cases. So are there more efficient methods of tuning? Yes, Parameter Efficient Tuning Methods, or PETM, are methods for tuning a large-language model on your own custom data without duplicating the model. The base model itself is not altered. Instead, a small number of add-on layers are tuned, which can be swapped in and out at inference time. Generative AI Studio lets you quickly explore and customize generative AI models that you can leverage in your applications on Google Cloud. Generative AI Studio helps developers create and deploy generative AI models by providing a variety of tools and resources that make it easy to get started. For example, there's a library of pre-trained models, a tool for fine-tuning models, a tool for deploying models to production, and a community forum for developers to share ideas and collaborate. Generative AI App Builder lets you create Gen AI apps without having to write any code. Gen AI App Builder has a drag-and-drop interface that makes it easy to design and build apps, a visual editor that makes it easy to create and edit app content, a built-in search engine that allows users to search for information within the app, and a conversational AI engine that allows users to interact with the app using natural language. You can create your own chatbots, digital assistants, custom search engines, knowledge bases, training applications, and more. Palm API lets you test and experiment with Google's large language models and Gen AI tools. To make prototyping quick and more accessible, developers can integrate Palm API with Makersuite and use it to access the API using a graphical user interface. The suite includes a number of different tools, such as a model training tool, a model deployment tool, and a model monitoring tool. The model training tool helps developers train ML models on their data using different algorithms. The model deployment tool helps developers deploy ML models to production with a number of different deployment options, and the model monitoring tool helps developers monitor the performance of their ML models in production using a dashboard and a number of different metrics. That's all for now. Thanks for watching this course, Introduction to Large Language Models.\\n\\nHey everyone, I'm Shaw and I'm back with a new data science series. In this new series, I'm going to be talking about large language models and how to use them in practice. In this video, I will give a beginner-friendly introduction to large language models and describe three levels of working with them in practice. Future videos in this series will discuss various practical aspects of large language models, things like using OpenAI's Python API, using open-source solutions like the Hugging Phase Transformers library, how to fine-tune large language models, and of course, how to build a large language model from scratch. If you enjoyed this content, please be sure to like, subscribe, and share with others. And if you have any suggestions for me to include in this series, please share those in the comments section below. And so with that, let's get into the video. So to kick off the video series, in this video, I'm going to be giving a practical introduction to large language models. And this is meant to be very beginner-friendly and high level, and I'll leave more technical details and example code for future videos and blogs in this series. So a natural place to start is, what is a large language model, or LLM for short? So I'm sure most people are familiar with ChatGPT, however, if you are enlightened enough to not keep up with new cycles and tech hype and all this kind of stuff, ChatGPT is essentially a very impressive and advanced chatbot. So if you go to the ChatGPT website, you can ask it questions like, what's a large language model? And it will generate a response very quickly, like the one that we are seeing here. And that is really impressive. Like if you were ever on AOL, Instant Messenger, also called AIM, you know, back in early 2000s or in the early days of the internet, there were chatbots then, there have been chatbots for a long time, but this one feels different. Like the text is very impressive and it almost feels human-like. A question you might have when you hear the term large language model is, what makes it large? What's the difference between a large language model and a not large language model? And this was exactly the question I had when I first heard the term. And so one way we can put it is that large language models are a special type of language model. But what makes them so special? And I'm sure there's a lot that can be said about large language models. But to keep things simple, I'm going to talk about two distinguishing properties. The first quantitative and the second qualitative. So first, quantitatively, large language models are large. They have many, many more model parameters than past language models. And so these days, this is anywhere from tens to hundreds of billions of parameters. The model parameters are numbers that define how the model will take an input and generate the output. So it's essentially the numbers that define the model itself. Okay, so that's a quantitative perspective of what distinguishes large language models from not large language models. But there's also this qualitative perspective and these so-called emergent properties that start to show up when language models become large. And so emergent properties is the language used in this paper cited below, a survey of large language models available in the archive. Really great beginner's guide, I recommend it. But essentially what this term means is there are properties in large language models that do not appear in smaller language models. And so one example of this is zero-shot learning. One definition of zero-shot learning is the capability of a machine learning model to complete a task it was not explicitly trained to do. So while this may not sound super impressive to us very smart and sophisticated humans, this is actually a major innovation in how these state-of-the-art machine learning models are developed. So to see this, we can compare the old state-of-the-art paradigm to this new state-of-the-art paradigm. The old way, and not too long ago, we can say like about 5, 10 years ago, the way the high-performing best machine learning models were developed was strictly through supervised learning. What this would typically look like is you would train a model on thousands, if not millions, of labeled examples. And so what this might have looked like is you have some input text like, hello, hola, how's it going, esta bien, so on and so forth. And you take all these examples and you manually assign a label to each example. Here we're labeling the language, so English, Spanish, so on. And so you can imagine that this would take a tremendous amount of human effort to get thousands, if not millions, of high-quality examples. So let's compare this to the more recent innovation with large language models who use a different paradigm. They use so-called self-supervised learning. What that looks like in the context of large language models is you train a very large model on a very large corpus of data. And so what this can look like is if you're trying to build a model that can do language classification, instead of painstakingly generating this labeled data set, you can just take a corpus of English text and a corpus of Spanish text and train a model in a self-supervised way. So in contrast to supervised learning, self-supervised learning does not require manual labeling of each example in your data set. The so-called labels or targets for the model are actually defined from the inherent structure of the data or this context of the text. So you might be thinking to yourself, how does this self-supervised learning actually work? And so one of the most popular ways that this is done is the next word prediction paradigm. So suppose we have this text, listen to your, and we want to predict what the next word would be. But clearly there's not just one word that can go after this string of words. There are actually many words you can put after this text and it would make sense. In this next word prediction paradigm, what the language model is trying to do is to predict the probability distribution of the next word given the previous words. What this might look like is listen to your heart might be the most probable next word, but another likely word could be gut or listen to your body or listen to your parents and listen to your grandma. And so this is essentially the core task that these large language models are trained to do. And the way the large language model will learn these probabilities is that it'll see so many examples in this massive corpus of text that it's trained on and it has a massive number of internal parameters so it can efficiently represent all the different statistical associations with different words. And an important point here is that context matters. If we simply added the word don't to the front of this string here and it changed it to don't listen to your, then this probability distribution could look entirely different because just by adding one word before this sentence, we completely change the meaning of the sentence. And so to put this a bit more mathematically, and I promise this is the most technical thing in this video, this is an example of a auto regression task. So auto meaning self, regression meaning you're trying to predict something. So what this notation means is what is the probability of the nth text or more technically the nth token given the preceding m token. So n minus one, n minus two, n minus three, so on and so forth. And so if you really want to boil everything down, this is the core task most large language models are doing. And somehow through this very simple task of predict the next word, we get this incredible performance from tools like chat GPT and other large language models. So now with that foundation set, hopefully you have a decent understanding of what large language models are and how they work and a broader context for them. Now let's talk about how we can use these in practice. Here I will talk about three levels in which we can use large language models. These three levels are ordered by the technical expertise and computational resources required. The most accessible way to use large language models is prompt engineering. Next we have model fine tuning. And then finally we have build your own large language model. So starting from level one, prompt engineering here, I have a pretty broad definition of prompt engineering. Here I define it as just using an LLM out of the box. So more specifically, not touching any of the model parameters. So of these tens of billions or hundreds of billions of parameters that define the model, we're not going to touch any of them. We're just going to leave them as is. Here I'll talk about two ways we can do this. One is the easy way and I'm sure is the way that most people in the world have interacted with large language models, which is using things like chat GPT. These are like intuitive user interfaces. They don't require any code and they're completely free. Someone can just go to the chat GPT website, type in a prompt and it'll spit out a response. So while this is definitely the easiest way to do it, it is a bit restrictive in that you have to go to their website. This doesn't really scale well if you're trying to build a product or service around it. But for a lot of use cases, this is actually super helpful. So for applications where the easy way doesn't cut it, there is the less easy way, which is using things like the open AI API or the hugging face transformers library. And these tools provide ways to interact with large language models programmatically. So essentially using Python. In the case of the open AI API, instead of typing your request in the chat GPT user interface, you can send it over to open AI using Python and their API and then you will get a response back. Of course, their API is not free, so you have to pay per API call. Another way we can do this is via open source solutions, one of which is the hugging face transformers library, which gives you easy access to open source large language models. So it's free and you can run these models locally. So no need to send your potentially proprietary or confidential information to a third party in open AI. So future videos of the series, we'll dive into all these different aspects. I'll talk about the open AI API, what it is, how it works, share example code. I'll dive into the hugging face transformers library, same situation. What the heck is it? How does it work? And then sharing some Python example code there. I'll also do a video talking about prompt engineering more generally. How can we create prompts to get good responses from large language models? And so while prompt engineering is the most accessible way to work with large language models, just working with a model out of the box may give you suboptimal performance on a specific task or use case. Or the model has really good performance, but it's massive. It has like a hundred billion parameters. So a question might be, is there a way we can use a smaller model, but kind of tweak it in a way to have good performance on our very narrow and specific use case. And so this brings us to level two, which is model fine tuning, which here I define as adjusting at least one internal model parameter for a particular task. And so here there are just generally two steps. One, you get a pre-trained large language model, maybe from open AI, or maybe an open source model from the hugging face transformers library. And then you update the model parameters given task specific examples. Kind of going back to the supervised learning versus self-supervised learning, the pre-trained model is going to be a self-supervised model. So it'll be trained on this simple word prediction task. But in step two, here's where we're going to do supervised learning or even reinforcement learning to tweak the model parameters for a specific use case. And so this turns out to work very well. Examples like ChatGPT, you're not working with the raw pre-trained model. The model that you are interacting with in ChatGPT is actually a fine-tuned model developed using reinforcement learning. And so a reason why this might work is that in doing this self-supervised task and doing the word prediction, the base model, this pre-trained large language model is learning useful representations for a wide variety of tasks. So in a future video, I will dive in more deeply into fine-tuning techniques. Another one is low rank adaptation or LORA for short. And then another popular one is reinforcement learning with human feedback or RLHF. And of course, there is a third step here. You'll deploy your fine-tuned large language model to do some kind of service or, you know, use it in your day-to-day life and you'll profit somehow. And so my sense is between prompt engineering and model fine-tuning, you can probably handle 99% of large language model use cases and applications. However, if you're a large organization, large enterprise, and security is a big concern. So you don't want to use open source models or you don't want to send data to a third party via an API. And maybe you want your large language model to be very good at a relatively specific set of tasks. You want to customize the training data in a very specific way and you want to own all the rights, have it for commercial use, all this kind of stuff. Then it can make sense to go one step further beyond model fine-tuning and build your own large language model. And so here I define it as just coming up with all the model parameters. So I'll just talk about how to do this at a very high level here, and I'll leave technical details for a future video in the series. First, we need to get our data. And so what this might look like is you'll get a book corpus, a Wikipedia corpus, and a Python corpus. And so this is billions of tokens of text. And then you will take that and pre-process it, refine it into your training data set. And then you can take the training data set and do the model training through self-supervised learning. And then out of that comes the pre-trained large language model. So you can take this as your starting point for level two and go from there. And so if you enjoyed this video and you want to read more, be sure to check out the blog in Towards Data Science. There I share some more details that I may have missed in this video. This series is both a video and blog series. So each video will have an associated blog, and there will also be tons of example code on the GitHub repository. The goal of the series is to really just make information about large language models much more accessible. I really do think this is the technological innovation of our time, and there are so many opportunities for potential use cases, applications, products, services that can come out of large language models. And that's something that I want to support. I think we'll be better off if more people understand this technology and are applying it to solving problems. So with that, be sure to hit the subscribe button to keep up with future videos in this series. If you have any questions or suggestions for other topics I should cover in this series, please drop those in the comment section below. And as always, thank you so much for your time and thanks for watching.\\n\\nHi everyone. So recently I gave a 30-minute talk on large language models, just kind of like an intro talk. Unfortunately that talk was not recorded, but a lot of people came to me after the talk and they told me that they really liked the talk, so I thought I would just re-record it and basically put it up on YouTube. So here we go, the busy person's intro to large language models, Director Scott. Okay, so let's begin. First of all, what is a large language model really? Well, a large language model is just two files, right? There will be two files in this hypothetical directory. So, for example, working with the specific example of the LLAMA2 70b model, this is a large language model released by Meta.ai, and this is basically the LLAMA series of language models, the second iteration of it, and this is the 70 billion parameter model of this series. So there's multiple models belonging to the LLAMA2 series, 7 billion, 13 billion, 34 billion, and 70 billion is the biggest one. Now many people like this model specifically because it is probably today the most powerful open weights model. So basically the weights and the architecture and a paper was all released by Meta, so anyone can work with this model very easily by themselves. This is unlike many other language models that you might be familiar with. For example, if you're using ChatsGPT or something like that, the model architecture was never released. It is owned by OpenAI, and you're allowed to use the language model through a web interface, but you don't have actually access to that model. So in this case, the LLAMA2 70b model is really just two files on your file system, the parameters file and the run some kind of a code that runs those parameters. So the parameters are basically the weights or the parameters of this neural network that is the language model. We'll go into that in a bit. Because this is a 70 billion parameter model, every one of those parameters is stored as two bytes, and so therefore the parameters file here is 140 gigabytes, and it's two bytes because this is a float 16 number as the data type. Now in addition to these parameters, that's just like a large list of parameters for that neural network. You also need something that runs that neural network, and this piece of code is implemented in our run file. Now this could be a C file, or a Python file, or any other programming language really. It can be written any arbitrary language, but C is sort of like a very simple language just to give you a sense, and it would only require about 500 lines of C with no other dependencies to implement the neural network architecture, and that uses basically the parameters to run the model. So it's only these two files. You can take these two files, and you can take your MacBook, and this is a fully self-contained package. This is everything that's necessary. You don't need any connectivity to the internet or anything else. You can take these two files, you compile your C code, you get a binary that you can point at the parameters, and you can talk to this language model. So for example, you can send it text, like for example, write a poem about the company Scale.ai, and this language model will start generating text, and in this case, it will follow the directions and give you a poem about Scale.ai. Now the reason that I'm picking on Scale.ai here, and you're going to see that throughout the talk, is because the event that I originally presented this talk with was run by Scale.ai, and so I'm picking on them throughout the slides a little bit, just in an effort to make it concrete. So this is how we can run the model. Just requires two files, just requires a MacBook. I'm slightly cheating here, because this was not actually, in terms of the speed of this video here, this was not running a 70 billion parameter model, it was only running a 7 billion parameter model. A 70B would be running about 10 times slower, but I wanted to give you an idea of sort of just the text generation and what that looks like. So not a lot is necessary to run the model. This is a very small package, but the computational complexity really comes in when we'd like to get those parameters. So how do we get the parameters, and where are they from? Because whatever is in the run.c file, the neural network architecture, and sort of the forward pass of that network, everything is algorithmically understood and open and so on. But the magic really is in the parameters, and how do we obtain them? So to obtain the parameters, basically the model training, as we call it, is a lot more involved than model inference, which is the part that I showed you earlier. So model inference is just running it on your MacBook. Model training is a competitionally very involved process. So basically what we're doing can best be sort of understood as kind of a compression of a good chunk of internet. So because Lama270B is an open source model, we know quite a bit about how it was trained, because Meta released that information in paper. So these are some of the numbers of what's involved. You basically take a chunk of the internet that is roughly, you should be thinking, 10 terabytes of text. This typically comes from like a crawl of the internet. So just imagine just collecting tons of text from all kinds of different websites and collecting it together. So you take a large chunk of internet, then you procure a GPU cluster, and these are very specialized computers intended for very heavy computational workloads like training of neural networks. You need about 6,000 GPUs, and you would run this for about 12 days to get a Lama270B. And this would cost you about $2 million. And what this is doing is basically it is compressing this large chunk of text into what you can think of as a kind of a zip file. So these parameters that I showed you in an earlier slide are best thought of as like a zip file of the internet. And in this case, what would come out are these parameters, 140 gigabytes. So you can see that the compression ratio here is roughly like 100x, roughly speaking. But this is not exactly a zip file because a zip file is lossless compression. What's happening here is a lossy compression. We're just kind of like getting a kind of a gestalt of the text that we trained on. We don't have an identical copy of it in these parameters. And so it's kind of like a lossy compression. You can think about it that way. The one more thing to point out here is these numbers here are actually by today's standards in terms of state-of-the-art, rookie numbers. So if you want to think about state-of-the-art neural networks, like say what you might use in chatGPT or Clod or Bard or something like that, these numbers are off by a factor of 10 or more. So you would just go in and you would just like start multiplying by quite a bit more. And that's why these training runs today are many tens or even potentially hundreds of millions of dollars, very large clusters, very large datasets. And this process here is very involved to get those parameters. Once you have those parameters, running the neural network is fairly computationally cheap. Okay. So what is this neural network really doing? I mentioned that there are these parameters. This neural network basically is just trying to predict the next word in a sequence. You can think about it that way. So you can feed in a sequence of words, for example, cat sat on A. This feeds into a neural net and these parameters are dispersed throughout this neural network. And there's neurons and they're connected to each other and they all fire in a certain way. You can think about it that way. And out comes a prediction for what word comes next. So for example, in this case, this neural network might predict that in this context of four words, the next word will probably be a mat with say 97% probability. So this is fundamentally the problem that the neural network is performing. And you can show mathematically that there's a very close relationship between prediction and compression, which is why I sort of allude to this neural network as kind of training it as kind of like a compression of the internet, because if you can predict sort of the next word very accurately, you can use that to compress the dataset. So it's just a next word prediction neural network. You give it some words, it gives you the next word. Now, the reason that what you get out of the training is actually quite a magical artifact is that basically the next word prediction task you might think is a very simple objective, but it's actually a pretty powerful objective because it forces you to learn a lot about the world inside the parameters of the neural network. So here I took a random webpage at the time when I was making this talk. I just grabbed it from the main page of Wikipedia and it was about Ruth Handler. And so think about being the neural network and you're given some amount of words and trying to predict the next word in a sequence. Well, in this case, I'm highlighting here in red some of the words that would contain a lot of information. And so, for example, if your objective is to predict the next word, presumably your parameters have to learn a lot of this knowledge. You have to know about Ruth and Handler and when she was born and when she died, who she was, what she's done, and so on. And so in the task of next word prediction, you're learning a ton about the world and all this knowledge is being compressed into the weights, the parameters. Now, how do we actually use these neural networks? Well, once we've trained them, I showed you that the model inference is a very simple process. We basically generate what comes next. We sample from the model. So we pick a word and then we continue feeding it back in and get the next word and continue feeding that back in. So we can iterate this process and this network then dreams internet documents. So, for example, if we just run the neural network, or as we say, perform inference, we would get sort of like web page dreams. You can almost think about it that way, right? Because this network was trained on web pages and then you can sort of like let it loose. So on the left, we have some kind of a Java code dream, it looks like. In the middle, we have some kind of a, what looks like almost like an Amazon product dream. And on the right, we have something that almost looks like Wikipedia article. Focusing for a bit on the middle one, as an example, the title, the author, the ISBN number, everything else, this is all just totally made up by the network. The network is dreaming text from the distribution that it was trained on. It's mimicking these documents. But this is all kind of like hallucinated. So, for example, the ISBN number, this number probably, I would guess almost certainly does not exist. The model network just knows that what comes after ISBN colon is some kind of a number of roughly this length, and it's got all these digits. And it just like puts it in. It just kind of like puts in whatever looks reasonable. So it's parroting the training dataset distribution. On the right, the black nose dace, I looked it up, and it is actually a kind of fish. And what's happening here is this text verbatim is not found in the training set documents, but this information, if you actually look it up, is actually roughly correct with respect to this fish. And so the network has knowledge about this fish. It knows a lot about this fish. It's not going to exactly parrot documents that it saw in the training set. But again, it's some kind of a lossy compression of the internet. It kind of remembers the gestalt. It kind of knows the knowledge. And it just kind of like goes and it creates the form. It creates kind of like the correct form and fills it with some of its knowledge. And you're never 100% sure if what it comes up with is as we call hallucination or like an incorrect answer or like a correct answer necessarily. So some of this stuff could be memorized and some of it is not memorized and you don't exactly know which is which. But for the most part, this is just kind of like hallucinating or like dreaming internet text from its data distribution. Okay, let's now switch gears to how does this network work? How does it actually perform this next word prediction task? What goes on inside it? Well, this is where things complicate a little bit. This is kind of like the schematic diagram of the neural network. If we kind of like zoom in into the toy diagram of this neural net, this is what we call the transformer neural network architecture. And this is kind of like a diagram of it. Now, what's remarkable about this neural net is we actually understand in full detail the architecture. We know exactly what mathematical operations happen at all the different stages of it. The problem is that these 100 billion parameters are dispersed throughout the entire neural network. And so basically, these billions of parameters are throughout the neural net. And all we know is how to adjust these parameters iteratively to make the network as a whole better at the next word prediction task. So we know how to optimize these parameters. We know how to adjust them over time to get a better next word prediction. But we don't actually really know what these 100 billion parameters are doing. We can measure that it's getting better at the next word prediction. But we don't know how these parameters collaborate to actually perform that. We have some kind of models that you can try to think through on a high level for what the network might be doing. So we kind of understand that they build and maintain some kind of a knowledge database. But even this knowledge database is very strange and imperfect and weird. So a recent viral example is what we call the reversal course. So as an example, if you go to chat GPT and you talk to GPT-4, the best language model currently available, you say, who is Tom Cruise's mother? It will tell you it's Mary Lee Pfeiffer, which is correct. But if you say, who is Mary Lee Pfeiffer's son? It will tell you it doesn't know. So this knowledge is weird, and it's kind of one dimensional. And you have to sort of like, this knowledge isn't just like stored and can be accessed in all the different ways. You sort of like ask it from a certain direction almost. And so that's really weird and strange. And fundamentally, we don't really know because all you can kind of measure is whether it works or not, and with what probability. So long story short, think of LLMs as kind of like mostly inscrutable artifacts. They're not similar to anything else you might build in an engineering discipline. They're not like a car where we sort of understand all the parts. They're these neural nets that come from a long process of optimization. And so we don't currently understand exactly how they work, although there's a field called interpretability or mechanistic interpretability, trying to kind of go in and try to figure out what all the parts of this neural net are doing. And you can do that to some extent, but not fully right now. But right now, we kind of treat them mostly as empirical artifacts. We can give them some inputs and we can measure the outputs. We can basically measure their behavior. We can look at the text that they generate in many different situations. And so I think this requires basically correspondingly sophisticated evaluations to work with these models because they're mostly empirical. So now let's go to how we actually obtain an assistant. So far, we've only talked about these internet document generators, right? And so that's the first stage of training. We call that stage pre-training. We're now moving to the second stage of training, which we call fine-tuning. And this is where we obtain what we call an assistant model because we don't actually really just want document generators. That's not very helpful for many tasks. We want to give questions to something and we want it to generate answers based on those questions. So we really want an assistant model instead. And the way you obtain these assistant models is fundamentally through the following process. We basically keep the optimization identical. So the training will be the same. It's just a next word prediction task, but we're going to swap out the dataset on which we are training. So it used to be that we are trying to train on internet documents. We're going to now swap it out for datasets that we collect manually. And the way we collect them is by using lots of people. So typically a company will hire people and they will give them labeling instructions and they will ask people to come up with questions and then write answers for them. So here's an example of a single example that might basically make it into your training set. So there's a user and it says something like, can you write a short introduction about the relevance of the term monopsony in economics and so on. And then there's assistant. And again, the person fills in what the ideal response should be and the ideal response and how that is specified and what it should look like all just comes from labeling documentations that we provide these people and the engineers at a company like OpenAI or Anthropic or whatever else will come up with these labeling documentations. Now the pre-training stage is about a large quantity of text, but potentially low quality because it just comes from the internet and there's tens of hundreds of terabytes of it and it's not all very high quality. But in this second stage, we prefer quality over quantity. So we may have many fewer documents, for example, a hundred thousand, but all of these documents now are conversations and they should be very high quality conversations. And fundamentally people create them based on labeling instructions. So we swap out the dataset now and we train on these Q&A documents. And this process is called fine tuning. Once you do this, you obtain what we call an assistant model. So this assistant\\n\\nLarge language models. They are everywhere. They get some things amazingly right and other things very interestingly wrong. My name is Marina Danilevsky. I am a senior research scientist here at IBM Research, and I want to tell you about a framework to help large language models be more accurate and more up-to-date. Retrieval Augmented Generation, or RAG. Let's just talk about the generation part for a minute. So forget the retrieval augmented. So the generation, this refers to large language models or LLMs that generate text in response to a user query referred to as a prompt. These models can have some undesirable behavior. I want to tell you an anecdote to illustrate this. So my kids, they recently asked me this question, in our solar system, what planet has the most moons? And my response was, oh, that's really great that you're asking me this question. I loved space when I was your age. Of course, that was like 30 years ago. But I know this. I read an article and the article said that it was Jupiter and 88 moons. So that's the answer. Now, actually, there's a couple of things wrong with my answer. First of all, I have no source to support what I'm saying. So even though I confidently said I read an article, I know the answer, I'm not sourcing it. I'm giving the answer off the top of my head. And also, I actually haven't kept up with this for a while. And my answer is out of date. So we have two problems here. One is no source. And the second problem is that I am out of date. And these, in fact, are two behaviors that are often observed as problematic when interacting with large language models. They are LLM challenges. Now, what would have happened if I'd taken a beat and first gone and looked up the answer on a reputable source like NASA? Well, then I would have been able to say, ah, OK. So the answer is Saturn with 146 moons. And in fact, this keeps changing because scientists keep on discovering more and more moons. So I have now grounded my answer in something more believable. I have not hallucinated or made up an answer. Oh, and by the way, I didn't leak personal information about how long ago it's been since I was obsessed with space. All right. So what does this have to do with large language models? Well, how would a large language model have answered this question? So let's say that I have a user asking this question about moons. A large language model would confidently say, OK, I have been trained. And from what I know in my parameters during my training, the answer is Jupiter. The answer is wrong. But, you know, we don't know. The large language model is very confident in what it answered. Now, what happens when you add this retrieval augmented part here? What does that mean? That means that now, instead of just relying on what the LLM knows, we are adding a content store. This could be open, like the internet. This could be closed, like some collection of documents, collection of policies, whatever. The point, though, now is that the LLM first goes and talks to the content store and says, hey, can you retrieve from me information that is relevant to what the user's query was? And now, with this retriever augmented answer, it's not Jupiter anymore. We know that it is Saturn. What does this look like? Well, first, user prompts the LLM with their question. They say this is what my question was. And originally, if we're just talking to a generative model, the generative model says, oh, OK, I know the response. Here it is. Here's my response. But now, in the RAM framework, the generative model actually has an instruction that says, no, no, no. First, go and retrieve relevant content. Combine that with the user's question, and only then generate the answer. So the prompt now has three parts, the instruction to pay attention to the retrieved content, together with the user's question. Now give a response. And in fact, now you can get evidence for why your response was what it was. So now, hopefully, you can see, how does RAD help the two LLM challenges that I had mentioned before? So first of all, I'll start with the out of date part. Now, instead of having to retrain your model if new information comes up, like, hey, we found some more moons. Now it's a Jupiter again. Maybe it'll be Saturn again in the future. All you have to do is you augment your data store with new information, updated information. So now, the next time that a user comes and asks the question, we're ready. We just go ahead and retrieve the most up-to-date information. The second problem, source. Well, the LLM model is now being instructed to pay attention to primary source data before giving its response. And in fact, now being able to give evidence. This makes it less likely to hallucinate or to leak data, because it is less likely to rely only on information that it learned during training. It also allows us to get the model to have a behavior that can be very positive, which is knowing when to say, I don't know. If the user's question cannot be reliably answered based on your data store, the model should say, I don't know, instead of making up something that is believable and may mislead the user. This can have a negative effect as well, though, because if the retriever is not sufficiently good to give the large language model the best, most highest quality grounding information, then maybe the user's query that is answerable doesn't get an answer. So this is actually why lots of folks, including many of us here at IBM, are working the problem on both sides. We are both working to improve the retriever, to give the large language model the best quality data on which to ground its response, and also the generative part, so that the LLM can give the richest, best response, finally, to the user when it generates the answer. Thank you for learning more about RAG, and like and subscribe to the channel. Thank you.\\n\\nSo, let's get started, so I'll be talking about building LLMs today, so I think a lot of you have heard of LLMs before, but just as a quick recap, LLMs, standing for Large Language Models, are basically all the chatbots that you've been hearing about recently, so ChatGPT from OpenAI, Claude from Entropiq, Gemini, and Lama, and other type of models like this, and today we'll be talking about how do they actually work, so it's going to be an overview because it's only one lecture, and it's hard to compress everything, but hopefully I'll touch a little bit about all the components that are needed to train some of these LLMs. Also, if you have questions, please interrupt me and ask. If you have a question, most likely other people in the room or on Zoom have the same question, so please ask. Great, so what matters when training LLMs? So, there are a few key components that matter. One is the architecture, so as you probably all know, LLMs are neural networks, and when you think about neural networks, you have to think about what architecture you're using. Another component which is really important is the training loss and the training algorithm, so how you actually train these models. Then it's data, so what do you train these models on? The evaluation, which is how do you know whether you're actually making progress towards the goal of LLMs, and then the system component, so that is like how do you actually make these models run on modern hardware, which is really important because these models are really large, so now more than ever, systems are actually really an important topic for LLMs. So, those five components, you probably all know that LLMs, and if you don't know, LLMs are all based on transformers, or at least some version of transformers. I'm actually not going to talk about the architecture today, one, because I gave a lecture on transformers a few weeks ago, and two, because you can find so much information online on transformers, but I think you can- there's much less information about the other four topics, so I really want to talk about those. Another thing to say is that most of academia actually focuses on architecture and training algorithm and losses. As academics, and I've done that for a lot- a big part of my career, is simply we like thinking that this is like we make new architectures, new models, and it seems like it's very important, but in reality, honestly, what matters in practice is mostly the three other topics, so data, evaluation, and systems, which is what most of industry actually focuses on. So that's also one of the reasons why I don't want to talk too much about the architecture, because really the rest is super important. Great, so overview of the lecture, I'll be talking about pre-training. So pre-training, you probably heard that word, this is the general word, this is kind of the classical language modeling paradigm, where you basically train your language model to essentially model all of internet. And then there's a post-training, which is a more recent paradigm, which is taking these large language models and making them essentially AI assistants. So this is more of a recent trend since ChatGPT. So if you ever heard of GPT-3 or GPT-2, that's really pre-training land. If you heard of ChatGPT, which you probably have, this is really post-training land. So I'll be talking about both, but I'll start with pre-training. And specifically, I'll talk about what is the task of pre-training LLMs and what is the laws that people actually use. So language modeling, this is a quick recap. Language models at a high level are simply models of probability distribution over sequences of tokens or of words. So it's basically some model of p of x1 to xl, where x1 is basically word 1 and xl is the last word in the sequence or in the sentence. So very concretely, if you have a sentence like the mouse ate the cheese, what the language model gives you is simply a probability of this sentence being uttered by a human or being found online. So if you have another sentence like the mouse ate cheese, here there's grammatical mistakes. So the model should know that this should have some syntactic knowledge. So it should know that this has less likelihood of appearing online. If you have another sentence like the cheese ate the mouse, then the model should hopefully know about the fact that usually cheese don't eat mouse. So there's some semantic knowledge and this is less likely than the first sentence. So this is basically at a high level what language models are. One word that you probably have been hearing a lot in the news are generative models. So this is just something that can generate models that can generate sentences or can generate some data. The reason why we say language models are generative models is that once you have a model of a distribution, you can simply sample from this model and then we can generate data. So you can generate sentences using a language model. So the type of models that people are all currently using are what we call autoregressive language models. And the key idea of autoregressive language models is that you take this distribution over words and you basically decompose it into the distribution of the first word, multiply it by the distribution of the likelihood of the distribution of the second word given the first word, and multiply it by p of the third word given the first two words. So there's no approximation here. This is just the chain rule of probability, which hopefully you all know about, really no approximation. This is just one way of modeling a distribution. So slightly more concisely, you can write it as a product of p's of the next word given everything which happened in the past, so of the context. So this is what we call autoregressive language models. Again, this is really not the only way of modeling distribution, this is just one way. It has some benefits and some downsides. One downside of autoregressive language models is that when you actually sample from this autoregressive language model, you basically have a for loop which generates the next word, then conditions on that next word, and then regenerate the other word. So basically, if you have a longer sentence that you want to generate, it takes more time to generate it. So there are some downsides of this current paradigm, but that's what we currently have, so I'm going to talk about this one. Great. So autoregressive language models. At a high level, what the task of autoregressive language model is, is simply predicting the next word, as I just said. So if you have a sentence like she likely prefers, one potential next word might be dogs. And the way we do it is that we first tokenize. So you take these words or sub words, you tokenize them, and then you give an ID for each token. So here I have one, two, three. Then you pass it through this black box. As I already said, we're not going to talk about the architecture. You just pass it through a model, and you then get a distribution, a probability distribution over the next word, over the next token. And then you sample from this distribution, you get a new token, and then you de-tokenize. So you get a new ID, you de-tokenize, and that's how you basically sample from a language model. One thing which is important to note is that the last two steps are actually only needed during inference. When you do training, you just need to predict the most likely token, and you can just compare to the real token, which happened in practice, and then you basically change the weights of your model to increase the probability of generating that token. Great. So autoregressive neural language models. So to be slightly more specific, still without talking about the architecture, the first thing we do is that we have all of these- oh, sorry, yes? On the previous slide, when you're predicting the probability of the next token, does this mean that your final output vector has to be the same dimensionality as the number of tokens that you have? Yes. How do you deal with if you're adding more tokens to your co-presenters? Yeah. So we're going to talk about tokenization actually later, so you will get some sense of this. You basically can't deal with adding new tokens. I'm kind of exaggerating. There are methods for doing it, but essentially people don't do it. So it's really important to think about how you tokenize your text, and that's why we'll talk about that later. But it's a very good point to note is that basically the vocabulary size, so the number of tokens that you have, is essentially the output of your language model. So it's actually pretty large. Okay, so autoregressive neural language models. First thing you do is that you take every word or every token. You embed them, so you get some vector representation for each of these tokens. You pass them through some neural network, as we said, it's a transformer. Then you get a representation for all the words in the context. So it's basically a representation of the entire sentence. You pass it through a linear layer, as you just said, to basically map it to the number so that the output, the number of outputs is the number of tokens. You then pass it through some softmax, and you basically get a probability distribution over the next words given every word in the context. And the laws that you use is basically, it's essentially a task of classifying the next token. So it's a very simple kind of machine learning task. So you use the cross-entropy laws, where you basically look at the actual target that happened, which is a target distribution, which is a one-hot encoding, which here in this case says, I saw the real word that happened is cat. So that's a one-hot distribution over cat. And here, this is the distribution that you generated. And basically, you do cross-entropy, which really just increases the probability of generating cat and decreases the probability of generating all the other tokens. One thing to notice is that, as you all know, again, this is just equivalent to maximizing the text log likelihood, because you can just rewrite the max over the probability of this autoregressive language modeling task as just being this minimum of, I just added the log here and minus, which is just the minimum of the loss, which is the cross-entropy loss. So basically, minimizing the loss is the same thing as maximizing the likelihood of your text. Any questions? OK, tokenizer. So this is one thing that people usually don't talk that much about. Tokenizers are extremely important. So it's really important that you understand, at least, what they do at a high level. So why do we need tokenizers in the first place? First, it's more general than words. So one simple thing that you might think is, oh, we're just going to take every word that we will have. And you just say, every word is a token in its own. But then what happens is, if there's a typo in your word, then you might not have any token associated with this word with a typo. And then you don't know how to actually pass this word with a typo into a large language model. So what do you do next? And also, even if you think about words, words are fine with Latin-based languages. But if you think about a language like Thai, you won't have a simple way of tokenizing by spaces, because there are no spaces between words. So really, tokens are much more general than words. First thing. Second thing that you might think is that you might tokenize every sentence, character by character. You might say, A is one token, B is another token. That would actually work, and probably very well. The issue is that then your sequence becomes super long. And as you probably remember from the lecture on transformers, the complexity grows quadratically with the length of sequences. So you really don't want to have a super long sequence. So tokenizers basically try to deal with those two problems and give common sub-sequences a certain token. And usually, how you should be thinking about it is around an average of every token is around three, four letters. And there are many algorithms for tokenization. I'll just talk about one of them to give you a high level, which is what we call byte-pair encoding, which is actually pretty common, one of the two most common tokenizers. And the way that you train a tokenizer is that first, you start with a very large corpus of text. And here, I'm really not talking about training a large language model yet. This is purely for the tokenization step. So this is my large corpus of text with these five words. Then you associate every character in this corpus of text a different token. So here, I just split up every character with a different token, and I just color coded all of those tokens. And then what you do is that you go through your text, and every time you see pairs of tokens that are very common, the most common pair of token, you just merge them. So here, you see three times the tokens T and O next to each other, so you're just going to say this is a new token. And then you continue. You repeat that. So now you have T-O-K, TOK, which happens three times, TOK with an E, that happens two times, and TOKEN, which happens twice, and then EX, which also happens twice. So this is that if you were to train a tokenizer on this corpus of text, which is very small, that's how you would finish with a trained tokenizer. In reality, you do it on much larger corpuses of text. And this is the real tokenizer of, actually, I think this is GPT-3 or ChatGPT. And here, you see how it would actually separate these words. So basically, you see the same thing as what we gave in the previous example, TOKEN becomes its own token. So tokenizer is actually split up into two tokens, TOKEN and EIZER. So yeah, that's all about tokenizers. Any question on that? Yeah? How do you deal with spaces, and how do you deal with introduction? Yeah. So actually, there's a step before tokenizers, which is what we call pre-tokenizers, which is exactly what you just said. So this is mostly, in theory, there's no reason to deal with spaces and punctuation separately. You could just say every space gets its own token, every punctuation gets its own token, and you could just do all the merging. The problem is that, so there's an efficiency question. Actually, training these tokenizers takes a long time. So you're better off, because you have to consider every pair of token. So what you end up doing is saying, if there's a space, this is very, like pre-tokenizers are very English-specific. So you say, if there's a space, we're not going to start looking at the token that came before and the token that came afterwards. So you're not merging in between spaces. But this is just like a computation optimization. You could theoretically just deal with it the same way as you deal with any other character. Yeah? When you merge tokens, do you delete the tokens that you merged away, or do you keep the smaller tokens that you merged? You actually keep the smaller tokens. I mean, in reality, it doesn't matter much, because usually on large corpus of text, you will have actually everything. But you usually keep the small ones. And the reason why you want to do that is because if, in case there's a, as we said before, you have some grammatical mistakes or some typos, you still want to be able to represent these words by character. So yeah. Yes? Are the tokens unique? So, I mean, say, in this case, T-O-K-E-N, is there only one occurrence, or do you need to leave multiple occurrence so they can take on different meanings or something? Oh, oh, I see what you say. No, no, no. It's every token has its own unique ID. So a usual, this is a great question. For example, if you think about a bank, which could be bank for money or bank like water, they will have the same token, but the model will learn, the transformer will learn that based on the words that are around it, it should associate that, I'm saying, I'm being very hand-wavy here, but associate that with a representation that is either more like the bank money side or the bank water side. But that's the transformer that does that. It's not a tokenizer. Yes? Yeah, so you mentioned during tokenization, you keep the smaller tokens to start with, right? So if you start with a T, you keep the T, and then you tell your tokenizer to expand that amount of token. So let's say maybe you didn't train on token, but in your data, you are trying to encode token. So how does the tokenizer know to encode it with token or to do it with T? Yeah, it's a great question. You basically, when you, so when you tokenize, so that's after training of the tokenizer, when you actually apply the tokenizer, you basically always choose the largest token that you can apply. So if you can do token, you will never do T. You will always do token. But it's actually, so people don't usually talk that much about tokenizers, but there's a lot of computational benefits or computational tricks that you can do for making these things faster. So I really don't think we, and honestly, I think a lot of people think that we should just get away from tokenizers and just kind of tokenize\", 'answer': 'The purpose of a Large Language Model (LLM) is to perform a variety of natural language processing tasks such as text classification, question answering, document summarization, and text generation. LLMs are pre-trained on large datasets and can be fine-tuned for specific purposes, making them versatile tools for various applications. They are designed to understand and generate human language effectively.'}\n",
      "{'input': 'What is my name?', 'chat_history': [HumanMessage(content='Hi my name is Obi-Wan, what is the purpose of an LLM?', additional_kwargs={}, response_metadata={}, id='df4b4ce7-3a69-4865-b825-902c8c2a9c06'), AIMessage(content='The purpose of a Large Language Model (LLM) is to understand and generate human language based on patterns it has learned from vast amounts of text data. LLMs are a type of neural network designed to handle a wide range of tasks, including summarization, text generation, creative writing, question-answering, programming, and more. They are trained on massive datasets from various sources like web pages, books, and transcripts, allowing them to learn and adapt to different contexts and tasks.', additional_kwargs={}, response_metadata={}, id='9f050a0f-10d9-4255-8424-207ec27a7305'), HumanMessage(content='Hi my name is Obi-Wan, what is the purpose of an LLM?', additional_kwargs={}, response_metadata={}, id='cee5cdce-60ca-4b17-a9a7-f161deb66fa0'), AIMessage(content='The purpose of a Large Language Model (LLM) is to perform a variety of natural language processing tasks such as text classification, question answering, document summarization, and text generation. LLMs are pre-trained on large datasets and can be fine-tuned for specific purposes, making them versatile tools for various applications. They are designed to understand and generate human language effectively.', additional_kwargs={}, response_metadata={}, id='9ef736e6-f305-41ba-98d4-f5d5b9b4e303'), HumanMessage(content='How can I train them?', additional_kwargs={}, response_metadata={}, id='4ec1671d-3a48-4072-9409-d23ddfa15574'), AIMessage(content='To train large language models, you can use techniques such as prompt engineering, model fine-tuning, or building your own model from scratch. Prompt engineering involves using the model out of the box without changing its parameters, while model fine-tuning adjusts the model parameters for a specific task. Building your own model requires creating a new model from the ground up, which is more resource-intensive but allows for full customization.', additional_kwargs={}, response_metadata={}, id='2f7c7293-0895-4018-a035-4b9450f0d56c')], 'context': \"Hey everyone, I'm Shaw and I'm back with a new data science series. In this new series, I'm going to be talking about large language models and how to use them in practice. In this video, I will give a beginner-friendly introduction to large language models and describe three levels of working with them in practice. Future videos in this series will discuss various practical aspects of large language models, things like using OpenAI's Python API, using open-source solutions like the Hugging Phase Transformers library, how to fine-tune large language models, and of course, how to build a large language model from scratch. If you enjoyed this content, please be sure to like, subscribe, and share with others. And if you have any suggestions for me to include in this series, please share those in the comments section below. And so with that, let's get into the video. So to kick off the video series, in this video, I'm going to be giving a practical introduction to large language models. And this is meant to be very beginner-friendly and high level, and I'll leave more technical details and example code for future videos and blogs in this series. So a natural place to start is, what is a large language model, or LLM for short? So I'm sure most people are familiar with ChatGPT, however, if you are enlightened enough to not keep up with new cycles and tech hype and all this kind of stuff, ChatGPT is essentially a very impressive and advanced chatbot. So if you go to the ChatGPT website, you can ask it questions like, what's a large language model? And it will generate a response very quickly, like the one that we are seeing here. And that is really impressive. Like if you were ever on AOL, Instant Messenger, also called AIM, you know, back in early 2000s or in the early days of the internet, there were chatbots then, there have been chatbots for a long time, but this one feels different. Like the text is very impressive and it almost feels human-like. A question you might have when you hear the term large language model is, what makes it large? What's the difference between a large language model and a not large language model? And this was exactly the question I had when I first heard the term. And so one way we can put it is that large language models are a special type of language model. But what makes them so special? And I'm sure there's a lot that can be said about large language models. But to keep things simple, I'm going to talk about two distinguishing properties. The first quantitative and the second qualitative. So first, quantitatively, large language models are large. They have many, many more model parameters than past language models. And so these days, this is anywhere from tens to hundreds of billions of parameters. The model parameters are numbers that define how the model will take an input and generate the output. So it's essentially the numbers that define the model itself. Okay, so that's a quantitative perspective of what distinguishes large language models from not large language models. But there's also this qualitative perspective and these so-called emergent properties that start to show up when language models become large. And so emergent properties is the language used in this paper cited below, a survey of large language models available in the archive. Really great beginner's guide, I recommend it. But essentially what this term means is there are properties in large language models that do not appear in smaller language models. And so one example of this is zero-shot learning. One definition of zero-shot learning is the capability of a machine learning model to complete a task it was not explicitly trained to do. So while this may not sound super impressive to us very smart and sophisticated humans, this is actually a major innovation in how these state-of-the-art machine learning models are developed. So to see this, we can compare the old state-of-the-art paradigm to this new state-of-the-art paradigm. The old way, and not too long ago, we can say like about 5, 10 years ago, the way the high-performing best machine learning models were developed was strictly through supervised learning. What this would typically look like is you would train a model on thousands, if not millions, of labeled examples. And so what this might have looked like is you have some input text like, hello, hola, how's it going, esta bien, so on and so forth. And you take all these examples and you manually assign a label to each example. Here we're labeling the language, so English, Spanish, so on. And so you can imagine that this would take a tremendous amount of human effort to get thousands, if not millions, of high-quality examples. So let's compare this to the more recent innovation with large language models who use a different paradigm. They use so-called self-supervised learning. What that looks like in the context of large language models is you train a very large model on a very large corpus of data. And so what this can look like is if you're trying to build a model that can do language classification, instead of painstakingly generating this labeled data set, you can just take a corpus of English text and a corpus of Spanish text and train a model in a self-supervised way. So in contrast to supervised learning, self-supervised learning does not require manual labeling of each example in your data set. The so-called labels or targets for the model are actually defined from the inherent structure of the data or this context of the text. So you might be thinking to yourself, how does this self-supervised learning actually work? And so one of the most popular ways that this is done is the next word prediction paradigm. So suppose we have this text, listen to your, and we want to predict what the next word would be. But clearly there's not just one word that can go after this string of words. There are actually many words you can put after this text and it would make sense. In this next word prediction paradigm, what the language model is trying to do is to predict the probability distribution of the next word given the previous words. What this might look like is listen to your heart might be the most probable next word, but another likely word could be gut or listen to your body or listen to your parents and listen to your grandma. And so this is essentially the core task that these large language models are trained to do. And the way the large language model will learn these probabilities is that it'll see so many examples in this massive corpus of text that it's trained on and it has a massive number of internal parameters so it can efficiently represent all the different statistical associations with different words. And an important point here is that context matters. If we simply added the word don't to the front of this string here and it changed it to don't listen to your, then this probability distribution could look entirely different because just by adding one word before this sentence, we completely change the meaning of the sentence. And so to put this a bit more mathematically, and I promise this is the most technical thing in this video, this is an example of a auto regression task. So auto meaning self, regression meaning you're trying to predict something. So what this notation means is what is the probability of the nth text or more technically the nth token given the preceding m token. So n minus one, n minus two, n minus three, so on and so forth. And so if you really want to boil everything down, this is the core task most large language models are doing. And somehow through this very simple task of predict the next word, we get this incredible performance from tools like chat GPT and other large language models. So now with that foundation set, hopefully you have a decent understanding of what large language models are and how they work and a broader context for them. Now let's talk about how we can use these in practice. Here I will talk about three levels in which we can use large language models. These three levels are ordered by the technical expertise and computational resources required. The most accessible way to use large language models is prompt engineering. Next we have model fine tuning. And then finally we have build your own large language model. So starting from level one, prompt engineering here, I have a pretty broad definition of prompt engineering. Here I define it as just using an LLM out of the box. So more specifically, not touching any of the model parameters. So of these tens of billions or hundreds of billions of parameters that define the model, we're not going to touch any of them. We're just going to leave them as is. Here I'll talk about two ways we can do this. One is the easy way and I'm sure is the way that most people in the world have interacted with large language models, which is using things like chat GPT. These are like intuitive user interfaces. They don't require any code and they're completely free. Someone can just go to the chat GPT website, type in a prompt and it'll spit out a response. So while this is definitely the easiest way to do it, it is a bit restrictive in that you have to go to their website. This doesn't really scale well if you're trying to build a product or service around it. But for a lot of use cases, this is actually super helpful. So for applications where the easy way doesn't cut it, there is the less easy way, which is using things like the open AI API or the hugging face transformers library. And these tools provide ways to interact with large language models programmatically. So essentially using Python. In the case of the open AI API, instead of typing your request in the chat GPT user interface, you can send it over to open AI using Python and their API and then you will get a response back. Of course, their API is not free, so you have to pay per API call. Another way we can do this is via open source solutions, one of which is the hugging face transformers library, which gives you easy access to open source large language models. So it's free and you can run these models locally. So no need to send your potentially proprietary or confidential information to a third party in open AI. So future videos of the series, we'll dive into all these different aspects. I'll talk about the open AI API, what it is, how it works, share example code. I'll dive into the hugging face transformers library, same situation. What the heck is it? How does it work? And then sharing some Python example code there. I'll also do a video talking about prompt engineering more generally. How can we create prompts to get good responses from large language models? And so while prompt engineering is the most accessible way to work with large language models, just working with a model out of the box may give you suboptimal performance on a specific task or use case. Or the model has really good performance, but it's massive. It has like a hundred billion parameters. So a question might be, is there a way we can use a smaller model, but kind of tweak it in a way to have good performance on our very narrow and specific use case. And so this brings us to level two, which is model fine tuning, which here I define as adjusting at least one internal model parameter for a particular task. And so here there are just generally two steps. One, you get a pre-trained large language model, maybe from open AI, or maybe an open source model from the hugging face transformers library. And then you update the model parameters given task specific examples. Kind of going back to the supervised learning versus self-supervised learning, the pre-trained model is going to be a self-supervised model. So it'll be trained on this simple word prediction task. But in step two, here's where we're going to do supervised learning or even reinforcement learning to tweak the model parameters for a specific use case. And so this turns out to work very well. Examples like ChatGPT, you're not working with the raw pre-trained model. The model that you are interacting with in ChatGPT is actually a fine-tuned model developed using reinforcement learning. And so a reason why this might work is that in doing this self-supervised task and doing the word prediction, the base model, this pre-trained large language model is learning useful representations for a wide variety of tasks. So in a future video, I will dive in more deeply into fine-tuning techniques. Another one is low rank adaptation or LORA for short. And then another popular one is reinforcement learning with human feedback or RLHF. And of course, there is a third step here. You'll deploy your fine-tuned large language model to do some kind of service or, you know, use it in your day-to-day life and you'll profit somehow. And so my sense is between prompt engineering and model fine-tuning, you can probably handle 99% of large language model use cases and applications. However, if you're a large organization, large enterprise, and security is a big concern. So you don't want to use open source models or you don't want to send data to a third party via an API. And maybe you want your large language model to be very good at a relatively specific set of tasks. You want to customize the training data in a very specific way and you want to own all the rights, have it for commercial use, all this kind of stuff. Then it can make sense to go one step further beyond model fine-tuning and build your own large language model. And so here I define it as just coming up with all the model parameters. So I'll just talk about how to do this at a very high level here, and I'll leave technical details for a future video in the series. First, we need to get our data. And so what this might look like is you'll get a book corpus, a Wikipedia corpus, and a Python corpus. And so this is billions of tokens of text. And then you will take that and pre-process it, refine it into your training data set. And then you can take the training data set and do the model training through self-supervised learning. And then out of that comes the pre-trained large language model. So you can take this as your starting point for level two and go from there. And so if you enjoyed this video and you want to read more, be sure to check out the blog in Towards Data Science. There I share some more details that I may have missed in this video. This series is both a video and blog series. So each video will have an associated blog, and there will also be tons of example code on the GitHub repository. The goal of the series is to really just make information about large language models much more accessible. I really do think this is the technological innovation of our time, and there are so many opportunities for potential use cases, applications, products, services that can come out of large language models. And that's something that I want to support. I think we'll be better off if more people understand this technology and are applying it to solving problems. So with that, be sure to hit the subscribe button to keep up with future videos in this series. If you have any questions or suggestions for other topics I should cover in this series, please drop those in the comment section below. And as always, thank you so much for your time and thanks for watching.\\n\\nHello, and welcome to Introduction to Large Language Models. My name is John Ewald, and I'm a training developer here at Google Cloud. In this course, you'll learn to define large language models, or LLMs, describe LLM use cases, explain prompt tuning, and describe Google's GenAI development tools. Large language models, or LLMs, are a subset of deep learning. To find out more about deep learning, see our Introduction to Generative AI course video. LLMs and generative AI intersect, and they are both a part of deep learning. Another area of AI you may be hearing a lot about is generative AI. This is a type of artificial intelligence that can produce new content, including text, images, audio, and synthetic data. So what are large language models? Large language models refer to large, general-purpose language models that can be pre-trained and then fine-tuned for specific purposes. What do pre-trained and fine-tuned mean? Imagine training a dog. Often you train your dog basic commands, such as sit, come, down, and stay. These commands are normally sufficient for everyday life and help your dog become a good canine citizen. However, if you need a special service dog, such as a police dog, a guide dog, or a hunting dog, you add special trainings. This similar idea applies to large language models. These models are trained for general purposes to solve common language problems such as text classification, question answering, document summarization, and text generation across industries. The models can then be tailored to solve specific problems in different fields such as retail, finance, and entertainment using a relatively small size of field datasets. Let's further break down the concept into three major features of large language models. Large indicates two meanings. First is the enormous size of the training dataset, sometimes at the petabyte scale. Second, it refers to the parameter count. In ML, parameters are often called hyperparameters. Hyperparameters are basically the memories and the knowledge that the machine learned from the model training. Parameters define the skill of a model in solving a problem, such as predicting text. General purpose means that the models are sufficient to solve common problems. Two reasons lead to this idea. First is the commonality of a human language, regardless of the specific tasks, and second is the resource restriction. Only certain organizations have the capability to train such large language models with huge datasets and a tremendous number of parameters. How about letting them create fundamental language models for others to use? This leads to the last point, pre-trained and fine-tuned, meaning to pre-train a large language model for a general purpose with a large dataset and then fine-tune it for specific aims with a much smaller dataset. The benefits of using large language models are straightforward. First, a single model can be used for different tasks. This is a dream come true. These large language models that are trained with petabytes of data and generate billions of parameters are smart enough to solve different tasks including language translation, sentence completion, text classification, question answering, and more. Second, large language models require minimal field training data when you tailor them to solve your specific problem. Large language models obtain decent performance even with little domain training data. In other words, they can be used for few-shot or even zero-shot scenarios. In machine learning, few-shot refers to training a model with minimal data, and zero-shot implies that a model can recognize things that have not explicitly been taught in the training before. Third, the performance of large language models is continuously growing when you add more data and parameters. Let's take Palm as an example. In April 2022, Google released Palm, short for Pathways Language Model, a 540 billion parameter model that achieves a state-of-the-art performance across multiple language tasks. Palm is a dense decoder-only transformer model. It has 540 billion parameters. It leverages the new Pathways system, which has enabled Google to efficiently train a single model across multiple TPUv4 pods. Pathway is a new AI architecture that will handle many tasks at once, learn new tasks quickly, and reflect a better understanding of the world. The system enables Palm to orchestrate distributed computation for accelerators. We previously mentioned that Palm is a transformer model. A transformer model consists of encoder and decoder. The encoder encodes the input sequence and passes it to the decoder, which learns how to decode the representations for a relevant task. We've come a long way from traditional programming to neural networks to generative models. In traditional programming, we used to have to hard-code the rules for distinguishing a cat. Type? Animal. Legs? Four. Ears? Two. Fur? Yes. Likes? Yarn. Catnip. In the wave of neural networks, we could give the network pictures of cats and dogs and ask, is this a cat? And it would predict a cat. In the generative wave, we, as users, can generate our own content, whether it be text, images, audio, video, or other. For example, models like Palm or Lambda, or language model for dialogue applications, ingest very, very large data from multiple sources across the internet, and build foundation language models we can use simply by asking a question, whether typing it into a prompt or verbally talking into the prompt. So when you ask it, what's a cat? It can give you everything it has learned about a cat. Let's compare LLM development using pre-trained models with traditional ML development. First, with LLM development, you don't need to be an expert. You don't need training examples, and there is no need to train a model. All you need to do is think about prompt design, which is the process of creating a prompt that is clear, concise, and informative. It is an important part of natural language processing. In traditional machine learning, you need training examples to train a model. You also need compute time and hardware. Let's take a look at an example of a text generation use case. Question answering, or QA, is a subfield of natural language processing that deals with the task of automatically answering questions posed in natural language. QA systems are typically trained on a large amount of text and code, and they are able to answer a wide range of questions including factual, definitional, and opinion-based questions. The key here is that you need domain knowledge to develop these question answering models. For example, domain knowledge is required to develop a question answering model for customer IT support, or healthcare, or supply chain. Using generative QA, the model generates free text directly based on the context. There is no need for domain knowledge. Let's look at three questions given to BARD, a large-language model chatbot developed by Google AI. Question 1. This year's sales are $100,000. Expenses are $60,000. How much is net profit? BARD first shares how net profit is calculated, then performs the calculation. Then BARD provides the definition of net profit. Here's another question. Inventory on hand is 6,000 units. A new order requires 8,000 units. How many units do I need to fill to complete the order? Again, BARD answers the question by performing the calculation. And our last example, we have 1,000 sensors in 10 geographic regions. How many sensors do we have on average in each region? BARD answers the question with an example on how to solve the problem and some additional context. In each of our questions, a desired response was obtained. This is due to prompt design. Prompt design and prompt engineering are two closely related concepts in natural language processing. Both involve the process of creating a prompt that is clear, concise, and informative. However, there are some key differences between the two. Prompt design is the process of creating a prompt that is tailored to the specific task that this system is being asked to perform. For example, if the system is being asked to translate a text from English to French, the prompt should be written in English and should specify that the translation should be in French. Prompt engineering is the process of creating a prompt that is designed to improve performance. This may involve using domain-specific knowledge, providing examples of the desired output, or using keywords that are known to be effective for the specific system. Prompt design is a more general concept while prompt engineering is a more specialized concept. Prompt design is essential while prompt engineering is only necessary for systems that require a high degree of accuracy or performance. There are three kinds of large language models, generic language models, instruction-tuned, and dialogue-tuned. Each needs prompting in a different way. Generic language models predict the next word based on the language in the training data. This is an example of a generic language model. The next word is a token based on the language in the training data. In this example, the cat sat on, the next word should be the, and you can see that the is the most likely next word. Think of this type as an autocomplete in search. In instruction-tuned, the model is trained to predict a response to the instructions given in the input. For example, summarize a text of x, generate a poem in the style of x, give me a list of keywords based on semantic similarity for x. And in this example, classify the text into neutral, negative, or positive. In dialogue-tuned, the model is trained to have a dialogue by the next response. Dialogue-tuned models are a special case of instruction-tuned where requests are typically framed as questions to a chatbot. Dialogue-tuning is expected to be in the context of a longer back-and-forth conversation and typically works better with natural question-like phrasings. Chain of thought reasoning is the observation that models are better at getting the right answer when they first output text that explains the reason for the answer. Let's look at the question. Roger has five tennis balls. He buys two more cans of tennis balls. Each can has three tennis balls. How many tennis balls does he have now? This question is posed initially with no response. The model is less likely to get the correct answer directly. However, by the time the second question is asked, the output is more likely to end with the correct answer. A model that can do everything has practical limitations. Task-specific tuning can make LLMs more reliable. FlexAI provides task-specific foundation models. Let's say you have a use case where you need to gather sentiments, or how your customers are feeling about your product or service. You can use the classification task Sentiment Analysis Task Model. Same for vision tasks. If you need to perform occupancy analytics, there is a task-specific model for your use case. Tuning a model enables you to customize the model response based on examples of the task that you want the model to perform. It is essentially the process of adapting a model to a new domain or set of custom use cases by training the model on new data. For example, we may collect training data and tune the model specifically for the legal or medical domain. You can also further tune the model by fine-tuning, where you bring your own dataset and retrain the model by tuning every weight in the LLM. This requires a big training job and hosting your own fine-tuned model. Here's an example of a medical foundation model trained on healthcare data. The tasks include question answering, image analysis, finding similar patients, and so forth. Fine-tuning is expensive and not realistic in many cases. So are there more efficient methods of tuning? Yes, Parameter Efficient Tuning Methods, or PETM, are methods for tuning a large-language model on your own custom data without duplicating the model. The base model itself is not altered. Instead, a small number of add-on layers are tuned, which can be swapped in and out at inference time. Generative AI Studio lets you quickly explore and customize generative AI models that you can leverage in your applications on Google Cloud. Generative AI Studio helps developers create and deploy generative AI models by providing a variety of tools and resources that make it easy to get started. For example, there's a library of pre-trained models, a tool for fine-tuning models, a tool for deploying models to production, and a community forum for developers to share ideas and collaborate. Generative AI App Builder lets you create Gen AI apps without having to write any code. Gen AI App Builder has a drag-and-drop interface that makes it easy to design and build apps, a visual editor that makes it easy to create and edit app content, a built-in search engine that allows users to search for information within the app, and a conversational AI engine that allows users to interact with the app using natural language. You can create your own chatbots, digital assistants, custom search engines, knowledge bases, training applications, and more. Palm API lets you test and experiment with Google's large language models and Gen AI tools. To make prototyping quick and more accessible, developers can integrate Palm API with Makersuite and use it to access the API using a graphical user interface. The suite includes a number of different tools, such as a model training tool, a model deployment tool, and a model monitoring tool. The model training tool helps developers train ML models on their data using different algorithms. The model deployment tool helps developers deploy ML models to production with a number of different deployment options, and the model monitoring tool helps developers monitor the performance of their ML models in production using a dashboard and a number of different metrics. That's all for now. Thanks for watching this course, Introduction to Large Language Models.\\n\\nSo, let's get started, so I'll be talking about building LLMs today, so I think a lot of you have heard of LLMs before, but just as a quick recap, LLMs, standing for Large Language Models, are basically all the chatbots that you've been hearing about recently, so ChatGPT from OpenAI, Claude from Entropiq, Gemini, and Lama, and other type of models like this, and today we'll be talking about how do they actually work, so it's going to be an overview because it's only one lecture, and it's hard to compress everything, but hopefully I'll touch a little bit about all the components that are needed to train some of these LLMs. Also, if you have questions, please interrupt me and ask. If you have a question, most likely other people in the room or on Zoom have the same question, so please ask. Great, so what matters when training LLMs? So, there are a few key components that matter. One is the architecture, so as you probably all know, LLMs are neural networks, and when you think about neural networks, you have to think about what architecture you're using. Another component which is really important is the training loss and the training algorithm, so how you actually train these models. Then it's data, so what do you train these models on? The evaluation, which is how do you know whether you're actually making progress towards the goal of LLMs, and then the system component, so that is like how do you actually make these models run on modern hardware, which is really important because these models are really large, so now more than ever, systems are actually really an important topic for LLMs. So, those five components, you probably all know that LLMs, and if you don't know, LLMs are all based on transformers, or at least some version of transformers. I'm actually not going to talk about the architecture today, one, because I gave a lecture on transformers a few weeks ago, and two, because you can find so much information online on transformers, but I think you can- there's much less information about the other four topics, so I really want to talk about those. Another thing to say is that most of academia actually focuses on architecture and training algorithm and losses. As academics, and I've done that for a lot- a big part of my career, is simply we like thinking that this is like we make new architectures, new models, and it seems like it's very important, but in reality, honestly, what matters in practice is mostly the three other topics, so data, evaluation, and systems, which is what most of industry actually focuses on. So that's also one of the reasons why I don't want to talk too much about the architecture, because really the rest is super important. Great, so overview of the lecture, I'll be talking about pre-training. So pre-training, you probably heard that word, this is the general word, this is kind of the classical language modeling paradigm, where you basically train your language model to essentially model all of internet. And then there's a post-training, which is a more recent paradigm, which is taking these large language models and making them essentially AI assistants. So this is more of a recent trend since ChatGPT. So if you ever heard of GPT-3 or GPT-2, that's really pre-training land. If you heard of ChatGPT, which you probably have, this is really post-training land. So I'll be talking about both, but I'll start with pre-training. And specifically, I'll talk about what is the task of pre-training LLMs and what is the laws that people actually use. So language modeling, this is a quick recap. Language models at a high level are simply models of probability distribution over sequences of tokens or of words. So it's basically some model of p of x1 to xl, where x1 is basically word 1 and xl is the last word in the sequence or in the sentence. So very concretely, if you have a sentence like the mouse ate the cheese, what the language model gives you is simply a probability of this sentence being uttered by a human or being found online. So if you have another sentence like the mouse ate cheese, here there's grammatical mistakes. So the model should know that this should have some syntactic knowledge. So it should know that this has less likelihood of appearing online. If you have another sentence like the cheese ate the mouse, then the model should hopefully know about the fact that usually cheese don't eat mouse. So there's some semantic knowledge and this is less likely than the first sentence. So this is basically at a high level what language models are. One word that you probably have been hearing a lot in the news are generative models. So this is just something that can generate models that can generate sentences or can generate some data. The reason why we say language models are generative models is that once you have a model of a distribution, you can simply sample from this model and then we can generate data. So you can generate sentences using a language model. So the type of models that people are all currently using are what we call autoregressive language models. And the key idea of autoregressive language models is that you take this distribution over words and you basically decompose it into the distribution of the first word, multiply it by the distribution of the likelihood of the distribution of the second word given the first word, and multiply it by p of the third word given the first two words. So there's no approximation here. This is just the chain rule of probability, which hopefully you all know about, really no approximation. This is just one way of modeling a distribution. So slightly more concisely, you can write it as a product of p's of the next word given everything which happened in the past, so of the context. So this is what we call autoregressive language models. Again, this is really not the only way of modeling distribution, this is just one way. It has some benefits and some downsides. One downside of autoregressive language models is that when you actually sample from this autoregressive language model, you basically have a for loop which generates the next word, then conditions on that next word, and then regenerate the other word. So basically, if you have a longer sentence that you want to generate, it takes more time to generate it. So there are some downsides of this current paradigm, but that's what we currently have, so I'm going to talk about this one. Great. So autoregressive language models. At a high level, what the task of autoregressive language model is, is simply predicting the next word, as I just said. So if you have a sentence like she likely prefers, one potential next word might be dogs. And the way we do it is that we first tokenize. So you take these words or sub words, you tokenize them, and then you give an ID for each token. So here I have one, two, three. Then you pass it through this black box. As I already said, we're not going to talk about the architecture. You just pass it through a model, and you then get a distribution, a probability distribution over the next word, over the next token. And then you sample from this distribution, you get a new token, and then you de-tokenize. So you get a new ID, you de-tokenize, and that's how you basically sample from a language model. One thing which is important to note is that the last two steps are actually only needed during inference. When you do training, you just need to predict the most likely token, and you can just compare to the real token, which happened in practice, and then you basically change the weights of your model to increase the probability of generating that token. Great. So autoregressive neural language models. So to be slightly more specific, still without talking about the architecture, the first thing we do is that we have all of these- oh, sorry, yes? On the previous slide, when you're predicting the probability of the next token, does this mean that your final output vector has to be the same dimensionality as the number of tokens that you have? Yes. How do you deal with if you're adding more tokens to your co-presenters? Yeah. So we're going to talk about tokenization actually later, so you will get some sense of this. You basically can't deal with adding new tokens. I'm kind of exaggerating. There are methods for doing it, but essentially people don't do it. So it's really important to think about how you tokenize your text, and that's why we'll talk about that later. But it's a very good point to note is that basically the vocabulary size, so the number of tokens that you have, is essentially the output of your language model. So it's actually pretty large. Okay, so autoregressive neural language models. First thing you do is that you take every word or every token. You embed them, so you get some vector representation for each of these tokens. You pass them through some neural network, as we said, it's a transformer. Then you get a representation for all the words in the context. So it's basically a representation of the entire sentence. You pass it through a linear layer, as you just said, to basically map it to the number so that the output, the number of outputs is the number of tokens. You then pass it through some softmax, and you basically get a probability distribution over the next words given every word in the context. And the laws that you use is basically, it's essentially a task of classifying the next token. So it's a very simple kind of machine learning task. So you use the cross-entropy laws, where you basically look at the actual target that happened, which is a target distribution, which is a one-hot encoding, which here in this case says, I saw the real word that happened is cat. So that's a one-hot distribution over cat. And here, this is the distribution that you generated. And basically, you do cross-entropy, which really just increases the probability of generating cat and decreases the probability of generating all the other tokens. One thing to notice is that, as you all know, again, this is just equivalent to maximizing the text log likelihood, because you can just rewrite the max over the probability of this autoregressive language modeling task as just being this minimum of, I just added the log here and minus, which is just the minimum of the loss, which is the cross-entropy loss. So basically, minimizing the loss is the same thing as maximizing the likelihood of your text. Any questions? OK, tokenizer. So this is one thing that people usually don't talk that much about. Tokenizers are extremely important. So it's really important that you understand, at least, what they do at a high level. So why do we need tokenizers in the first place? First, it's more general than words. So one simple thing that you might think is, oh, we're just going to take every word that we will have. And you just say, every word is a token in its own. But then what happens is, if there's a typo in your word, then you might not have any token associated with this word with a typo. And then you don't know how to actually pass this word with a typo into a large language model. So what do you do next? And also, even if you think about words, words are fine with Latin-based languages. But if you think about a language like Thai, you won't have a simple way of tokenizing by spaces, because there are no spaces between words. So really, tokens are much more general than words. First thing. Second thing that you might think is that you might tokenize every sentence, character by character. You might say, A is one token, B is another token. That would actually work, and probably very well. The issue is that then your sequence becomes super long. And as you probably remember from the lecture on transformers, the complexity grows quadratically with the length of sequences. So you really don't want to have a super long sequence. So tokenizers basically try to deal with those two problems and give common sub-sequences a certain token. And usually, how you should be thinking about it is around an average of every token is around three, four letters. And there are many algorithms for tokenization. I'll just talk about one of them to give you a high level, which is what we call byte-pair encoding, which is actually pretty common, one of the two most common tokenizers. And the way that you train a tokenizer is that first, you start with a very large corpus of text. And here, I'm really not talking about training a large language model yet. This is purely for the tokenization step. So this is my large corpus of text with these five words. Then you associate every character in this corpus of text a different token. So here, I just split up every character with a different token, and I just color coded all of those tokens. And then what you do is that you go through your text, and every time you see pairs of tokens that are very common, the most common pair of token, you just merge them. So here, you see three times the tokens T and O next to each other, so you're just going to say this is a new token. And then you continue. You repeat that. So now you have T-O-K, TOK, which happens three times, TOK with an E, that happens two times, and TOKEN, which happens twice, and then EX, which also happens twice. So this is that if you were to train a tokenizer on this corpus of text, which is very small, that's how you would finish with a trained tokenizer. In reality, you do it on much larger corpuses of text. And this is the real tokenizer of, actually, I think this is GPT-3 or ChatGPT. And here, you see how it would actually separate these words. So basically, you see the same thing as what we gave in the previous example, TOKEN becomes its own token. So tokenizer is actually split up into two tokens, TOKEN and EIZER. So yeah, that's all about tokenizers. Any question on that? Yeah? How do you deal with spaces, and how do you deal with introduction? Yeah. So actually, there's a step before tokenizers, which is what we call pre-tokenizers, which is exactly what you just said. So this is mostly, in theory, there's no reason to deal with spaces and punctuation separately. You could just say every space gets its own token, every punctuation gets its own token, and you could just do all the merging. The problem is that, so there's an efficiency question. Actually, training these tokenizers takes a long time. So you're better off, because you have to consider every pair of token. So what you end up doing is saying, if there's a space, this is very, like pre-tokenizers are very English-specific. So you say, if there's a space, we're not going to start looking at the token that came before and the token that came afterwards. So you're not merging in between spaces. But this is just like a computation optimization. You could theoretically just deal with it the same way as you deal with any other character. Yeah? When you merge tokens, do you delete the tokens that you merged away, or do you keep the smaller tokens that you merged? You actually keep the smaller tokens. I mean, in reality, it doesn't matter much, because usually on large corpus of text, you will have actually everything. But you usually keep the small ones. And the reason why you want to do that is because if, in case there's a, as we said before, you have some grammatical mistakes or some typos, you still want to be able to represent these words by character. So yeah. Yes? Are the tokens unique? So, I mean, say, in this case, T-O-K-E-N, is there only one occurrence, or do you need to leave multiple occurrence so they can take on different meanings or something? Oh, oh, I see what you say. No, no, no. It's every token has its own unique ID. So a usual, this is a great question. For example, if you think about a bank, which could be bank for money or bank like water, they will have the same token, but the model will learn, the transformer will learn that based on the words that are around it, it should associate that, I'm saying, I'm being very hand-wavy here, but associate that with a representation that is either more like the bank money side or the bank water side. But that's the transformer that does that. It's not a tokenizer. Yes? Yeah, so you mentioned during tokenization, you keep the smaller tokens to start with, right? So if you start with a T, you keep the T, and then you tell your tokenizer to expand that amount of token. So let's say maybe you didn't train on token, but in your data, you are trying to encode token. So how does the tokenizer know to encode it with token or to do it with T? Yeah, it's a great question. You basically, when you, so when you tokenize, so that's after training of the tokenizer, when you actually apply the tokenizer, you basically always choose the largest token that you can apply. So if you can do token, you will never do T. You will always do token. But it's actually, so people don't usually talk that much about tokenizers, but there's a lot of computational benefits or computational tricks that you can do for making these things faster. So I really don't think we, and honestly, I think a lot of people think that we should just get away from tokenizers and just kind of tokenize\\n\\nHi everyone. So recently I gave a 30-minute talk on large language models, just kind of like an intro talk. Unfortunately that talk was not recorded, but a lot of people came to me after the talk and they told me that they really liked the talk, so I thought I would just re-record it and basically put it up on YouTube. So here we go, the busy person's intro to large language models, Director Scott. Okay, so let's begin. First of all, what is a large language model really? Well, a large language model is just two files, right? There will be two files in this hypothetical directory. So, for example, working with the specific example of the LLAMA2 70b model, this is a large language model released by Meta.ai, and this is basically the LLAMA series of language models, the second iteration of it, and this is the 70 billion parameter model of this series. So there's multiple models belonging to the LLAMA2 series, 7 billion, 13 billion, 34 billion, and 70 billion is the biggest one. Now many people like this model specifically because it is probably today the most powerful open weights model. So basically the weights and the architecture and a paper was all released by Meta, so anyone can work with this model very easily by themselves. This is unlike many other language models that you might be familiar with. For example, if you're using ChatsGPT or something like that, the model architecture was never released. It is owned by OpenAI, and you're allowed to use the language model through a web interface, but you don't have actually access to that model. So in this case, the LLAMA2 70b model is really just two files on your file system, the parameters file and the run some kind of a code that runs those parameters. So the parameters are basically the weights or the parameters of this neural network that is the language model. We'll go into that in a bit. Because this is a 70 billion parameter model, every one of those parameters is stored as two bytes, and so therefore the parameters file here is 140 gigabytes, and it's two bytes because this is a float 16 number as the data type. Now in addition to these parameters, that's just like a large list of parameters for that neural network. You also need something that runs that neural network, and this piece of code is implemented in our run file. Now this could be a C file, or a Python file, or any other programming language really. It can be written any arbitrary language, but C is sort of like a very simple language just to give you a sense, and it would only require about 500 lines of C with no other dependencies to implement the neural network architecture, and that uses basically the parameters to run the model. So it's only these two files. You can take these two files, and you can take your MacBook, and this is a fully self-contained package. This is everything that's necessary. You don't need any connectivity to the internet or anything else. You can take these two files, you compile your C code, you get a binary that you can point at the parameters, and you can talk to this language model. So for example, you can send it text, like for example, write a poem about the company Scale.ai, and this language model will start generating text, and in this case, it will follow the directions and give you a poem about Scale.ai. Now the reason that I'm picking on Scale.ai here, and you're going to see that throughout the talk, is because the event that I originally presented this talk with was run by Scale.ai, and so I'm picking on them throughout the slides a little bit, just in an effort to make it concrete. So this is how we can run the model. Just requires two files, just requires a MacBook. I'm slightly cheating here, because this was not actually, in terms of the speed of this video here, this was not running a 70 billion parameter model, it was only running a 7 billion parameter model. A 70B would be running about 10 times slower, but I wanted to give you an idea of sort of just the text generation and what that looks like. So not a lot is necessary to run the model. This is a very small package, but the computational complexity really comes in when we'd like to get those parameters. So how do we get the parameters, and where are they from? Because whatever is in the run.c file, the neural network architecture, and sort of the forward pass of that network, everything is algorithmically understood and open and so on. But the magic really is in the parameters, and how do we obtain them? So to obtain the parameters, basically the model training, as we call it, is a lot more involved than model inference, which is the part that I showed you earlier. So model inference is just running it on your MacBook. Model training is a competitionally very involved process. So basically what we're doing can best be sort of understood as kind of a compression of a good chunk of internet. So because Lama270B is an open source model, we know quite a bit about how it was trained, because Meta released that information in paper. So these are some of the numbers of what's involved. You basically take a chunk of the internet that is roughly, you should be thinking, 10 terabytes of text. This typically comes from like a crawl of the internet. So just imagine just collecting tons of text from all kinds of different websites and collecting it together. So you take a large chunk of internet, then you procure a GPU cluster, and these are very specialized computers intended for very heavy computational workloads like training of neural networks. You need about 6,000 GPUs, and you would run this for about 12 days to get a Lama270B. And this would cost you about $2 million. And what this is doing is basically it is compressing this large chunk of text into what you can think of as a kind of a zip file. So these parameters that I showed you in an earlier slide are best thought of as like a zip file of the internet. And in this case, what would come out are these parameters, 140 gigabytes. So you can see that the compression ratio here is roughly like 100x, roughly speaking. But this is not exactly a zip file because a zip file is lossless compression. What's happening here is a lossy compression. We're just kind of like getting a kind of a gestalt of the text that we trained on. We don't have an identical copy of it in these parameters. And so it's kind of like a lossy compression. You can think about it that way. The one more thing to point out here is these numbers here are actually by today's standards in terms of state-of-the-art, rookie numbers. So if you want to think about state-of-the-art neural networks, like say what you might use in chatGPT or Clod or Bard or something like that, these numbers are off by a factor of 10 or more. So you would just go in and you would just like start multiplying by quite a bit more. And that's why these training runs today are many tens or even potentially hundreds of millions of dollars, very large clusters, very large datasets. And this process here is very involved to get those parameters. Once you have those parameters, running the neural network is fairly computationally cheap. Okay. So what is this neural network really doing? I mentioned that there are these parameters. This neural network basically is just trying to predict the next word in a sequence. You can think about it that way. So you can feed in a sequence of words, for example, cat sat on A. This feeds into a neural net and these parameters are dispersed throughout this neural network. And there's neurons and they're connected to each other and they all fire in a certain way. You can think about it that way. And out comes a prediction for what word comes next. So for example, in this case, this neural network might predict that in this context of four words, the next word will probably be a mat with say 97% probability. So this is fundamentally the problem that the neural network is performing. And you can show mathematically that there's a very close relationship between prediction and compression, which is why I sort of allude to this neural network as kind of training it as kind of like a compression of the internet, because if you can predict sort of the next word very accurately, you can use that to compress the dataset. So it's just a next word prediction neural network. You give it some words, it gives you the next word. Now, the reason that what you get out of the training is actually quite a magical artifact is that basically the next word prediction task you might think is a very simple objective, but it's actually a pretty powerful objective because it forces you to learn a lot about the world inside the parameters of the neural network. So here I took a random webpage at the time when I was making this talk. I just grabbed it from the main page of Wikipedia and it was about Ruth Handler. And so think about being the neural network and you're given some amount of words and trying to predict the next word in a sequence. Well, in this case, I'm highlighting here in red some of the words that would contain a lot of information. And so, for example, if your objective is to predict the next word, presumably your parameters have to learn a lot of this knowledge. You have to know about Ruth and Handler and when she was born and when she died, who she was, what she's done, and so on. And so in the task of next word prediction, you're learning a ton about the world and all this knowledge is being compressed into the weights, the parameters. Now, how do we actually use these neural networks? Well, once we've trained them, I showed you that the model inference is a very simple process. We basically generate what comes next. We sample from the model. So we pick a word and then we continue feeding it back in and get the next word and continue feeding that back in. So we can iterate this process and this network then dreams internet documents. So, for example, if we just run the neural network, or as we say, perform inference, we would get sort of like web page dreams. You can almost think about it that way, right? Because this network was trained on web pages and then you can sort of like let it loose. So on the left, we have some kind of a Java code dream, it looks like. In the middle, we have some kind of a, what looks like almost like an Amazon product dream. And on the right, we have something that almost looks like Wikipedia article. Focusing for a bit on the middle one, as an example, the title, the author, the ISBN number, everything else, this is all just totally made up by the network. The network is dreaming text from the distribution that it was trained on. It's mimicking these documents. But this is all kind of like hallucinated. So, for example, the ISBN number, this number probably, I would guess almost certainly does not exist. The model network just knows that what comes after ISBN colon is some kind of a number of roughly this length, and it's got all these digits. And it just like puts it in. It just kind of like puts in whatever looks reasonable. So it's parroting the training dataset distribution. On the right, the black nose dace, I looked it up, and it is actually a kind of fish. And what's happening here is this text verbatim is not found in the training set documents, but this information, if you actually look it up, is actually roughly correct with respect to this fish. And so the network has knowledge about this fish. It knows a lot about this fish. It's not going to exactly parrot documents that it saw in the training set. But again, it's some kind of a lossy compression of the internet. It kind of remembers the gestalt. It kind of knows the knowledge. And it just kind of like goes and it creates the form. It creates kind of like the correct form and fills it with some of its knowledge. And you're never 100% sure if what it comes up with is as we call hallucination or like an incorrect answer or like a correct answer necessarily. So some of this stuff could be memorized and some of it is not memorized and you don't exactly know which is which. But for the most part, this is just kind of like hallucinating or like dreaming internet text from its data distribution. Okay, let's now switch gears to how does this network work? How does it actually perform this next word prediction task? What goes on inside it? Well, this is where things complicate a little bit. This is kind of like the schematic diagram of the neural network. If we kind of like zoom in into the toy diagram of this neural net, this is what we call the transformer neural network architecture. And this is kind of like a diagram of it. Now, what's remarkable about this neural net is we actually understand in full detail the architecture. We know exactly what mathematical operations happen at all the different stages of it. The problem is that these 100 billion parameters are dispersed throughout the entire neural network. And so basically, these billions of parameters are throughout the neural net. And all we know is how to adjust these parameters iteratively to make the network as a whole better at the next word prediction task. So we know how to optimize these parameters. We know how to adjust them over time to get a better next word prediction. But we don't actually really know what these 100 billion parameters are doing. We can measure that it's getting better at the next word prediction. But we don't know how these parameters collaborate to actually perform that. We have some kind of models that you can try to think through on a high level for what the network might be doing. So we kind of understand that they build and maintain some kind of a knowledge database. But even this knowledge database is very strange and imperfect and weird. So a recent viral example is what we call the reversal course. So as an example, if you go to chat GPT and you talk to GPT-4, the best language model currently available, you say, who is Tom Cruise's mother? It will tell you it's Mary Lee Pfeiffer, which is correct. But if you say, who is Mary Lee Pfeiffer's son? It will tell you it doesn't know. So this knowledge is weird, and it's kind of one dimensional. And you have to sort of like, this knowledge isn't just like stored and can be accessed in all the different ways. You sort of like ask it from a certain direction almost. And so that's really weird and strange. And fundamentally, we don't really know because all you can kind of measure is whether it works or not, and with what probability. So long story short, think of LLMs as kind of like mostly inscrutable artifacts. They're not similar to anything else you might build in an engineering discipline. They're not like a car where we sort of understand all the parts. They're these neural nets that come from a long process of optimization. And so we don't currently understand exactly how they work, although there's a field called interpretability or mechanistic interpretability, trying to kind of go in and try to figure out what all the parts of this neural net are doing. And you can do that to some extent, but not fully right now. But right now, we kind of treat them mostly as empirical artifacts. We can give them some inputs and we can measure the outputs. We can basically measure their behavior. We can look at the text that they generate in many different situations. And so I think this requires basically correspondingly sophisticated evaluations to work with these models because they're mostly empirical. So now let's go to how we actually obtain an assistant. So far, we've only talked about these internet document generators, right? And so that's the first stage of training. We call that stage pre-training. We're now moving to the second stage of training, which we call fine-tuning. And this is where we obtain what we call an assistant model because we don't actually really just want document generators. That's not very helpful for many tasks. We want to give questions to something and we want it to generate answers based on those questions. So we really want an assistant model instead. And the way you obtain these assistant models is fundamentally through the following process. We basically keep the optimization identical. So the training will be the same. It's just a next word prediction task, but we're going to swap out the dataset on which we are training. So it used to be that we are trying to train on internet documents. We're going to now swap it out for datasets that we collect manually. And the way we collect them is by using lots of people. So typically a company will hire people and they will give them labeling instructions and they will ask people to come up with questions and then write answers for them. So here's an example of a single example that might basically make it into your training set. So there's a user and it says something like, can you write a short introduction about the relevance of the term monopsony in economics and so on. And then there's assistant. And again, the person fills in what the ideal response should be and the ideal response and how that is specified and what it should look like all just comes from labeling documentations that we provide these people and the engineers at a company like OpenAI or Anthropic or whatever else will come up with these labeling documentations. Now the pre-training stage is about a large quantity of text, but potentially low quality because it just comes from the internet and there's tens of hundreds of terabytes of it and it's not all very high quality. But in this second stage, we prefer quality over quantity. So we may have many fewer documents, for example, a hundred thousand, but all of these documents now are conversations and they should be very high quality conversations. And fundamentally people create them based on labeling instructions. So we swap out the dataset now and we train on these Q&A documents. And this process is called fine tuning. Once you do this, you obtain what we call an assistant model. So this assistant\\n\\nthat we'll talk about of how to do these things, uh, slightly more, um, automatically. And then at the end of training, uh, usually train, um, after training on all of this data that we saw, usually train on very high-quality data at the end of, uh, training a large-language model where you decrease your learning rate. Uh, and that basically means that you're kind of overfitting your model on a very high-quality data. So usually what you do there is like Wikipedia. You basically overfit on Wikipedia and you overfit on, like, hu- human, uh, data that was collected. Um, the other things like continual pre-training for getting longer context. I'm, I'm gonna skip over all of these things. Uh, but that's just to give you a sense of how hard it is when people just say, oh, I'm gonna train on Internet. That's a lot of work. Um, and really we haven't figured it out yet. So collecting world data is a huge part of practical large-language model. Uh, some might say it's actually the key. Yes. About data. So basic question. So usually when you start with like the terabyte of data, after I go through all the steps, what's the typical amount of data that you have to get in? And then like how, how large a team does it typically take to go through all the different steps you talked about? Sorry, how- is your question how large is the data after you filter? Yeah, after you filter and then you go through all the steps. How large a team do you need to go through, like, the filt- all the filtration steps you mentioned? Uh, how slow is it? Or how, how, like, how la- how many people would you need? Oh. To be able to do this? Uh, okay. That's a great question. I'm gonna somewhat answer about the data, uh, how large is the data set, uh, at the end of this slide. Uh, for number of people that work on it, uh, that's a good question. I'm actually not quite sure, but I would say, yeah, I actually don't quite know, but I would say it's probably even bigger than number of people that work on kind of the tuning of the pre-training of the model. Uh, so the data is bigger than kind of the modeling aspect. Um, yeah, I, I don't think I have a good sense. I would say probably in Lama's team, which have like 70-ish people, I would say maybe 15 work on data. Uh, yeah. All these things, you don't need that many people, you need a lot of computer also, because for data, you need a lot of CPUs. Um, so yeah, and I'll answer the second question at the end of this slide. So as I just kind of alluded to, really, we haven't solved data at all for pre-training. So there's a lot of research that has to be done. First, how do you process these things super efficiently? Uh, second, how do you balance the kind of like the- all of these different domains? Uh, can you do synthetic data generation? That's actually a big one right now. Uh, and because we don't have- we'll talk about that later, but we don't have enough data on the Internet. Um, can you use multi-modal data instead of just text data, and how does that improve even your text performance? Um, there's a lot of secrecy, because really, this is the key of most of the pre-train- pre-train large language models. Uh, so for competitive dynamics, uh, usually these- these, um, these companies don't talk about how they do the data collection, and also there's a copyright liability issue. They definitely don't want to tell you that they've trained on books even though they did, um, because if not, you can, uh, sue them. Uh, common academic benchmarks, uh, so that will kind of answer what you asked. Um, it started- so those are the smaller ones. Uh, it's- the names are not that important, but it started from around 150 billion tokens, which around, uh, 800 gigabytes of data, and now it's around 15 trillion of to- 15 trillion tokens, which is also, uh, the size of the models that are- right now, the best models are probably trained on that amount of data. So 15 trillion tokens, uh, which is probably, I guess, two order of magnitude bigger than that, so 80, uh, E3 gigabyte. So that would be around 100 to 1,000 times, uh, filtering of the common crawl, if I'm not mistaken. Um, so yeah. One very com- one very, uh, famous one is the PAL. So this is an academic benchmark of the PAL, and we can just look at what distribution of data they have. It's things like, um, Archive, PubMed Central, uh, which is all the- the biology stuff. Uh, here, it's Wikipedia. You see Stack Exchange, um, some GitHub, and some books, and things like this. Um, again, this is on the smaller side.\", 'answer': 'To train large language models, you can use techniques such as prompt engineering, model fine-tuning, or building your own model from scratch. Prompt engineering involves using the model out of the box without changing its parameters, while model fine-tuning adjusts the model parameters for a specific task. Building your own model requires creating a new model from the ground up, which is more resource-intensive but allows for full customization.'}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The purpose of a Large Language Model (LLM) is to perform a variety of natural language processing tasks such as text classification, question answering, document summarization, and text generation. LLMs are pre-trained on large datasets and can be fine-tuned for specific purposes, making them versatile tools for various applications. They are designed to understand and generate human language effectively.\n",
      "None\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "To train large language models, you can use techniques such as prompt engineering, model fine-tuning, or building your own model from scratch. Prompt engineering involves using the model out of the box without changing its parameters, while model fine-tuning adjusts the model parameters for a specific task. Building your own model requires creating a new model from the ground up, which is more resource-intensive but allows for full customization.\n",
      "None\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't know your name based on the provided context.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Define the configuration for the workflow\n",
    "config = {\"configurable\": {\"thread_id\": \"test_thread\"}}\n",
    "\n",
    "query = \"Hi my name is Obi-Wan, what is the purpose of an LLM?\"\n",
    "\n",
    "# Run the workflow\n",
    "q1 = app.invoke({\"input\": query}, config)\n",
    "q2 = app.invoke({\"input\": \"How can I train them?\"}, config)\n",
    "q3 = app.invoke({\"input\": \"What is my name?\"}, config)\n",
    "\n",
    "\n",
    "print( q1[\"chat_history\"][-1].pretty_print())\n",
    "print( q2[\"chat_history\"][-1].pretty_print())\n",
    "print( q3[\"chat_history\"][-1].pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
